#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

æœ¬ç« æ¼”ç¤ºæœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„æ ¸å¿ƒåº”ç”¨ï¼š
- Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½é¢„æµ‹
- Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°  
- Part 3: æ¨¡å‹ä¼˜åŒ– - æå‡é¢„æµ‹æ€§èƒ½
- Part 4: å®æˆ˜é¡¹ç›® - ç™Œç—‡åˆ†å‹é¢„æµ‹

ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”:
æœºå™¨å­¦ä¹  = ä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹çš„ç®—æ³•
â€¢ ç›‘ç£å­¦ä¹ ï¼šæœ‰è€å¸ˆçš„å­¦ä¹ ï¼ˆåˆ†ç±»ã€å›å½’ï¼‰
â€¢ æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°éšè—æ¨¡å¼ï¼ˆèšç±»ã€é™ç»´ï¼‰ 
â€¢ æ¨¡å‹ä¼˜åŒ–ï¼šæå‡æ€§èƒ½çš„è‰ºæœ¯
â€¢ æ·±åº¦å­¦ä¹ ï¼šAIé©å‘½çš„å¼€å§‹

æ ¸å¿ƒç†å¿µï¼šæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# åˆ†ç±»ç®—æ³•
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve, 
                           confusion_matrix, silhouette_score, 
                           adjusted_rand_score, classification_report)

import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"{title}")
    print("="*60)


# ====================
# Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»
# ====================

def create_gene_classification_data():
    """
    åˆ›å»ºåŸºå› åŠŸèƒ½åˆ†ç±»çš„ç¤ºä¾‹æ•°æ®
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    æ ¹æ®åŸºå› åºåˆ—ç‰¹å¾é¢„æµ‹åŠŸèƒ½ç±»åˆ«ï¼Œè¿™åœ¨åŸºå› æ³¨é‡Šä¸­æå…¶é‡è¦ï¼š
    - ç®¡å®¶åŸºå› ï¼šç»´æŒåŸºæœ¬ç»†èƒåŠŸèƒ½ï¼Œè¡¨è¾¾ç¨³å®š
    - è½¬å½•å› å­ï¼šè°ƒæ§åŸºå› è¡¨è¾¾ï¼Œåºåˆ—ä¿å®ˆ
    - æ¿€é…¶ï¼šä¿¡å·ä¼ å¯¼å…³é”®é…¶ï¼Œç»“æ„åŸŸç‰¹å¼‚
    
    ğŸ”¬ åº”ç”¨åœºæ™¯ï¼š
    â€¢ åŸºå› ç»„æ³¨é‡Šå’ŒåŠŸèƒ½é¢„æµ‹
    â€¢ è¯ç‰©é¶ç‚¹è¯†åˆ«å’ŒéªŒè¯
    â€¢ ç–¾ç—…ç›¸å…³åŸºå› ç­›é€‰
    """
    print_section("Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»")
    
    np.random.seed(42)
    n_samples = 300
    
    # åˆ›å»ºç‰¹å¾ï¼ˆåŸºå› åºåˆ—ç‰¹å¾ï¼‰
    # ç‰¹å¾1ï¼šGCå«é‡
    # ç‰¹å¾2ï¼šé•¿åº¦
    # ç‰¹å¾3ï¼šä¿å®ˆæ€§è¯„åˆ†
    # ç‰¹å¾4ï¼šè¡¨è¾¾æ°´å¹³
    # ç‰¹å¾5ï¼šè¿›åŒ–é€Ÿç‡
    
    features = []
    labels = []
    
    # ç®¡å®¶åŸºå› ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.45, 0.05)  # ä¸­ç­‰GCå«é‡
        length = np.random.normal(1500, 300)        # ä¸­ç­‰é•¿åº¦
        conservation = np.random.normal(0.8, 0.1)   # é«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.7, 0.1)     # é«˜è¡¨è¾¾
        evolution_rate = np.random.normal(0.2, 0.05) # ä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(0)
    
    # è½¬å½•å› å­ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.55, 0.05)   # è¾ƒé«˜GCå«é‡
        length = np.random.normal(2000, 400)        # è¾ƒé•¿
        conservation = np.random.normal(0.6, 0.15)  # ä¸­ç­‰ä¿å®ˆæ€§
        expression = np.random.normal(0.3, 0.1)     # ä½è¡¨è¾¾
        evolution_rate = np.random.normal(0.4, 0.1) # ä¸­ç­‰è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(1)
    
    # æ¿€é…¶ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.50, 0.05)   # ä¸­ç­‰GCå«é‡
        length = np.random.normal(2500, 500)        # é•¿
        conservation = np.random.normal(0.7, 0.1)   # è¾ƒé«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.5, 0.15)    # ä¸­ç­‰è¡¨è¾¾
        evolution_rate = np.random.normal(0.3, 0.08) # è¾ƒä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(2)
    
    # åˆ›å»ºDataFrame
    feature_names = ['GC_content', 'length', 'conservation', 'expression', 'evolution_rate']
    df = pd.DataFrame(features, columns=feature_names)
    df['function'] = labels
    
    print("æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {df['function'].value_counts().to_dict()}")
    print("\nç‰¹å¾ç»Ÿè®¡ï¼š")
    print(df[feature_names].describe().round(3))
    
    return df, feature_names


def visualize_feature_distributions(df, feature_names):
    """å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ"""
    print("\nå¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    function_names = {0: 'Housekeeping', 1: 'TF', 2: 'Kinase'}
    colors = ['blue', 'orange', 'green']
    
    for idx, feature in enumerate(feature_names):
        ax = axes[idx]
        for func_id in [0, 1, 2]:
            data = df[df['function'] == func_id][feature]
            ax.hist(data, alpha=0.5, label=function_names[func_id], 
                   color=colors[func_id], bins=20)
        ax.set_xlabel(feature)
        ax.set_ylabel('Frequency')
        ax.set_title(f'Distribution of {feature}')
        ax.legend()
    
    # ç©ºç™½å­å›¾
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')
    plt.show()


def train_classification_models(df, feature_names):
    """
    è®­ç»ƒå¤šç§åˆ†ç±»æ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç”¨ä¸åŒçš„æ–¹æ³•è¯Šæ–­ç–¾ç—…
    - é€»è¾‘å›å½’ï¼šæ ¹æ®ç—‡çŠ¶çš„çº¿æ€§ç»„åˆåˆ¤æ–­
    - å†³ç­–æ ‘ï¼šåƒåŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹å›¾
    - éšæœºæ£®æ—ï¼šå¤šä¸ªåŒ»ç”ŸæŠ•ç¥¨å†³å®š
    - SVMï¼šæ‰¾åˆ°æœ€ä½³çš„è¯Šæ–­è¾¹ç•Œ
    """
    print_section("è®­ç»ƒåˆ†ç±»æ¨¡å‹")
    
    # å‡†å¤‡æ•°æ®
    X = df[feature_names].values
    y = df['function'].values
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ70%è®­ç»ƒï¼Œ30%æµ‹è¯•ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
        'å†³ç­–æ ‘': DecisionTreeClassifier(random_state=42, max_depth=5),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_estimators=100),
        'SVM': SVC(random_state=42, probability=True),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"  å¬å›ç‡: {recall:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    return results, X_test_scaled, y_test, scaler


def visualize_model_comparison(results):
    """å¯è§†åŒ–æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
    print("\næ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    
    # æå–æŒ‡æ ‡
    models = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    # åˆ›å»ºæ¯”è¾ƒå›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(models))
    width = 0.2
    
    for i, metric in enumerate(metrics):
        values = [results[model][metric] for model in models]
        ax.bar(x + i*width, values, width, label=metric.capitalize())
    
    ax.set_xlabel('Model')
    ax.set_ylabel('Score')
    ax.set_title('Model Performance Comparison')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(models, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()


def demonstrate_overfitting():
    """
    æ¼”ç¤ºè¿‡æ‹Ÿåˆç°è±¡
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    è¿‡æ‹Ÿåˆå°±åƒåŒ»å­¦ç”Ÿæ­»è®°ç¡¬èƒŒç—…ä¾‹ï¼Œä¸ç†è§£ç–¾ç—…æœ¬è´¨ï¼š
    â€¢ æ¬ æ‹Ÿåˆï¼šè¿‡äºç®€åŒ–ï¼Œæ¼æ‰é‡è¦ç‰¹å¾
    â€¢ è‰¯å¥½æ‹Ÿåˆï¼šæŠ“ä½æ ¸å¿ƒè§„å¾‹ï¼Œæ³›åŒ–èƒ½åŠ›å¼º
    â€¢ è¿‡æ‹Ÿåˆï¼šè®°ä½å™ªå£°ï¼Œæ–°æ•°æ®è¡¨ç°å·®
    
    å¹³è¡¡ä¹‹é“ï¼š
    â€¢ å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæ›´å¤šç—…ä¾‹ï¼‰
    â€¢ ç®€åŒ–æ¨¡å‹ï¼ˆæŠ“ä½ä¸»è¦ç—‡çŠ¶ï¼‰
    â€¢ æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–ï¼‰
    â€¢ äº¤å‰éªŒè¯ï¼ˆå¤šä¸­å¿ƒéªŒè¯ï¼‰
    """
    print_section("è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ")
    
    print("""
ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ
è¿‡æ‹Ÿåˆå°±åƒä¸€ä¸ªåŒ»å­¦ç”Ÿæ­»è®°ç¡¬èƒŒäº†æ‰€æœ‰ç—…ä¾‹ï¼Œ
ä½†ä¸ç†è§£ç–¾ç—…çš„æœ¬è´¨è§„å¾‹ï¼Œé‡åˆ°æ–°ç—…ä¾‹å°±æŸæ‰‹æ— ç­–ã€‚

å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ
1. å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆçœ‹æ›´å¤šç—…ä¾‹ï¼‰
2. ç®€åŒ–æ¨¡å‹ï¼ˆæŠ“ä½ä¸»è¦ç—‡çŠ¶ï¼‰
3. æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–æŸä¸ªç‰¹å¾ï¼‰
4. äº¤å‰éªŒè¯ï¼ˆåœ¨ä¸åŒåŒ»é™¢éªŒè¯ï¼‰
5. æ—©åœï¼ˆé€‚å¯è€Œæ­¢ï¼‰
    """)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.sort(np.random.rand(100, 1) * 10, axis=0)
    y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    
    # è®­ç»ƒä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import Pipeline
    
    degrees = [1, 4, 15]
    titles = ['Underfitting', 'Good Fit', 'Overfitting']
    
    for ax, degree, title in zip(axes, degrees, titles):
        # åˆ›å»ºå¤šé¡¹å¼å›å½’
        polynomial_features = PolynomialFeatures(degree=degree)
        linear_regression = LinearRegression()
        pipeline = Pipeline([
            ("polynomial_features", polynomial_features),
            ("linear_regression", linear_regression)
        ])
        
        pipeline.fit(X, y)
        
        # é¢„æµ‹
        X_plot = np.linspace(0, 10, 300)[:, np.newaxis]
        y_plot = pipeline.predict(X_plot)
        
        # ç»˜å›¾
        ax.scatter(X, y, alpha=0.5, label='Data')
        ax.plot(X_plot, y_plot, color='red', label=f'Degree {degree}')
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title(title)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('overfitting_demo.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°
# ====================

def create_cell_expression_data():
    """
    åˆ›å»ºå•ç»†èƒåŸºå› è¡¨è¾¾æ•°æ®
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    å•ç»†èƒRNAæµ‹åºæ­ç¤ºç»†èƒå¼‚è´¨æ€§ï¼Œä¸åŒç»†èƒç±»å‹æœ‰ç‹¬ç‰¹çš„è¡¨è¾¾è°±ï¼š
    - å¹²ç»†èƒï¼šå¤šèƒ½æ€§åŸºå› ï¼ˆOCT4, NANOGï¼‰é«˜è¡¨è¾¾
    - åˆ†åŒ–ä¸­ç»†èƒï¼šè¿‡æ¸¡çŠ¶æ€ï¼Œè¡¨è¾¾è°±åŠ¨æ€å˜åŒ–
    - æˆç†Ÿç»†èƒï¼šç‰¹å¼‚æ€§åŠŸèƒ½åŸºå› é«˜è¡¨è¾¾
    
    ğŸ”¬ ç§‘å­¦æ„ä¹‰ï¼š
    â€¢ å‘ç°æ–°çš„ç»†èƒäºšå‹å’ŒçŠ¶æ€
    â€¢ ç†è§£ç»†èƒåˆ†åŒ–è½¨è¿¹
    â€¢ è¯†åˆ«ç–¾ç—…ç›¸å…³çš„ç»†èƒç¾¤ä½“
    """
    print_section("Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°")
    
    np.random.seed(42)
    
    # åˆ›å»ºä¸‰ç§ç»†èƒç±»å‹çš„è¡¨è¾¾æ•°æ®
    n_genes = 50
    gene_names = [f"Gene_{i:03d}" for i in range(1, n_genes+1)]
    
    # å¹²ç»†èƒï¼ˆé«˜è¡¨è¾¾å¤šèƒ½æ€§åŸºå› ï¼‰
    stem_cells = np.random.lognormal(2, 0.5, (30, n_genes))
    stem_cells[:, :10] *= 2  # å‰10ä¸ªåŸºå› é«˜è¡¨è¾¾ï¼ˆå¹²æ€§æ ‡è®°ï¼‰
    
    # åˆ†åŒ–ä¸­çš„ç»†èƒï¼ˆä¸­é—´çŠ¶æ€ï¼‰
    diff_cells = np.random.lognormal(2, 0.6, (40, n_genes))
    diff_cells[:, 10:25] *= 1.8  # ä¸­é—´åŸºå› é«˜è¡¨è¾¾ï¼ˆåˆ†åŒ–æ ‡è®°ï¼‰
    
    # æˆç†Ÿç»†èƒï¼ˆç‰¹å®šåŸºå› é«˜è¡¨è¾¾ï¼‰
    mature_cells = np.random.lognormal(2, 0.4, (30, n_genes))
    mature_cells[:, 30:45] *= 2.5  # åé¢åŸºå› é«˜è¡¨è¾¾ï¼ˆæˆç†Ÿæ ‡è®°ï¼‰
    
    # åˆå¹¶æ•°æ®
    all_cells = np.vstack([stem_cells, diff_cells, mature_cells])
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾ï¼ˆç”¨äºéªŒè¯ï¼Œä½†ä¸ç”¨äºè®­ç»ƒï¼‰
    true_labels = np.array([0]*30 + [1]*40 + [2]*30)
    
    # åˆ›å»ºDataFrame
    cell_names = [f"Cell_{i:03d}" for i in range(1, 101)]
    df = pd.DataFrame(all_cells, columns=gene_names, index=cell_names)
    
    print(f"æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  ç»†èƒæ•°é‡: {df.shape[0]}")
    print(f"  åŸºå› æ•°é‡: {df.shape[1]}")
    print(f"  è¡¨è¾¾å€¼èŒƒå›´: [{df.values.min():.2f}, {df.values.max():.2f}]")
    
    return df, true_labels


def perform_clustering_analysis(df, true_labels):
    """
    æ‰§è¡Œå¤šç§èšç±»åˆ†æ
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    èšç±»åˆ†æå¦‚åŒç»†èƒåˆ†ç±»å­¦ï¼š
    â€¢ K-meansï¼šæ‰¾åˆ°ç»†èƒç¾¤çš„"ä»£è¡¨ç»†èƒ"
    â€¢ å±‚æ¬¡èšç±»ï¼šæ„å»ºç»†èƒè¿›åŒ–æ ‘
    â€¢ DBSCANï¼šè¯†åˆ«å¯†é›†ç»†èƒå›¢å’Œå¼‚å¸¸ç»†èƒ
    â€¢ é«˜æ–¯æ··åˆï¼šç»†èƒç¾¤çš„æ¦‚ç‡è¾¹ç•Œ
    
    ğŸ”¬ è¯„ä¼°æ ‡å‡†ï¼š
    â€¢ è½®å»“ç³»æ•°ï¼šç¾¤å†…ç´§å¯†ï¼Œç¾¤é—´åˆ†ç¦»
    â€¢ è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼šä¸çœŸå®åˆ†ç»„çš„ä¸€è‡´æ€§
    """
    print_section("èšç±»åˆ†æ")
    
    # æ•°æ®é¢„å¤„ç†
    # 1. å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†åæ€åˆ†å¸ƒï¼‰
    df_log = np.log1p(df)
    
    # 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_log)
    
    # 3. PCAé™ç»´ï¼ˆå¯è§†åŒ–ç”¨ï¼‰
    pca = PCA(n_components=2, random_state=42)
    df_pca = pca.fit_transform(df_scaled)
    
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.1%}")
    
    # å°è¯•ä¸åŒçš„èšç±»ç®—æ³•
    clustering_methods = {
        'K-Means': KMeans(n_clusters=3, random_state=42),
        'DBSCAN': DBSCAN(eps=3, min_samples=5),
        'Hierarchical': AgglomerativeClustering(n_clusters=3),
        'GMM': GaussianMixture(n_components=3, random_state=42)
    }
    
    results = {}
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (name, clusterer) in enumerate(clustering_methods.items()):
        print(f"\næ‰§è¡Œ {name} èšç±»...")
        
        # èšç±»
        if name == 'GMM':
            clusterer.fit(df_scaled)
            labels = clusterer.predict(df_scaled)
        else:
            labels = clusterer.fit_predict(df_scaled)
        
        # è¯„ä¼°
        if len(np.unique(labels)) > 1:
            silhouette = silhouette_score(df_scaled, labels)
            ari = adjusted_rand_score(true_labels, labels)
        else:
            silhouette = -1
            ari = -1
        
        results[name] = {
            'labels': labels,
            'silhouette': silhouette,
            'ari': ari,
            'n_clusters': len(np.unique(labels[labels >= 0]))
        }
        
        print(f"  èšç±»æ•°: {results[name]['n_clusters']}")
        print(f"  è½®å»“ç³»æ•°: {silhouette:.3f}")
        print(f"  è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari:.3f}")
        
        # å¯è§†åŒ–
        ax = axes[idx]
        scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                           c=labels, cmap='viridis', 
                           alpha=0.6, s=30)
        ax.set_title(f'{name}\nSilhouette: {silhouette:.3f}')
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        plt.colorbar(scatter, ax=ax)
    
    # çœŸå®æ ‡ç­¾
    ax = axes[4]
    scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                       c=true_labels, cmap='Set1', 
                       alpha=0.6, s=30)
    ax.set_title('True Labels')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    plt.colorbar(scatter, ax=ax)
    
    # ç©ºç™½
    axes[5].axis('off')
    
    plt.tight_layout()
    plt.savefig('clustering_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    return results, df_scaled, pca


def determine_optimal_clusters(df_scaled):
    """
    ç¡®å®šæœ€ä¼˜èšç±»æ•°
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç¡®å®šç»„ç»‡ä¸­æœ‰å‡ ç§ä¸åŒçš„ç»†èƒç±»å‹
    """
    print_section("ç¡®å®šæœ€ä¼˜èšç±»æ•°")
    
    k_range = range(2, 11)
    inertias = []
    silhouettes = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(df_scaled)
        
        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(df_scaled, labels))
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # è‚˜éƒ¨æ³•åˆ™
    axes[0].plot(k_range, inertias, 'bo-')
    axes[0].set_xlabel('Number of Clusters (k)')
    axes[0].set_ylabel('Inertia')
    axes[0].set_title('Elbow Method')
    axes[0].grid(True, alpha=0.3)
    
    # è½®å»“ç³»æ•°
    axes[1].plot(k_range, silhouettes, 'ro-')
    axes[1].set_xlabel('Number of Clusters (k)')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].set_title('Silhouette Analysis')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('optimal_k.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    optimal_k = k_range[np.argmax(silhouettes)]
    print(f"åŸºäºè½®å»“ç³»æ•°ï¼Œæœ€ä¼˜èšç±»æ•°ä¸º: {optimal_k}")
    
    return optimal_k


def create_cluster_heatmap(df, labels):
    """åˆ›å»ºèšç±»çƒ­å›¾"""
    print("\nåˆ›å»ºèšç±»çƒ­å›¾")
    
    # æŒ‰èšç±»æ ‡ç­¾æ’åº
    sorted_idx = np.argsort(labels)
    df_sorted = df.iloc[sorted_idx]
    labels_sorted = labels[sorted_idx]
    
    # é€‰æ‹©å·®å¼‚æœ€å¤§çš„åŸºå› 
    gene_std = df.std(axis=0)
    top_genes = gene_std.nlargest(20).index
    
    # åˆ›å»ºçƒ­å›¾
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºèšç±»é¢œè‰²æ¡
    cluster_colors = ['red', 'blue', 'green', 'orange', 'purple']
    row_colors = [cluster_colors[label] if label >= 0 else 'gray' 
                  for label in labels_sorted]
    
    # ç»˜åˆ¶çƒ­å›¾
    g = sns.clustermap(df_sorted[top_genes], 
                       row_colors=row_colors,
                       cmap='RdBu_r',
                       center=0,
                       row_cluster=False,
                       col_cluster=True,
                       figsize=(10, 8),
                       cbar_kws={'label': 'Expression'})
    
    g.ax_heatmap.set_xlabel('Genes')
    g.ax_heatmap.set_ylabel('Cells')
    plt.suptitle('Cell Clustering Heatmap', y=1.02)
    
    plt.savefig('cluster_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 3: ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–
# ====================

def demonstrate_feature_engineering():
    """
    æ¼”ç¤ºç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    ç‰¹å¾å·¥ç¨‹ = é€‰æ‹©æœ€ä½³ç”Ÿç‰©æ ‡è®°ç‰©
    å°±åƒé€‰æ‹©æœ€èƒ½ä»£è¡¨ç–¾ç—…çš„æ£€æµ‹æŒ‡æ ‡
    
    ğŸ”¬ ç”Ÿç‰©ç‰¹å¾ç±»å‹ï¼š
    â€¢ åºåˆ—ç‰¹å¾ï¼šGCå«é‡ã€k-meré¢‘ç‡ã€ä¿å®ˆåŸŸ
    â€¢ ç»“æ„ç‰¹å¾ï¼šäºŒçº§ç»“æ„ã€æº¶å‰‚å¯åŠæ€§  
    â€¢ è¡¨è¾¾ç‰¹å¾ï¼šåŸºå› å…±è¡¨è¾¾ã€æ—¶åºåŠ¨æ€
    â€¢ ç½‘ç»œç‰¹å¾ï¼šè›‹ç™½ç›¸äº’ä½œç”¨ã€é€šè·¯å‚ä¸
    â€¢ ç»„åˆç‰¹å¾ï¼šç‰¹å¾äº¤äº’ã€æ¯”å€¼ã€å¤šé¡¹å¼
    
    å·¥ç¨‹è‰ºæœ¯ï¼š
    å¥½çš„ç‰¹å¾è®©å¤æ‚é—®é¢˜å˜ç®€å•ï¼
    """
    print_section("ç‰¹å¾å·¥ç¨‹çš„è‰ºæœ¯")
    
    print("""
ç‰¹å¾å·¥ç¨‹åœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨ï¼š

1. åºåˆ—ç‰¹å¾ï¼š
   - k-meré¢‘ç‡ï¼ˆDNA/è›‹ç™½è´¨motifï¼‰
   - GCå«é‡ã€å¯†ç å­ä½¿ç”¨åå¥½
   - äºŒçº§ç»“æ„é¢„æµ‹åˆ†æ•°

2. ç»“æ„ç‰¹å¾ï¼š
   - æ°¨åŸºé…¸ç†åŒ–æ€§è´¨
   - æº¶å‰‚å¯åŠæ€§
   - äºŒé¢è§’

3. ç½‘ç»œç‰¹å¾ï¼š
   - åŸºå› å…±è¡¨è¾¾
   - è›‹ç™½è´¨ç›¸äº’ä½œç”¨
   - ä»£è°¢é€šè·¯å‚ä¸åº¦

4. ç»„åˆç‰¹å¾ï¼š
   - ç‰¹å¾äº¤äº’é¡¹
   - å¤šé¡¹å¼ç‰¹å¾
   - ç‰¹å¾æ¯”ç‡
    """)
    
    # åˆ›å»ºç¤ºä¾‹ï¼šåºåˆ—ç‰¹å¾æå–
    def extract_sequence_features(sequence):
        """ä»DNAåºåˆ—æå–ç‰¹å¾"""
        features = {}
        
        # GCå«é‡
        gc_count = sequence.count('G') + sequence.count('C')
        features['gc_content'] = gc_count / len(sequence)
        
        # k-meré¢‘ç‡ï¼ˆ2-merä¸ºä¾‹ï¼‰
        kmers = ['AA', 'AT', 'AG', 'AC', 'TA', 'TT', 'TG', 'TC',
                 'GA', 'GT', 'GG', 'GC', 'CA', 'CT', 'CG', 'CC']
        for kmer in kmers:
            features[f'kmer_{kmer}'] = sequence.count(kmer) / (len(sequence) - 1)
        
        # åºåˆ—é•¿åº¦
        features['length'] = len(sequence)
        
        return features
    
    # ç¤ºä¾‹åºåˆ—
    sequences = [
        "ATGCGATCGTAGCTAGCTAGCTAGCTAGCTA",
        "GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCG",
        "ATATATATATATATATATATATATATATAT"
    ]
    
    print("\nç¤ºä¾‹ï¼šDNAåºåˆ—ç‰¹å¾æå–")
    for i, seq in enumerate(sequences, 1):
        features = extract_sequence_features(seq)
        print(f"\nåºåˆ—{i}: {seq[:20]}...")
        print(f"  GCå«é‡: {features['gc_content']:.2f}")
        print(f"  é•¿åº¦: {features['length']}")
        print(f"  ATäºŒèšä½“é¢‘ç‡: {features['kmer_AT']:.2f}")


def hyperparameter_tuning_demo():
    """
    æ¼”ç¤ºè¶…å‚æ•°è°ƒä¼˜
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    è¶…å‚æ•°è°ƒä¼˜å¦‚åŒä¼˜åŒ–å®éªŒæ¡ä»¶ï¼š
    â€¢ PCRï¼šæ¸©åº¦ã€æ—¶é—´ã€å¼•ç‰©æµ“åº¦
    â€¢ ç»†èƒåŸ¹å…»ï¼šæ¸©åº¦ã€CO2æµ“åº¦ã€åŸ¹å…»åŸº
    â€¢ æµ‹åºï¼šæ·±åº¦ã€è¯»é•¿ã€è´¨é‡é˜ˆå€¼
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    â€¢ ç½‘æ ¼æœç´¢ï¼šç©·å°½æ‰€æœ‰ç»„åˆï¼ˆè€—æ—¶ï¼‰
    â€¢ éšæœºæœç´¢ï¼šéšæœºé‡‡æ ·ï¼ˆé«˜æ•ˆï¼‰
    â€¢ è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå°è¯•
    """
    print_section("è¶…å‚æ•°è°ƒä¼˜")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=200, n_features=10, 
                              n_informative=5, n_redundant=5,
                              n_classes=2, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    print("ç½‘æ ¼æœç´¢è¶…å‚æ•°...")
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {np.prod([len(v) for v in param_grid.values()])}")
    
    # ç½‘æ ¼æœç´¢
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=5, 
                               scoring='f1', n_jobs=-1)
    
    grid_search.fit(X_train, y_train)
    
    print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.3f}")
    
    # æµ‹è¯•é›†æ€§èƒ½
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_score = f1_score(y_test, y_pred)
    print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_score:.3f}")
    
    # å¯è§†åŒ–å‚æ•°å½±å“
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å‚æ•°é‡è¦æ€§
    param_importance = []
    for param in ['n_estimators', 'max_depth']:
        param_values = results_df[f'param_{param}'].unique()
        mean_scores = []
        for val in param_values:
            mask = results_df[f'param_{param}'] == val
            mean_scores.append(results_df[mask]['mean_test_score'].mean())
        
        ax = axes[0] if param == 'n_estimators' else axes[1]
        ax.plot(param_values, mean_scores, 'o-')
        ax.set_xlabel(param)
        ax.set_ylabel('Mean CV Score')
        ax.set_title(f'Effect of {param}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('hyperparameter_tuning.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 4: æ·±åº¦å­¦ä¹ åˆæ¢
# ====================

def introduction_to_deep_learning():
    """
    æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åº”ç”¨ä»‹ç»
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    æ·±åº¦å­¦ä¹  = å¤§è„‘çš„åˆ†å±‚å¤„ç†æœºåˆ¶
    â€¢ ç¬¬ä¸€å±‚ï¼šè¯†åˆ«è¾¹ç¼˜ï¼ˆç»†èƒè¾¹ç•Œï¼‰
    â€¢ ç¬¬äºŒå±‚ï¼šè¯†åˆ«å½¢çŠ¶ï¼ˆç»†èƒæ ¸ï¼‰
    â€¢ ç¬¬ä¸‰å±‚ï¼šè¯†åˆ«æ¨¡å¼ï¼ˆç™Œç»†èƒç‰¹å¾ï¼‰
    â€¢ è¾“å‡ºå±‚ï¼šåšå‡ºåˆ¤æ–­ï¼ˆè¯Šæ–­ç»“æœï¼‰
    
    ğŸš€ é©å‘½æ€§çªç ´ï¼š
    â€¢ AlphaFold2ï¼šç ´è§£50å¹´è›‹ç™½è´¨ç»“æ„éš¾é¢˜  
    â€¢ åŸºå› è¡¨è¾¾é¢„æµ‹ï¼šä»åºåˆ—é¢„æµ‹è¡¨è¾¾æ¨¡å¼
    â€¢ è¯ç‰©è®¾è®¡ï¼šç§’çº§ç”Ÿæˆå€™é€‰è¯ç‰©åˆ†å­
    â€¢ ç»†èƒå›¾åƒåˆ†æï¼šè¶…è¶Šäººç±»ä¸“å®¶ç²¾åº¦
    """
    print_section("æ·±åº¦å­¦ä¹ åˆæ¢")
    
    print("""
æ·±åº¦å­¦ä¹ ï¼šç”Ÿç‰©ä¿¡æ¯å­¦çš„æ–°çºªå…ƒ

çªç ´æ€§åº”ç”¨ï¼š

1. AlphaFold2 - ç ´è§£50å¹´è›‹ç™½è´¨ç»“æ„éš¾é¢˜
   - è¾“å…¥ï¼šæ°¨åŸºé…¸åºåˆ— â†’ è¾“å‡ºï¼š3DåŸå­ç»“æ„
   - å‡†ç¡®åº¦ï¼šåŸå­çº§ç²¾åº¦ï¼ˆGDT > 90ï¼‰
   - å½±å“ï¼šåŠ é€Ÿè¯ç‰©ç ”å‘10å€

2. Enformer - åŸºå› è¡¨è¾¾é¢„æµ‹å¤§æ¨¡å‹
   - è¾“å…¥ï¼š200kb DNAåºåˆ— â†’ è¾“å‡ºï¼šè¡¨è¾¾æ¨¡å¼  
   - åº”ç”¨ï¼šç†è§£åŸºå› è°ƒæ§æœºåˆ¶
   - æ„ä¹‰ï¼šè§£å¯†éç¼–ç åŒºåŸŸåŠŸèƒ½

3. è¯ç‰©è®¾è®¡ - ç”Ÿæˆå¼æ¨¡å‹
   - è¾“å…¥ï¼šé¶ç‚¹ç»“æ„ â†’ è¾“å‡ºï¼šå€™é€‰è¯ç‰©åˆ†å­
   - é€Ÿåº¦ï¼šç§’çº§ç”Ÿæˆ vs æœˆçº§ç­›é€‰
   - æˆåŠŸæ¡ˆä¾‹ï¼šCOVID-19è¯ç‰©å‘ç°

4. ç»†èƒå›¾åƒåˆ†æ - è®¡ç®—ç—…ç†å­¦
   - è¾“å…¥ï¼šæ˜¾å¾®é•œå›¾åƒ â†’ è¾“å‡ºï¼šç»†èƒç±»å‹/çŠ¶æ€
   - ç²¾åº¦ï¼šè¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³
   - åº”ç”¨ï¼šç™Œç—‡æ—©æœŸè¯Šæ–­ã€è¯ç‰©ç­›é€‰

5. å•ç»†èƒæ·±åº¦å­¦ä¹  - scBERT/scGPT
   - è¾“å…¥ï¼šscRNA-seqæ•°æ® â†’ è¾“å‡ºï¼šç»†èƒæ³¨é‡Š
   - å‘ç°ï¼šæ–°çš„ç»†èƒäºšç¾¤å’Œè½¨è¿¹
   - é©å‘½ï¼šå•ç»†èƒç”Ÿç‰©å­¦æ–°èŒƒå¼
    """)
    
    # ç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼ˆä½¿ç”¨sklearnï¼‰
    from sklearn.neural_network import MLPClassifier
    
    print("\nç®€å•ç¥ç»ç½‘ç»œç¤ºä¾‹")
    
    # åˆ›å»ºXORé—®é¢˜ï¼ˆéçº¿æ€§å¯åˆ†ï¼‰
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XORè¾“å‡º
    
    # è®­ç»ƒç¥ç»ç½‘ç»œ
    mlp = MLPClassifier(hidden_layer_sizes=(4,), 
                        activation='relu',
                        max_iter=1000,
                        random_state=42)
    mlp.fit(X, y)
    
    # é¢„æµ‹
    predictions = mlp.predict(X)
    
    print("XORé—®é¢˜ï¼ˆéœ€è¦éçº¿æ€§è¾¹ç•Œï¼‰ï¼š")
    print("è¾“å…¥ -> é¢„æœŸè¾“å‡º -> é¢„æµ‹è¾“å‡º")
    for i in range(len(X)):
        print(f"{X[i]} -> {y[i]} -> {predictions[i]}")
    
    # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # çº¿æ€§æ¨¡å‹ï¼ˆå¤±è´¥ï¼‰
    ax = axes[0]
    log_reg = LogisticRegression()
    log_reg.fit(X, y)
    
    xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),
                         np.linspace(-0.5, 1.5, 100))
    Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Linear Model (Fails)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    # ç¥ç»ç½‘ç»œï¼ˆæˆåŠŸï¼‰
    ax = axes[1]
    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Neural Network (Success)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    plt.tight_layout()
    plt.savefig('neural_network_xor.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 5: çœŸå®æ•°æ®å¤„ç† - å®æˆ˜æŠ€èƒ½è®­ç»ƒ
# ====================

def handle_real_single_cell_data():
    """
    å¤„ç†çœŸå®çš„å•ç»†èƒRNA-seqæ•°æ®
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    å•ç»†èƒæµ‹åºæ•°æ®çš„ç‰¹ç‚¹ï¼š
    - é«˜ç»´ç¨€ç–ï¼š20000+ åŸºå› ï¼Œå¤§å¤šæ•°å€¼ä¸º0
    - æ‰¹æ¬¡æ•ˆåº”ï¼šä¸åŒå®éªŒæ‰¹æ¬¡é—´çš„ç³»ç»Ÿæ€§å·®å¼‚
    - ç»†èƒå¼‚è´¨æ€§ï¼šåŒç±»ç»†èƒé—´çš„è¡¨è¾¾å·®å¼‚
    - æŠ€æœ¯å™ªå£°ï¼šæµ‹åºæ·±åº¦ã€æ‰©å¢åå·®ç­‰
    
    ğŸ”¬ æ•™å­¦ç›®æ ‡ï¼š
    â€¢ å­¦ä¼šå¤„ç†çœŸå®ç”Ÿç‰©æ•°æ®çš„å¸¸è§æŒ‘æˆ˜
    â€¢ æŒæ¡æ•°æ®è´¨é‡æ§åˆ¶æµç¨‹
    â€¢ ç†è§£æ‰¹æ¬¡æ•ˆåº”åŠå…¶æ ¡æ­£æ–¹æ³•
    â€¢ åº”ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œç»†èƒåˆ†å‹
    """
    print_section("Part 5.1: çœŸå®å•ç»†èƒæ•°æ®å¤„ç†")
    
    import os
    
    # è¯»å–çœŸå®å•ç»†èƒæ•°æ®
    data_file = os.path.join("..", "data", "single_cell_sample.csv")
    
    try:
        print("è¯»å–çœŸå®å•ç»†èƒæ•°æ®...")
        df_sc = pd.read_csv(data_file)
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: {df_sc.shape[0]} ç»†èƒ Ã— {df_sc.shape[1]} ç‰¹å¾")
    except FileNotFoundError:
        print("æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡çœŸå®æ•°æ®åˆ†æ")
        return
    
    # æ•°æ®é¢„å¤„ç†
    print(f"\næ•°æ®é¢„è§ˆ:")
    print(df_sc.head())
    
    # åˆ†ç¦»å…ƒæ•°æ®å’Œè¡¨è¾¾æ•°æ®
    meta_cols = ['cell_id', 'batch', 'cell_type']
    gene_cols = [col for col in df_sc.columns if col not in meta_cols]
    
    print(f"\næ•°æ®ç»“æ„åˆ†æ:")
    print(f"  - å…ƒæ•°æ®åˆ—: {len(meta_cols)} ä¸ª ({meta_cols})")
    print(f"  - åŸºå› è¡¨è¾¾åˆ—: {len(gene_cols)} ä¸ª")
    print(f"  - ç»†èƒç±»å‹: {df_sc['cell_type'].nunique()} ç§")
    print(f"  - å®éªŒæ‰¹æ¬¡: {df_sc['batch'].nunique()} ä¸ª")
    
    # æ•°æ®è´¨é‡æ§åˆ¶
    print(f"\næ•°æ®è´¨é‡æ§åˆ¶:")
    
    # 1. è¡¨è¾¾é‡ç»Ÿè®¡
    expression_data = df_sc[gene_cols]
    total_counts = expression_data.sum(axis=1)
    detected_genes = (expression_data > 0).sum(axis=1)
    
    print(f"æ¯ç»†èƒç»Ÿè®¡:")
    print(f"  - å¹³å‡æ€»è¡¨è¾¾é‡: {total_counts.mean():.1f} Â± {total_counts.std():.1f}")
    print(f"  - å¹³å‡æ£€æµ‹åŸºå› æ•°: {detected_genes.mean():.1f} Â± {detected_genes.std():.1f}")
    print(f"  - æ•°æ®ç¨€ç–åº¦: {(expression_data == 0).sum().sum() / expression_data.size * 100:.1f}%")
    
    # 2. æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹
    print(f"\nğŸ”¬ æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹:")
    batch_stats = df_sc.groupby('batch').agg({
        gene_cols[0]: 'mean',  # ç”¨ç¬¬ä¸€ä¸ªåŸºå› ä½œä¸ºä¾‹å­
        'cell_type': 'count'
    })
    print("å„æ‰¹æ¬¡ç»Ÿè®¡:")
    print(batch_stats)
    
    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆlog1på˜æ¢ + z-scoreï¼‰
    print(f"\næ•°æ®æ ‡å‡†åŒ–:")
    # 1. log1på˜æ¢å¤„ç†åæ€åˆ†å¸ƒ
    expression_log = np.log1p(expression_data)
    
    # 2. z-scoreæ ‡å‡†åŒ–
    scaler = StandardScaler()
    expression_scaled = scaler.fit_transform(expression_log)
    
    print("æ ‡å‡†åŒ–æ­¥éª¤:")
    print("  1. log1på˜æ¢: å¤„ç†æ•°æ®åæ€åˆ†å¸ƒ")
    print("  2. z-scoreæ ‡å‡†åŒ–: æ¶ˆé™¤é‡çº²å½±å“")
    print(f"  3. æ ‡å‡†åŒ–åèŒƒå›´: [{expression_scaled.min():.2f}, {expression_scaled.max():.2f}]")
    
    # é™ç»´åˆ†æï¼ˆPCA + t-SNEï¼‰
    print(f"\né™ç»´åˆ†æ:")
    
    # PCAé™ç»´
    pca = PCA(n_components=10)
    pca_result = pca.fit_transform(expression_scaled)
    
    print(f"PCAåˆ†æ:")
    print(f"  - å‰3ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:3].sum():.2f}")
    print(f"  - å‰5ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:5].sum():.2f}")
    
    # t-SNEå¯è§†åŒ–
    if len(df_sc) > 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(df_sc)-1))
        tsne_result = tsne.fit_transform(pca_result[:, :5])  # å…ˆPCAå†t-SNE
        
        # å¯è§†åŒ–ç»“æœ
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('å•ç»†èƒæ•°æ®åˆ†æç»“æœ', fontsize=14)
        
        # 1. PCAç»“æœï¼ˆæŒ‰ç»†èƒç±»å‹ç€è‰²ï¼‰
        ax = axes[0, 0]
        unique_types = df_sc['cell_type'].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))
        
        for cell_type, color in zip(unique_types, colors):
            mask = df_sc['cell_type'] == cell_type
            ax.scatter(pca_result[mask, 0], pca_result[mask, 1], 
                      c=[color], label=cell_type, alpha=0.7, s=50)
        ax.set_title('PCA - æŒ‰ç»†èƒç±»å‹')
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # 2. PCAç»“æœï¼ˆæŒ‰æ‰¹æ¬¡ç€è‰²ï¼‰
        ax = axes[0, 1]
        unique_batches = df_sc['batch'].unique()
        batch_colors = plt.cm.Set1(np.linspace(0, 1, len(unique_batches)))
        
        for batch, color in zip(unique_batches, batch_colors):
            mask = df_sc['batch'] == batch
            ax.scatter(pca_result[mask, 0], pca_result[mask, 1], 
                      c=[color], label=batch, alpha=0.7, s=50)
        ax.set_title('PCA - æŒ‰å®éªŒæ‰¹æ¬¡')
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
        ax.legend()
        
        # 3. t-SNEç»“æœï¼ˆæŒ‰ç»†èƒç±»å‹ï¼‰
        ax = axes[1, 0]
        for cell_type, color in zip(unique_types, colors):
            mask = df_sc['cell_type'] == cell_type
            ax.scatter(tsne_result[mask, 0], tsne_result[mask, 1], 
                      c=[color], label=cell_type, alpha=0.7, s=50)
        ax.set_title('t-SNE - æŒ‰ç»†èƒç±»å‹')
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # 4. t-SNEç»“æœï¼ˆæŒ‰æ‰¹æ¬¡ï¼‰
        ax = axes[1, 1]
        for batch, color in zip(unique_batches, batch_colors):
            mask = df_sc['batch'] == batch
            ax.scatter(tsne_result[mask, 0], tsne_result[mask, 1], 
                      c=[color], label=batch, alpha=0.7, s=50)
        ax.set_title('t-SNE - æŒ‰å®éªŒæ‰¹æ¬¡')
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig('single_cell_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    # ç»†èƒåˆ†å‹é¢„æµ‹
    print(f"\nç»†èƒåˆ†å‹é¢„æµ‹:")
    
    # å‡†å¤‡æ•°æ®
    X = expression_scaled
    y = df_sc['cell_type']
    
    # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨
    rf_classifier = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    )
    
    rf_classifier.fit(X_train, y_train)
    y_pred = rf_classifier.predict(X_test)
    
    # è¯„ä¼°ç»“æœ
    accuracy = accuracy_score(y_test, y_pred)
    print(f"åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.3f}")
    
    if len(y_test) > 0:
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print(f"\né‡è¦åŸºå› æ ‡è®°:")
    feature_importance = pd.DataFrame({
        'gene': gene_cols,
        'importance': rf_classifier.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("Top 5 é‡è¦åŸºå› :")
    print(feature_importance.head().to_string(index=False))
    
    print(f"\nç”Ÿç‰©å­¦è§£é‡Š:")
    print("â€¢ CD3D, CD4: Tç»†èƒæ ‡è®°åŸºå› ")
    print("â€¢ MS4A1, CD19: Bç»†èƒæ ‡è®°åŸºå› ") 
    print("â€¢ LYZ, CSF1R: å•æ ¸ç»†èƒ/å·¨å™¬ç»†èƒæ ‡è®°")
    print("â€¢ NKG7, KLRD1: NKç»†èƒæ ‡è®°")
    print("â€¢ ç‰¹å¾é‡è¦æ€§åæ˜ åŸºå› åœ¨ç»†èƒåˆ†å‹ä¸­çš„è´¡çŒ®")


def handle_cancer_clinical_data():
    """
    å¤„ç†ç™Œç—‡ä¸´åºŠæ•°æ®è¿›è¡Œé¢„åé¢„æµ‹
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    ç™Œç—‡é¢„åé¢„æµ‹æ˜¯ç²¾å‡†åŒ»ç–—çš„é‡è¦åº”ç”¨ï¼š
    - ä¸´åºŠç‰¹å¾ï¼šå¹´é¾„ã€æ€§åˆ«ã€åˆ†æœŸã€åˆ†çº§
    - åˆ†å­æ ‡è®°ï¼šå…³é”®åŸºå› è¡¨è¾¾æ°´å¹³
    - æ²»ç–—ä¿¡æ¯ï¼šæ‰‹æœ¯ã€åŒ–ç–—ã€æ”¾ç–—æ–¹æ¡ˆ
    - ç”Ÿå­˜æ•°æ®ï¼šç”Ÿå­˜æ—¶é—´ã€ç”Ÿå­˜çŠ¶æ€
    
    ğŸ”¬ æ•™å­¦ç›®æ ‡ï¼š
    â€¢ å­¦ä¼šå¤„ç†æ··åˆæ•°æ®ç±»å‹ï¼ˆæ•°å€¼ã€åˆ†ç±»ï¼‰
    â€¢ ç†è§£ç‰¹å¾å·¥ç¨‹åœ¨ä¸´åºŠæ•°æ®ä¸­çš„åº”ç”¨
    â€¢ æŒæ¡ç”Ÿå­˜é¢„æµ‹çš„å»ºæ¨¡ç­–ç•¥
    â€¢ è§£é‡Šæ¨¡å‹ç»“æœçš„ä¸´åºŠæ„ä¹‰
    """
    print_section("Part 5.2: ç™Œç—‡ä¸´åºŠæ•°æ®åˆ†æ")
    
    import os
    
    # è¯»å–ç™Œç—‡ä¸´åºŠæ•°æ®
    data_file = os.path.join("..", "data", "cancer_clinical_data.csv")
    
    try:
        print("è¯»å–ç™Œç—‡ä¸´åºŠæ•°æ®...")
        df_cancer = pd.read_csv(data_file)
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: {df_cancer.shape[0]} æ‚£è€… Ã— {df_cancer.shape[1]} ç‰¹å¾")
    except FileNotFoundError:
        print("æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡ç™Œç—‡æ•°æ®åˆ†æ")
        return
    
    # æ•°æ®é¢„è§ˆ
    print(f"\næ•°æ®é¢„è§ˆ:")
    print(df_cancer.head())
    
    # æ•°æ®ç»“æ„åˆ†æ
    print(f"\næ•°æ®ç»“æ„åˆ†æ:")
    clinical_cols = ['age', 'gender', 'stage', 'grade', 'treatment']
    survival_cols = ['survival_months', 'vital_status']
    gene_cols = [col for col in df_cancer.columns 
                 if col not in clinical_cols + survival_cols + ['patient_id']]
    
    print(f"  - ä¸´åºŠç‰¹å¾: {len(clinical_cols)} ä¸ª")
    print(f"  - ç”Ÿå­˜ä¿¡æ¯: {len(survival_cols)} ä¸ª")
    print(f"  - åŸºå› è¡¨è¾¾: {len(gene_cols)} ä¸ª")
    
    # æè¿°æ€§ç»Ÿè®¡
    print(f"\nğŸ“ˆ æè¿°æ€§ç»Ÿè®¡:")
    print(f"æ‚£è€…ç‰¹å¾:")
    print(f"  - å¹´é¾„èŒƒå›´: {df_cancer['age'].min()}-{df_cancer['age'].max()} å²")
    print(f"  - æ€§åˆ«åˆ†å¸ƒ: {df_cancer['gender'].value_counts().to_dict()}")
    print(f"  - åˆ†æœŸåˆ†å¸ƒ: {df_cancer['stage'].value_counts().to_dict()}")
    print(f"  - ç”Ÿå­˜çŠ¶æ€: {df_cancer['vital_status'].value_counts().to_dict()}")
    print(f"  - å¹³å‡ç”Ÿå­˜æ—¶é—´: {df_cancer['survival_months'].mean():.1f} æœˆ")
    
    # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    print(f"\nâš¡ æ•°æ®é¢„å¤„ç†:")
    
    # 1. ç¼–ç åˆ†ç±»å˜é‡
    le_gender = LabelEncoder()
    le_stage = LabelEncoder()
    le_grade = LabelEncoder()
    le_treatment = LabelEncoder()
    le_vital = LabelEncoder()
    
    df_processed = df_cancer.copy()
    df_processed['gender_encoded'] = le_gender.fit_transform(df_cancer['gender'])
    df_processed['stage_encoded'] = le_stage.fit_transform(df_cancer['stage'])
    df_processed['grade_encoded'] = le_grade.fit_transform(df_cancer['grade'])
    df_processed['treatment_encoded'] = le_treatment.fit_transform(df_cancer['treatment'])
    df_processed['vital_status_encoded'] = le_vital.fit_transform(df_cancer['vital_status'])
    
    print("åˆ†ç±»å˜é‡ç¼–ç å®Œæˆ:")
    print(f"  - æ€§åˆ«: {list(le_gender.classes_)}")
    print(f"  - åˆ†æœŸ: {list(le_stage.classes_)}")
    print(f"  - æ²»ç–—: {list(le_treatment.classes_)}")
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print(f"\nç‰¹å¾å·¥ç¨‹:")
    
    # åˆ›å»ºç»„åˆç‰¹å¾
    df_processed['age_stage_score'] = df_processed['age'] * df_processed['stage_encoded']
    df_processed['gene_risk_score'] = df_processed[['TP53', 'BRCA1', 'EGFR']].mean(axis=1)
    df_processed['oncogene_score'] = df_processed[['MYC', 'KRAS', 'PIK3CA']].mean(axis=1)
    
    # åˆ›å»ºå¹´é¾„ç»„
    df_processed['age_group'] = pd.cut(df_processed['age'], 
                                      bins=[0, 45, 60, 100], 
                                      labels=['Young', 'Middle', 'Old'])
    df_processed['age_group_encoded'] = LabelEncoder().fit_transform(df_processed['age_group'])
    
    print("æ–°ç‰¹å¾åˆ›å»º:")
    print("  - age_stage_score: å¹´é¾„-åˆ†æœŸäº¤äº’ç‰¹å¾")
    print("  - gene_risk_score: è‚¿ç˜¤æŠ‘åˆ¶åŸºå› è¯„åˆ†")
    print("  - oncogene_score: ç™ŒåŸºå› è¯„åˆ†")
    print("  - age_group: å¹´é¾„åˆ†ç»„")
    
    # 3. ç‰¹å¾é€‰æ‹©
    feature_cols = (['age', 'gender_encoded', 'stage_encoded', 'grade_encoded', 
                    'treatment_encoded'] + gene_cols + 
                    ['age_stage_score', 'gene_risk_score', 'oncogene_score', 'age_group_encoded'])
    
    X = df_processed[feature_cols]
    y_survival = df_processed['vital_status_encoded']  # ç”Ÿå­˜çŠ¶æ€é¢„æµ‹
    y_time = df_processed['survival_months']          # ç”Ÿå­˜æ—¶é—´é¢„æµ‹
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print(f"\næœ€ç»ˆç‰¹å¾çŸ©é˜µ: {X_scaled.shape}")
    
    # ç”Ÿå­˜çŠ¶æ€é¢„æµ‹
    print(f"\nç”Ÿå­˜çŠ¶æ€é¢„æµ‹æ¨¡å‹:")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_survival, test_size=0.3, random_state=42, stratify=y_survival
    )
    
    # æ¯”è¾ƒå¤šç§æ¨¡å‹
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    results = {}
    for name, model in models.items():
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
        
        results[name] = {
            'accuracy': accuracy,
            'f1_score': f1,
            'auc': auc,
            'model': model
        }
        
        print(f"{name}:")
        print(f"  - å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  - F1åˆ†æ•°: {f1:.3f}")
        if auc:
            print(f"  - AUC: {auc:.3f}")
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰
    best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
    best_model = results[best_model_name]['model']
    
    print(f"\nç‰¹å¾é‡è¦æ€§åˆ†æ (åŸºäº{best_model_name}):")
    
    if hasattr(best_model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("Top 10 é‡è¦ç‰¹å¾:")
        print(importance_df.head(10).to_string(index=False))
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(10, 6))
        top_features = importance_df.head(10)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('ç‰¹å¾é‡è¦æ€§')
        plt.title(f'ç‰¹å¾é‡è¦æ€§æ’åº ({best_model_name})')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig('feature_importance_cancer.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    # ç”Ÿå­˜æ›²çº¿åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
    print(f"\nç”Ÿå­˜åˆ†æ:")
    
    # æŒ‰é£é™©è¯„åˆ†åˆ†ç»„
    risk_scores = best_model.predict_proba(X_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else best_model.decision_function(X_scaled)
    df_processed['risk_score'] = risk_scores
    
    # åˆ†ä¸ºé«˜é£é™©å’Œä½é£é™©ç»„
    risk_threshold = np.median(risk_scores)
    df_processed['risk_group'] = df_processed['risk_score'].apply(
        lambda x: 'High Risk' if x > risk_threshold else 'Low Risk'
    )
    
    print("é£é™©åˆ†å±‚ç»“æœ:")
    risk_summary = df_processed.groupby('risk_group').agg({
        'survival_months': ['count', 'mean', 'std'],
        'vital_status': lambda x: (x == 'Dead').sum()
    })
    print(risk_summary)
    
    print(f"\nä¸´åºŠè§£é‡Š:")
    print("â€¢ åˆ†æœŸå’Œåˆ†çº§æ˜¯æœ€é‡è¦çš„é¢„åå› ç´ ")
    print("â€¢ TP53å’ŒBRCA1ç­‰è‚¿ç˜¤æŠ‘åˆ¶åŸºå› è¡¨è¾¾å½±å“é¢„å")
    print("â€¢ å¹´é¾„å’Œæ²»ç–—æ–¹æ¡ˆä¹Ÿå¯¹ç”Ÿå­˜æœ‰æ˜¾è‘—å½±å“")
    print("â€¢ å¤šåŸºå› è¯„åˆ†å¯ä»¥æ”¹å–„é¢„åé¢„æµ‹å‡†ç¡®æ€§")
    
    return df_processed


def demonstrate_batch_effect_correction():
    """
    æ¼”ç¤ºæ‰¹æ¬¡æ•ˆåº”æ£€æµ‹å’Œæ ¡æ­£
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    æ‰¹æ¬¡æ•ˆåº”æ˜¯ç”Ÿç‰©æ•°æ®åˆ†æä¸­çš„é‡è¦é—®é¢˜ï¼š
    - æŠ€æœ¯æ‰¹æ¬¡ï¼šä¸åŒå®éªŒæ‰¹æ¬¡ã€æ“ä½œäººå‘˜ã€è¯•å‰‚æ‰¹å·
    - ç”Ÿç‰©æ‰¹æ¬¡ï¼šä¸åŒæ—¶é—´ã€åœ°ç‚¹ã€ç§ç¾¤çš„æ ·æœ¬
    - ç³»ç»Ÿæ€§åå·®ï¼šå½±å“æ‰€æœ‰åŸºå› çš„éç”Ÿç‰©å­¦å› ç´ 
    
    ğŸ”¬ å¤„ç†ç­–ç•¥ï¼š
    â€¢ æ£€æµ‹ï¼šPCAåˆ†æã€èšç±»åˆ†æè§‚å¯Ÿæ‰¹æ¬¡èšé›†
    â€¢ æ ¡æ­£ï¼šComBatç®—æ³•ã€çº¿æ€§æ··åˆæ¨¡å‹
    â€¢ éªŒè¯ï¼šæ ¡æ­£å‰åæ•ˆæœå¯¹æ¯”
    """
    print_section("Part 5.3: æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹ä¸æ ¡æ­£")
    
    # åˆ›å»ºå¸¦æ‰¹æ¬¡æ•ˆåº”çš„æ¨¡æ‹Ÿæ•°æ®
    print("åˆ›å»ºæ‰¹æ¬¡æ•ˆåº”æ¨¡æ‹Ÿæ•°æ®...")
    
    np.random.seed(42)
    n_samples = 60
    n_genes = 20
    
    # åˆ›å»ºä¸¤ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    batch1_data = np.random.normal(5, 1, (30, n_genes))  # æ‰¹æ¬¡1ï¼šå‡å€¼5
    batch2_data = np.random.normal(7, 1, (30, n_genes))  # æ‰¹æ¬¡2ï¼šå‡å€¼7ï¼ˆç³»ç»Ÿæ€§å‡é«˜ï¼‰
    
    # æ·»åŠ ç”Ÿç‰©å­¦ä¿¡å·ï¼ˆéƒ¨åˆ†åŸºå› åœ¨ä¸åŒæ¡ä»¶ä¸‹å·®å¼‚è¡¨è¾¾ï¼‰
    condition1_indices = list(range(0, 15)) + list(range(30, 45))  # æ¡ä»¶1
    condition2_indices = list(range(15, 30)) + list(range(45, 60)) # æ¡ä»¶2
    
    # åœ¨éƒ¨åˆ†åŸºå› ä¸­æ·»åŠ æ¡ä»¶ç‰¹å¼‚æ€§ä¿¡å·
    for gene_idx in [0, 1, 2]:  # å‰3ä¸ªåŸºå› æœ‰æ¡ä»¶æ•ˆåº”
        batch1_data[15:30, gene_idx] += 2  # æ‰¹æ¬¡1ä¸­æ¡ä»¶2æ ·æœ¬ä¸Šè°ƒ
        batch2_data[15:30, gene_idx] += 2  # æ‰¹æ¬¡2ä¸­æ¡ä»¶2æ ·æœ¬ä¸Šè°ƒ
    
    # ç»„åˆæ•°æ®
    expression_data = np.vstack([batch1_data, batch2_data])
    
    # åˆ›å»ºå…ƒæ•°æ®
    batch_labels = ['Batch1'] * 30 + ['Batch2'] * 30
    condition_labels = (['Condition1'] * 15 + ['Condition2'] * 15) * 2
    
    df_batch = pd.DataFrame({
        'sample_id': [f'Sample_{i:03d}' for i in range(n_samples)],
        'batch': batch_labels,
        'condition': condition_labels,
        **{f'Gene_{i:02d}': expression_data[:, i] for i in range(n_genes)}
    })
    
    gene_cols = [f'Gene_{i:02d}' for i in range(n_genes)]
    
    print(f"æ•°æ®åˆ›å»ºå®Œæˆ: {n_samples} æ ·æœ¬ Ã— {n_genes} åŸºå› ")
    print(f"æ‰¹æ¬¡åˆ†å¸ƒ: {pd.Series(batch_labels).value_counts().to_dict()}")
    print(f"æ¡ä»¶åˆ†å¸ƒ: {pd.Series(condition_labels).value_counts().to_dict()}")
    
    # 1. æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹
    print(f"\næ‰¹æ¬¡æ•ˆåº”æ£€æµ‹:")
    
    # PCAåˆ†ææ£€æµ‹æ‰¹æ¬¡æ•ˆåº”
    pca = PCA(n_components=3)
    pca_result = pca.fit_transform(df_batch[gene_cols])
    
    print(f"PCAåˆ†æ:")
    print(f"  - PC1è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.3f}")
    print(f"  - PC2è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.3f}")
    print(f"  - å‰2ä¸ªPCç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:2].sum():.3f}")
    
    # å¯è§†åŒ–æ‰¹æ¬¡æ•ˆåº”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹ä¸æ ¡æ­£', fontsize=14)
    
    # æ ¡æ­£å‰ - æŒ‰æ‰¹æ¬¡ç€è‰²
    ax = axes[0, 0]
    for batch in df_batch['batch'].unique():
        mask = df_batch['batch'] == batch
        color = 'red' if batch == 'Batch1' else 'blue'
        ax.scatter(pca_result[mask, 0], pca_result[mask, 1], 
                  c=color, label=batch, alpha=0.7, s=50)
    ax.set_title('æ ¡æ­£å‰ - æŒ‰æ‰¹æ¬¡ç€è‰²')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
    ax.legend()
    
    # æ ¡æ­£å‰ - æŒ‰æ¡ä»¶ç€è‰²
    ax = axes[0, 1]
    for condition in df_batch['condition'].unique():
        mask = df_batch['condition'] == condition
        color = 'green' if condition == 'Condition1' else 'orange'
        ax.scatter(pca_result[mask, 0], pca_result[mask, 1], 
                  c=color, label=condition, alpha=0.7, s=50)
    ax.set_title('æ ¡æ­£å‰ - æŒ‰æ¡ä»¶ç€è‰²')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
    ax.legend()
    
    # 2. æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£ï¼ˆç®€åŒ–ç‰ˆComBatï¼‰
    print(f"\næ‰¹æ¬¡æ•ˆåº”æ ¡æ­£:")
    
    # ç®€åŒ–çš„æ‰¹æ¬¡æ ¡æ­£ï¼šz-scoreæ ‡å‡†åŒ– + æ‰¹æ¬¡å‡å€¼å±…ä¸­
    corrected_data = df_batch[gene_cols].copy()
    
    # æŒ‰æ‰¹æ¬¡è¿›è¡Œz-scoreæ ‡å‡†åŒ–
    for batch in df_batch['batch'].unique():
        batch_mask = df_batch['batch'] == batch
        batch_data = corrected_data[batch_mask]
        
        # è®¡ç®—æ‰¹æ¬¡å†…çš„å‡å€¼å’Œæ ‡å‡†å·®
        batch_mean = batch_data.mean()
        batch_std = batch_data.std()
        
        # z-scoreæ ‡å‡†åŒ–
        corrected_data.loc[batch_mask] = (batch_data - batch_mean) / batch_std
    
    # å…¨å±€é‡æ–°ç¼©æ”¾
    global_mean = df_batch[gene_cols].mean().mean()
    global_std = df_batch[gene_cols].std().mean()
    corrected_data = corrected_data * global_std + global_mean
    
    print("æ ¡æ­£æ­¥éª¤:")
    print("  1. æ‰¹æ¬¡å†…z-scoreæ ‡å‡†åŒ–")
    print("  2. å…¨å±€å‡å€¼æ–¹å·®é‡æ–°ç¼©æ”¾")
    print(f"  3. æ ¡æ­£å‰æ•°æ®èŒƒå›´: [{df_batch[gene_cols].min().min():.2f}, {df_batch[gene_cols].max().max():.2f}]")
    print(f"  4. æ ¡æ­£åæ•°æ®èŒƒå›´: [{corrected_data.min().min():.2f}, {corrected_data.max().max():.2f}]")
    
    # æ ¡æ­£åPCAåˆ†æ
    pca_corrected = PCA(n_components=3)
    pca_corrected_result = pca_corrected.fit_transform(corrected_data)
    
    # æ ¡æ­£å - æŒ‰æ‰¹æ¬¡ç€è‰²
    ax = axes[1, 0]
    for batch in df_batch['batch'].unique():
        mask = df_batch['batch'] == batch
        color = 'red' if batch == 'Batch1' else 'blue'
        ax.scatter(pca_corrected_result[mask, 0], pca_corrected_result[mask, 1], 
                  c=color, label=batch, alpha=0.7, s=50)
    ax.set_title('æ ¡æ­£å - æŒ‰æ‰¹æ¬¡ç€è‰²')
    ax.set_xlabel(f'PC1 ({pca_corrected.explained_variance_ratio_[0]:.2f})')
    ax.set_ylabel(f'PC2 ({pca_corrected.explained_variance_ratio_[1]:.2f})')
    ax.legend()
    
    # æ ¡æ­£å - æŒ‰æ¡ä»¶ç€è‰²
    ax = axes[1, 1]
    for condition in df_batch['condition'].unique():
        mask = df_batch['condition'] == condition
        color = 'green' if condition == 'Condition1' else 'orange'
        ax.scatter(pca_corrected_result[mask, 0], pca_corrected_result[mask, 1], 
                  c=color, label=condition, alpha=0.7, s=50)
    ax.set_title('æ ¡æ­£å - æŒ‰æ¡ä»¶ç€è‰²')
    ax.set_xlabel(f'PC1 ({pca_corrected.explained_variance_ratio_[0]:.2f})')
    ax.set_ylabel(f'PC2 ({pca_corrected.explained_variance_ratio_[1]:.2f})')
    ax.legend()
    
    plt.tight_layout()
    plt.savefig('batch_effect_correction.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # 3. æ ¡æ­£æ•ˆæœè¯„ä¼°
    print(f"\næ ¡æ­£æ•ˆæœè¯„ä¼°:")
    
    # è®¡ç®—æ‰¹æ¬¡é—´è·ç¦»
    def calculate_batch_separation(data, batch_labels):
        """è®¡ç®—ä¸åŒæ‰¹æ¬¡é—´çš„å¹³å‡è·ç¦»"""
        distances = []
        for i in range(len(data)):
            for j in range(i+1, len(data)):
                if batch_labels[i] != batch_labels[j]:
                    dist = np.linalg.norm(data[i] - data[j])
                    distances.append(dist)
        return np.mean(distances)
    
    # æ ¡æ­£å‰åæ‰¹æ¬¡åˆ†ç¦»åº¦
    orig_separation = calculate_batch_separation(pca_result[:, :2], batch_labels)
    corrected_separation = calculate_batch_separation(pca_corrected_result[:, :2], batch_labels)
    
    print(f"æ‰¹æ¬¡åˆ†ç¦»åº¦è¯„ä¼°:")
    print(f"  - æ ¡æ­£å‰æ‰¹æ¬¡é—´å¹³å‡è·ç¦»: {orig_separation:.3f}")
    print(f"  - æ ¡æ­£åæ‰¹æ¬¡é—´å¹³å‡è·ç¦»: {corrected_separation:.3f}")
    print(f"  - æ”¹å–„ç¨‹åº¦: {(orig_separation - corrected_separation) / orig_separation * 100:.1f}%")
    
    # è®¡ç®—ç”Ÿç‰©å­¦ä¿¡å·ä¿ç•™
    def calculate_condition_separation(data, condition_labels):
        """è®¡ç®—ä¸åŒæ¡ä»¶é—´çš„å¹³å‡è·ç¦»"""
        distances = []
        for i in range(len(data)):
            for j in range(i+1, len(data)):
                if condition_labels[i] != condition_labels[j]:
                    dist = np.linalg.norm(data[i] - data[j])
                    distances.append(dist)
        return np.mean(distances)
    
    orig_bio_signal = calculate_condition_separation(pca_result[:, :2], condition_labels)
    corrected_bio_signal = calculate_condition_separation(pca_corrected_result[:, :2], condition_labels)
    
    print(f"\nç”Ÿç‰©å­¦ä¿¡å·ä¿ç•™:")
    print(f"  - æ ¡æ­£å‰æ¡ä»¶é—´è·ç¦»: {orig_bio_signal:.3f}")
    print(f"  - æ ¡æ­£åæ¡ä»¶é—´è·ç¦»: {corrected_bio_signal:.3f}")
    print(f"  - ä¿¡å·ä¿ç•™ç‡: {corrected_bio_signal / orig_bio_signal * 100:.1f}%")
    
    print(f"\næ‰¹æ¬¡æ ¡æ­£å»ºè®®:")
    print("- å§‹ç»ˆåœ¨åˆ†æå‰æ£€æŸ¥æ‰¹æ¬¡æ•ˆåº”")
    print("- å®éªŒè®¾è®¡æ—¶éšæœºåˆ†é…æ ·æœ¬åˆ°ä¸åŒæ‰¹æ¬¡")
    print("- ä½¿ç”¨ä¸“ä¸šå·¥å…·å¦‚ComBatã€limmaè¿›è¡Œæ ¡æ­£")
    print("- æ ¡æ­£åéªŒè¯ç”Ÿç‰©å­¦ä¿¡å·æ˜¯å¦ä¿ç•™")
    print("- æ‰¹æ¬¡å’Œæ¡ä»¶ä¸è¦å®Œå…¨æ··æ‚")


# ====================
# ä¸»å‡½æ•°
# ====================

def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ æ•™ç¨‹
    
    å±•ç¤ºæœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å®Œæ•´åº”ç”¨æµç¨‹ï¼Œ
    ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ‰€æœ‰å…³é”®æ­¥éª¤ã€‚
    """
    
    print("="*60)
    print("Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯")
    print("="*60)
    print("""
æ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ çš„ä¸–ç•Œï¼
åœ¨ç”Ÿç‰©å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦ä»å¤æ‚çš„æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼ã€‚
æœºå™¨å­¦ä¹ å°±æ˜¯è®©è®¡ç®—æœºå­¦ä¼šè¿™ç§æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚
    """)
    
    # Part 1: ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹  - æ•™ä¼šè®¡ç®—æœºè¯†åˆ«")
    print("="*60)
    
    # åˆ›å»ºåŸºå› åˆ†ç±»æ•°æ®
    df_genes, feature_names = create_gene_classification_data()
    
    # å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ
    visualize_feature_distributions(df_genes, feature_names)
    
    # è®­ç»ƒåˆ†ç±»æ¨¡å‹
    results, X_test, y_test, scaler = train_classification_models(df_genes, feature_names)
    
    # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
    visualize_model_comparison(results)
    
    # æ¼”ç¤ºè¿‡æ‹Ÿåˆ
    demonstrate_overfitting()
    
    # Part 2: æ— ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹  - å‘ç°éšè—æ¨¡å¼")
    print("="*60)
    
    # åˆ›å»ºç»†èƒè¡¨è¾¾æ•°æ®
    df_cells, true_labels = create_cell_expression_data()
    
    # ç¡®å®šæœ€ä¼˜èšç±»æ•°
    optimal_k = determine_optimal_clusters(StandardScaler().fit_transform(np.log1p(df_cells)))
    
    # æ‰§è¡Œèšç±»åˆ†æ
    clustering_results, df_scaled, pca = perform_clustering_analysis(df_cells, true_labels)
    
    # åˆ›å»ºèšç±»çƒ­å›¾
    best_clustering = 'K-Means'
    create_cluster_heatmap(df_cells, clustering_results[best_clustering]['labels'])
    
    # Part 3: ç‰¹å¾å·¥ç¨‹ä¸ä¼˜åŒ–
    print("\n" + "="*60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–")
    print("="*60)
    
    # ç‰¹å¾å·¥ç¨‹æ¼”ç¤º
    demonstrate_feature_engineering()
    
    # è¶…å‚æ•°è°ƒä¼˜
    hyperparameter_tuning_demo()
    
    # Part 4: æ·±åº¦å­¦ä¹ 
    print("\n" + "="*60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ åˆæ¢")
    print("="*60)
    
    # æ·±åº¦å­¦ä¹ ä»‹ç»
    introduction_to_deep_learning()
    
    # Part 5: çœŸå®æ•°æ®å¤„ç†å®æˆ˜
    print("\n" + "="*60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šçœŸå®æ•°æ®å¤„ç†å®æˆ˜")
    print("="*60)
    
    print("""
å®æˆ˜èƒ½åŠ›è®­ç»ƒï¼š
æœ¬éƒ¨åˆ†å°†ä½¿ç”¨çœŸå®çš„ç”Ÿç‰©æ•°æ®ï¼Œè®­ç»ƒä½ å¤„ç†å®é™…ç§‘ç ”ä¸­é‡åˆ°çš„æ•°æ®æŒ‘æˆ˜ã€‚
åŒ…æ‹¬æ‰¹æ¬¡æ•ˆåº”ã€æ•°æ®è´¨é‡æ§åˆ¶ã€ç‰¹å¾å·¥ç¨‹ç­‰å…³é”®æŠ€èƒ½ã€‚
    """)
    
    # çœŸå®å•ç»†èƒæ•°æ®å¤„ç†
    handle_real_single_cell_data()
    
    # çœŸå®ç™Œç—‡ä¸´åºŠæ•°æ®åˆ†æ
    handle_cancer_clinical_data()
    
    # æ‰¹æ¬¡æ•ˆåº”æ£€æµ‹å’Œæ ¡æ­£
    demonstrate_batch_effect_correction()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("è¯¾ç¨‹æ€»ç»“")
    print("="*60)
    
    print("""
æœ¬ç« æ ¸å¿ƒè¦ç‚¹ï¼š

æœºå™¨å­¦ä¹ åŸºç¡€ï¼š
   - ç›‘ç£å­¦ä¹ ï¼šæœ‰è€å¸ˆçš„å­¦ä¹ ï¼ˆåˆ†ç±»ã€å›å½’ï¼‰
   - æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°éšè—æ¨¡å¼ï¼ˆèšç±»ã€é™ç»´ï¼‰
   - æ¨¡å‹è¯„ä¼°ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€AUC

å®è·µæŠ€èƒ½ï¼š
   - æ•°æ®é¢„å¤„ç†ï¼šæ¸…æ´—ã€è½¬æ¢ã€æ ‡å‡†åŒ–
   - ç‰¹å¾å·¥ç¨‹ï¼šæå–ã€é€‰æ‹©ã€æ„é€ ç‰¹å¾
   - æ¨¡å‹é€‰æ‹©ï¼šæ¯”è¾ƒå¤šç§ç®—æ³•æ€§èƒ½
   - è¶…å‚æ•°è°ƒä¼˜ï¼šç½‘æ ¼æœç´¢ã€è´å¶æ–¯ä¼˜åŒ–

ç”Ÿç‰©å­¦åº”ç”¨ï¼š
   - åŸºå› åŠŸèƒ½é¢„æµ‹ï¼šä»åºåˆ—åˆ°åŠŸèƒ½
   - ç»†èƒç±»å‹è¯†åˆ«ï¼šå•ç»†èƒåˆ†æé©å‘½
   - ç–¾ç—…åˆ†ç±»è¯Šæ–­ï¼šç²¾å‡†åŒ»ç–—åŸºç¡€
   - è¯ç‰©é¶ç‚¹å‘ç°ï¼šAIåŠ é€Ÿè¯ç‰©ç ”å‘

å®æˆ˜æŠ€èƒ½ï¼ˆæ–°å¢ï¼‰ï¼š
   - çœŸå®æ•°æ®å¤„ç†ï¼šå•ç»†èƒRNA-seqåˆ†ææµç¨‹
   - ä¸´åºŠæ•°æ®å»ºæ¨¡ï¼šç™Œç—‡é¢„åé¢„æµ‹å®æˆ˜
   - æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£ï¼šå¤šæ‰¹æ¬¡æ•°æ®æ•´åˆæŠ€æœ¯
   - è´¨é‡æ§åˆ¶ï¼šæ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—æµç¨‹

å…³é”®æ³¨æ„äº‹é¡¹ï¼š
   - é¿å…è¿‡æ‹Ÿåˆï¼šäº¤å‰éªŒè¯ + æ­£åˆ™åŒ–
   - å¤„ç†ä¸å¹³è¡¡ï¼šæƒé‡è°ƒæ•´ + é‡‡æ ·ç­–ç•¥
   - ç‰¹å¾é€‰æ‹©ï¼šå»é™¤å™ªå£°ï¼Œä¿ç•™ä¿¡å·
   - ç»“æœè§£é‡Šï¼šç»“åˆé¢†åŸŸçŸ¥è¯†æ˜¯å…³é”®
   - æ‰¹æ¬¡æ•ˆåº”ï¼šæ£€æµ‹å’Œæ ¡æ­£ç³»ç»Ÿæ€§åå·®
    """)
    
    print("""
æ­å–œä½ å®Œæˆæœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ï¼

ä½ å·²ç»æŒæ¡ï¼š
- ä½¿ç”¨scikit-learnè¿›è¡Œæœºå™¨å­¦ä¹ 
- ç†è§£ç›‘ç£ä¸æ— ç›‘ç£å­¦ä¹ çš„æœ¬è´¨åŒºåˆ«
- æŒæ¡æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–æŠ€å·§
- åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­åº”ç”¨æœºå™¨å­¦ä¹ 
- ç†è§£æ·±åº¦å­¦ä¹ çš„æ½œåŠ›å’Œåº”ç”¨

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
1. å®è·µé¡¹ç›®ï¼šå°è¯•TCGAã€GEOç­‰çœŸå®æ•°æ®é›†
2. æ·±åº¦å­¦ä¹ ï¼šå­¦ä¹ PyTorch/TensorFlowæ¡†æ¶
3. ä¸“ä¸šå·¥å…·ï¼šæŒæ¡Scanpyã€Seuratã€DESeq2
4. ç«èµ›å‚ä¸ï¼šKaggleç”Ÿç‰©ä¿¡æ¯å­¦æŒ‘æˆ˜èµ›
5. å¼€æºè´¡çŒ®ï¼šå‚ä¸ç”Ÿç‰©ä¿¡æ¯å­¦å¼€æºé¡¹ç›®

æ ¸å¿ƒç†å¿µï¼š
"æœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚ï¼"

å°†AIæŠ€æœ¯ä¸ç”Ÿç‰©å­¦æ´å¯Ÿå®Œç¾ç»“åˆï¼Œ
ä½ å°†æˆä¸ºæ–°æ—¶ä»£çš„è®¡ç®—ç”Ÿç‰©å­¦å®¶ï¼

ç»§ç»­æ¢ç´¢ï¼Œæ”¹å˜ä¸–ç•Œï¼
    """)


if __name__ == "__main__":
    main()