#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

æœ¬ç« æ¼”ç¤ºå®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ï¼š
Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»
Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°
Part 3: æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
Part 4: æ·±åº¦å­¦ä¹ åˆæ¢

ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
æœºå™¨å­¦ä¹ å°±åƒè®­ç»ƒä¸€ä¸ªç—…ç†å­¦å®¶
- ç›‘ç£å­¦ä¹ ï¼šç”¨å·²çŸ¥è¯Šæ–­çš„ç—…ä¾‹æ•™ä¼šè¯†åˆ«ç–¾ç—…
- æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°æœªçŸ¥çš„ç–¾ç—…äºšå‹
- æ·±åº¦å­¦ä¹ ï¼šåƒäººè„‘ä¸€æ ·å¤šå±‚æ¬¡ç†è§£å¤æ‚æ¨¡å¼
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# åˆ†ç±»ç®—æ³•
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve, 
                           confusion_matrix, silhouette_score, 
                           adjusted_rand_score, classification_report)

import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"ğŸ§¬ {title}")
    print("="*60)


# ====================
# Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»
# ====================

def create_gene_classification_data():
    """
    åˆ›å»ºåŸºå› åŠŸèƒ½åˆ†ç±»çš„ç¤ºä¾‹æ•°æ®
    
    ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    æ ¹æ®åŸºå› çš„åºåˆ—ç‰¹å¾é¢„æµ‹å…¶åŠŸèƒ½ç±»åˆ«
    - ç±»åˆ«0ï¼šç®¡å®¶åŸºå› ï¼ˆhousekeepingï¼‰
    - ç±»åˆ«1ï¼šè½¬å½•å› å­ï¼ˆtranscription factorï¼‰
    - ç±»åˆ«2ï¼šæ¿€é…¶ï¼ˆkinaseï¼‰
    """
    print_section("Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»")
    
    np.random.seed(42)
    n_samples = 300
    
    # åˆ›å»ºç‰¹å¾ï¼ˆåŸºå› åºåˆ—ç‰¹å¾ï¼‰
    # ç‰¹å¾1ï¼šGCå«é‡
    # ç‰¹å¾2ï¼šé•¿åº¦
    # ç‰¹å¾3ï¼šä¿å®ˆæ€§è¯„åˆ†
    # ç‰¹å¾4ï¼šè¡¨è¾¾æ°´å¹³
    # ç‰¹å¾5ï¼šè¿›åŒ–é€Ÿç‡
    
    features = []
    labels = []
    
    # ç®¡å®¶åŸºå› ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.45, 0.05)  # ä¸­ç­‰GCå«é‡
        length = np.random.normal(1500, 300)        # ä¸­ç­‰é•¿åº¦
        conservation = np.random.normal(0.8, 0.1)   # é«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.7, 0.1)     # é«˜è¡¨è¾¾
        evolution_rate = np.random.normal(0.2, 0.05) # ä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(0)
    
    # è½¬å½•å› å­ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.55, 0.05)   # è¾ƒé«˜GCå«é‡
        length = np.random.normal(2000, 400)        # è¾ƒé•¿
        conservation = np.random.normal(0.6, 0.15)  # ä¸­ç­‰ä¿å®ˆæ€§
        expression = np.random.normal(0.3, 0.1)     # ä½è¡¨è¾¾
        evolution_rate = np.random.normal(0.4, 0.1) # ä¸­ç­‰è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(1)
    
    # æ¿€é…¶ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.50, 0.05)   # ä¸­ç­‰GCå«é‡
        length = np.random.normal(2500, 500)        # é•¿
        conservation = np.random.normal(0.7, 0.1)   # è¾ƒé«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.5, 0.15)    # ä¸­ç­‰è¡¨è¾¾
        evolution_rate = np.random.normal(0.3, 0.08) # è¾ƒä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(2)
    
    # åˆ›å»ºDataFrame
    feature_names = ['GC_content', 'length', 'conservation', 'expression', 'evolution_rate']
    df = pd.DataFrame(features, columns=feature_names)
    df['function'] = labels
    
    print("ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {df['function'].value_counts().to_dict()}")
    print("\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ï¼š")
    print(df[feature_names].describe().round(3))
    
    return df, feature_names


def visualize_feature_distributions(df, feature_names):
    """å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ"""
    print("\nğŸ“Š å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    function_names = {0: 'Housekeeping', 1: 'TF', 2: 'Kinase'}
    colors = ['blue', 'orange', 'green']
    
    for idx, feature in enumerate(feature_names):
        ax = axes[idx]
        for func_id in [0, 1, 2]:
            data = df[df['function'] == func_id][feature]
            ax.hist(data, alpha=0.5, label=function_names[func_id], 
                   color=colors[func_id], bins=20)
        ax.set_xlabel(feature)
        ax.set_ylabel('Frequency')
        ax.set_title(f'Distribution of {feature}')
        ax.legend()
    
    # ç©ºç™½å­å›¾
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')
    plt.show()


def train_classification_models(df, feature_names):
    """
    è®­ç»ƒå¤šç§åˆ†ç±»æ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç”¨ä¸åŒçš„æ–¹æ³•è¯Šæ–­ç–¾ç—…
    - é€»è¾‘å›å½’ï¼šæ ¹æ®ç—‡çŠ¶çš„çº¿æ€§ç»„åˆåˆ¤æ–­
    - å†³ç­–æ ‘ï¼šåƒåŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹å›¾
    - éšæœºæ£®æ—ï¼šå¤šä¸ªåŒ»ç”ŸæŠ•ç¥¨å†³å®š
    - SVMï¼šæ‰¾åˆ°æœ€ä½³çš„è¯Šæ–­è¾¹ç•Œ
    """
    print_section("è®­ç»ƒåˆ†ç±»æ¨¡å‹")
    
    # å‡†å¤‡æ•°æ®
    X = df[feature_names].values
    y = df['function'].values
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ70%è®­ç»ƒï¼Œ30%æµ‹è¯•ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
        'å†³ç­–æ ‘': DecisionTreeClassifier(random_state=42, max_depth=5),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_estimators=100),
        'SVM': SVC(random_state=42, probability=True),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"  å¬å›ç‡: {recall:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    return results, X_test_scaled, y_test, scaler


def visualize_model_comparison(results):
    """å¯è§†åŒ–æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    
    # æå–æŒ‡æ ‡
    models = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    # åˆ›å»ºæ¯”è¾ƒå›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(models))
    width = 0.2
    
    for i, metric in enumerate(metrics):
        values = [results[model][metric] for model in models]
        ax.bar(x + i*width, values, width, label=metric.capitalize())
    
    ax.set_xlabel('Model')
    ax.set_ylabel('Score')
    ax.set_title('Model Performance Comparison')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(models, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()


def demonstrate_overfitting():
    """
    æ¼”ç¤ºè¿‡æ‹Ÿåˆç°è±¡
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    è¿‡æ‹Ÿåˆå°±åƒè®°ä½äº†æ‰€æœ‰è®­ç»ƒç—…äººçš„åå­—å’Œç—‡çŠ¶ï¼Œ
    ä½†é‡åˆ°æ–°ç—…äººå°±ä¸ä¼šè¯Šæ–­äº†
    """
    print_section("è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ")
    
    print("""
ğŸ” ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ
è¿‡æ‹Ÿåˆå°±åƒä¸€ä¸ªåŒ»å­¦ç”Ÿæ­»è®°ç¡¬èƒŒäº†æ‰€æœ‰ç—…ä¾‹ï¼Œ
ä½†ä¸ç†è§£ç–¾ç—…çš„æœ¬è´¨è§„å¾‹ï¼Œé‡åˆ°æ–°ç—…ä¾‹å°±æŸæ‰‹æ— ç­–ã€‚

ğŸ” å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ
1. å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆçœ‹æ›´å¤šç—…ä¾‹ï¼‰
2. ç®€åŒ–æ¨¡å‹ï¼ˆæŠ“ä½ä¸»è¦ç—‡çŠ¶ï¼‰
3. æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–æŸä¸ªç‰¹å¾ï¼‰
4. äº¤å‰éªŒè¯ï¼ˆåœ¨ä¸åŒåŒ»é™¢éªŒè¯ï¼‰
5. æ—©åœï¼ˆé€‚å¯è€Œæ­¢ï¼‰
    """)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.sort(np.random.rand(100, 1) * 10, axis=0)
    y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    
    # è®­ç»ƒä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import Pipeline
    
    degrees = [1, 4, 15]
    titles = ['Underfitting', 'Good Fit', 'Overfitting']
    
    for ax, degree, title in zip(axes, degrees, titles):
        # åˆ›å»ºå¤šé¡¹å¼å›å½’
        polynomial_features = PolynomialFeatures(degree=degree)
        linear_regression = LinearRegression()
        pipeline = Pipeline([
            ("polynomial_features", polynomial_features),
            ("linear_regression", linear_regression)
        ])
        
        pipeline.fit(X, y)
        
        # é¢„æµ‹
        X_plot = np.linspace(0, 10, 300)[:, np.newaxis]
        y_plot = pipeline.predict(X_plot)
        
        # ç»˜å›¾
        ax.scatter(X, y, alpha=0.5, label='Data')
        ax.plot(X_plot, y_plot, color='red', label=f'Degree {degree}')
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title(title)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('overfitting_demo.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°
# ====================

def create_cell_expression_data():
    """
    åˆ›å»ºå•ç»†èƒåŸºå› è¡¨è¾¾æ•°æ®
    
    ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    æ¨¡æ‹Ÿä¸åŒç»†èƒç±»å‹çš„åŸºå› è¡¨è¾¾è°±
    - å¹²ç»†èƒ
    - åˆ†åŒ–ä¸­çš„ç»†èƒ
    - æˆç†Ÿç»†èƒ
    """
    print_section("Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°")
    
    np.random.seed(42)
    
    # åˆ›å»ºä¸‰ç§ç»†èƒç±»å‹çš„è¡¨è¾¾æ•°æ®
    n_genes = 50
    gene_names = [f"Gene_{i:03d}" for i in range(1, n_genes+1)]
    
    # å¹²ç»†èƒï¼ˆé«˜è¡¨è¾¾å¤šèƒ½æ€§åŸºå› ï¼‰
    stem_cells = np.random.lognormal(2, 0.5, (30, n_genes))
    stem_cells[:, :10] *= 2  # å‰10ä¸ªåŸºå› é«˜è¡¨è¾¾ï¼ˆå¹²æ€§æ ‡è®°ï¼‰
    
    # åˆ†åŒ–ä¸­çš„ç»†èƒï¼ˆä¸­é—´çŠ¶æ€ï¼‰
    diff_cells = np.random.lognormal(2, 0.6, (40, n_genes))
    diff_cells[:, 10:25] *= 1.8  # ä¸­é—´åŸºå› é«˜è¡¨è¾¾ï¼ˆåˆ†åŒ–æ ‡è®°ï¼‰
    
    # æˆç†Ÿç»†èƒï¼ˆç‰¹å®šåŸºå› é«˜è¡¨è¾¾ï¼‰
    mature_cells = np.random.lognormal(2, 0.4, (30, n_genes))
    mature_cells[:, 30:45] *= 2.5  # åé¢åŸºå› é«˜è¡¨è¾¾ï¼ˆæˆç†Ÿæ ‡è®°ï¼‰
    
    # åˆå¹¶æ•°æ®
    all_cells = np.vstack([stem_cells, diff_cells, mature_cells])
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾ï¼ˆç”¨äºéªŒè¯ï¼Œä½†ä¸ç”¨äºè®­ç»ƒï¼‰
    true_labels = np.array([0]*30 + [1]*40 + [2]*30)
    
    # åˆ›å»ºDataFrame
    cell_names = [f"Cell_{i:03d}" for i in range(1, 101)]
    df = pd.DataFrame(all_cells, columns=gene_names, index=cell_names)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  ç»†èƒæ•°é‡: {df.shape[0]}")
    print(f"  åŸºå› æ•°é‡: {df.shape[1]}")
    print(f"  è¡¨è¾¾å€¼èŒƒå›´: [{df.values.min():.2f}, {df.values.max():.2f}]")
    
    return df, true_labels


def perform_clustering_analysis(df, true_labels):
    """
    æ‰§è¡Œå¤šç§èšç±»åˆ†æ
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒåœ¨æ˜¾å¾®é•œä¸‹è§‚å¯Ÿç»†èƒï¼Œæ ¹æ®å½¢æ€å°†ç›¸ä¼¼çš„å½’ä¸ºä¸€ç»„
    """
    print_section("èšç±»åˆ†æ")
    
    # æ•°æ®é¢„å¤„ç†
    # 1. å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†åæ€åˆ†å¸ƒï¼‰
    df_log = np.log1p(df)
    
    # 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_log)
    
    # 3. PCAé™ç»´ï¼ˆå¯è§†åŒ–ç”¨ï¼‰
    pca = PCA(n_components=2, random_state=42)
    df_pca = pca.fit_transform(df_scaled)
    
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.1%}")
    
    # å°è¯•ä¸åŒçš„èšç±»ç®—æ³•
    clustering_methods = {
        'K-Means': KMeans(n_clusters=3, random_state=42),
        'DBSCAN': DBSCAN(eps=3, min_samples=5),
        'Hierarchical': AgglomerativeClustering(n_clusters=3),
        'GMM': GaussianMixture(n_components=3, random_state=42)
    }
    
    results = {}
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (name, clusterer) in enumerate(clustering_methods.items()):
        print(f"\næ‰§è¡Œ {name} èšç±»...")
        
        # èšç±»
        if name == 'GMM':
            labels = clusterer.fit_predict(df_scaled)
        else:
            labels = clusterer.fit_predict(df_scaled)
        
        # è¯„ä¼°
        if len(np.unique(labels)) > 1:
            silhouette = silhouette_score(df_scaled, labels)
            ari = adjusted_rand_score(true_labels, labels)
        else:
            silhouette = -1
            ari = -1
        
        results[name] = {
            'labels': labels,
            'silhouette': silhouette,
            'ari': ari,
            'n_clusters': len(np.unique(labels[labels >= 0]))
        }
        
        print(f"  èšç±»æ•°: {results[name]['n_clusters']}")
        print(f"  è½®å»“ç³»æ•°: {silhouette:.3f}")
        print(f"  è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari:.3f}")
        
        # å¯è§†åŒ–
        ax = axes[idx]
        scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                           c=labels, cmap='viridis', 
                           alpha=0.6, s=30)
        ax.set_title(f'{name}\nSilhouette: {silhouette:.3f}')
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        plt.colorbar(scatter, ax=ax)
    
    # çœŸå®æ ‡ç­¾
    ax = axes[4]
    scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                       c=true_labels, cmap='Set1', 
                       alpha=0.6, s=30)
    ax.set_title('True Labels')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    plt.colorbar(scatter, ax=ax)
    
    # ç©ºç™½
    axes[5].axis('off')
    
    plt.tight_layout()
    plt.savefig('clustering_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    return results, df_scaled, pca


def determine_optimal_clusters(df_scaled):
    """
    ç¡®å®šæœ€ä¼˜èšç±»æ•°
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç¡®å®šç»„ç»‡ä¸­æœ‰å‡ ç§ä¸åŒçš„ç»†èƒç±»å‹
    """
    print_section("ç¡®å®šæœ€ä¼˜èšç±»æ•°")
    
    k_range = range(2, 11)
    inertias = []
    silhouettes = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(df_scaled)
        
        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(df_scaled, labels))
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # è‚˜éƒ¨æ³•åˆ™
    axes[0].plot(k_range, inertias, 'bo-')
    axes[0].set_xlabel('Number of Clusters (k)')
    axes[0].set_ylabel('Inertia')
    axes[0].set_title('Elbow Method')
    axes[0].grid(True, alpha=0.3)
    
    # è½®å»“ç³»æ•°
    axes[1].plot(k_range, silhouettes, 'ro-')
    axes[1].set_xlabel('Number of Clusters (k)')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].set_title('Silhouette Analysis')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('optimal_k.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    optimal_k = k_range[np.argmax(silhouettes)]
    print(f"ğŸ¯ åŸºäºè½®å»“ç³»æ•°ï¼Œæœ€ä¼˜èšç±»æ•°ä¸º: {optimal_k}")
    
    return optimal_k


def create_cluster_heatmap(df, labels):
    """åˆ›å»ºèšç±»çƒ­å›¾"""
    print("\nğŸ“Š åˆ›å»ºèšç±»çƒ­å›¾")
    
    # æŒ‰èšç±»æ ‡ç­¾æ’åº
    sorted_idx = np.argsort(labels)
    df_sorted = df.iloc[sorted_idx]
    labels_sorted = labels[sorted_idx]
    
    # é€‰æ‹©å·®å¼‚æœ€å¤§çš„åŸºå› 
    gene_std = df.std(axis=0)
    top_genes = gene_std.nlargest(20).index
    
    # åˆ›å»ºçƒ­å›¾
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºèšç±»é¢œè‰²æ¡
    cluster_colors = ['red', 'blue', 'green', 'orange', 'purple']
    row_colors = [cluster_colors[label] if label >= 0 else 'gray' 
                  for label in labels_sorted]
    
    # ç»˜åˆ¶çƒ­å›¾
    g = sns.clustermap(df_sorted[top_genes], 
                       row_colors=row_colors,
                       cmap='RdBu_r',
                       center=0,
                       row_cluster=False,
                       col_cluster=True,
                       figsize=(10, 8),
                       cbar_kws={'label': 'Expression'})
    
    g.ax_heatmap.set_xlabel('Genes')
    g.ax_heatmap.set_ylabel('Cells')
    plt.suptitle('Cell Clustering Heatmap', y=1.02)
    
    plt.savefig('cluster_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 3: ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–
# ====================

def demonstrate_feature_engineering():
    """
    æ¼”ç¤ºç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    ç‰¹å¾å·¥ç¨‹å°±åƒé€‰æ‹©åˆé€‚çš„ç”Ÿç‰©æ ‡è®°ç‰©
    å¥½çš„æ ‡è®°ç‰©è®©è¯Šæ–­äº‹åŠåŠŸå€
    """
    print_section("ç‰¹å¾å·¥ç¨‹çš„è‰ºæœ¯")
    
    print("""
ğŸ”¬ ç‰¹å¾å·¥ç¨‹åœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨ï¼š

1. åºåˆ—ç‰¹å¾ï¼š
   - k-meré¢‘ç‡ï¼ˆDNA/è›‹ç™½è´¨motifï¼‰
   - GCå«é‡ã€å¯†ç å­ä½¿ç”¨åå¥½
   - äºŒçº§ç»“æ„é¢„æµ‹åˆ†æ•°

2. ç»“æ„ç‰¹å¾ï¼š
   - æ°¨åŸºé…¸ç†åŒ–æ€§è´¨
   - æº¶å‰‚å¯åŠæ€§
   - äºŒé¢è§’

3. ç½‘ç»œç‰¹å¾ï¼š
   - åŸºå› å…±è¡¨è¾¾
   - è›‹ç™½è´¨ç›¸äº’ä½œç”¨
   - ä»£è°¢é€šè·¯å‚ä¸åº¦

4. ç»„åˆç‰¹å¾ï¼š
   - ç‰¹å¾äº¤äº’é¡¹
   - å¤šé¡¹å¼ç‰¹å¾
   - ç‰¹å¾æ¯”ç‡
    """)
    
    # åˆ›å»ºç¤ºä¾‹ï¼šåºåˆ—ç‰¹å¾æå–
    def extract_sequence_features(sequence):
        """ä»DNAåºåˆ—æå–ç‰¹å¾"""
        features = {}
        
        # GCå«é‡
        gc_count = sequence.count('G') + sequence.count('C')
        features['gc_content'] = gc_count / len(sequence)
        
        # k-meré¢‘ç‡ï¼ˆ2-merä¸ºä¾‹ï¼‰
        kmers = ['AA', 'AT', 'AG', 'AC', 'TA', 'TT', 'TG', 'TC',
                 'GA', 'GT', 'GG', 'GC', 'CA', 'CT', 'CG', 'CC']
        for kmer in kmers:
            features[f'kmer_{kmer}'] = sequence.count(kmer) / (len(sequence) - 1)
        
        # åºåˆ—é•¿åº¦
        features['length'] = len(sequence)
        
        return features
    
    # ç¤ºä¾‹åºåˆ—
    sequences = [
        "ATGCGATCGTAGCTAGCTAGCTAGCTAGCTA",
        "GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCG",
        "ATATATATATATATATATATATATATATAT"
    ]
    
    print("\nç¤ºä¾‹ï¼šDNAåºåˆ—ç‰¹å¾æå–")
    for i, seq in enumerate(sequences, 1):
        features = extract_sequence_features(seq)
        print(f"\nåºåˆ—{i}: {seq[:20]}...")
        print(f"  GCå«é‡: {features['gc_content']:.2f}")
        print(f"  é•¿åº¦: {features['length']}")
        print(f"  ATäºŒèšä½“é¢‘ç‡: {features['kmer_AT']:.2f}")


def hyperparameter_tuning_demo():
    """
    æ¼”ç¤ºè¶…å‚æ•°è°ƒä¼˜
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒä¼˜åŒ–PCRæ¡ä»¶ï¼Œéœ€è¦è°ƒæ•´æ¸©åº¦ã€æ—¶é—´ã€æµ“åº¦ç­‰å‚æ•°
    """
    print_section("è¶…å‚æ•°è°ƒä¼˜")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=200, n_features=10, 
                              n_informative=5, n_redundant=5,
                              n_classes=2, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    print("ğŸ” ç½‘æ ¼æœç´¢è¶…å‚æ•°...")
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {np.prod([len(v) for v in param_grid.values()])}")
    
    # ç½‘æ ¼æœç´¢
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=5, 
                               scoring='f1', n_jobs=-1)
    
    grid_search.fit(X_train, y_train)
    
    print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.3f}")
    
    # æµ‹è¯•é›†æ€§èƒ½
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_score = f1_score(y_test, y_pred)
    print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_score:.3f}")
    
    # å¯è§†åŒ–å‚æ•°å½±å“
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å‚æ•°é‡è¦æ€§
    param_importance = []
    for param in ['n_estimators', 'max_depth']:
        param_values = results_df[f'param_{param}'].unique()
        mean_scores = []
        for val in param_values:
            mask = results_df[f'param_{param}'] == val
            mean_scores.append(results_df[mask]['mean_test_score'].mean())
        
        ax = axes[0] if param == 'n_estimators' else axes[1]
        ax.plot(param_values, mean_scores, 'o-')
        ax.set_xlabel(param)
        ax.set_ylabel('Mean CV Score')
        ax.set_title(f'Effect of {param}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('hyperparameter_tuning.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 4: æ·±åº¦å­¦ä¹ åˆæ¢
# ====================

def introduction_to_deep_learning():
    """
    æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åº”ç”¨ä»‹ç»
    """
    print_section("æ·±åº¦å­¦ä¹ åˆæ¢")
    
    print("""
ğŸ§  æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©å­¦ä¸­çš„é©å‘½æ€§åº”ç”¨ï¼š

1. AlphaFold - è›‹ç™½è´¨ç»“æ„é¢„æµ‹
   - è¾“å…¥ï¼šæ°¨åŸºé…¸åºåˆ—
   - è¾“å‡ºï¼š3Dç»“æ„åæ ‡
   - å‡†ç¡®åº¦ï¼šæ¥è¿‘å®éªŒæ°´å¹³

2. åŸºå› è¡¨è¾¾é¢„æµ‹
   - è¾“å…¥ï¼šDNAåºåˆ—
   - è¾“å‡ºï¼šè¡¨è¾¾æ°´å¹³
   - åº”ç”¨ï¼šç†è§£åŸºå› è°ƒæ§

3. è¯ç‰©å‘ç°
   - è¾“å…¥ï¼šåˆ†å­ç»“æ„
   - è¾“å‡ºï¼šæ´»æ€§é¢„æµ‹
   - åŠ é€Ÿï¼šç¼©çŸ­ç ”å‘å‘¨æœŸ

4. åŒ»å­¦å½±åƒåˆ†æ
   - è¾“å…¥ï¼šç—…ç†åˆ‡ç‰‡
   - è¾“å‡ºï¼šç–¾ç—…è¯Šæ–­
   - ä¼˜åŠ¿ï¼šè¶…è¶Šäººç±»ä¸“å®¶

5. å•ç»†èƒåˆ†æ
   - è¾“å…¥ï¼šscRNA-seqæ•°æ®
   - è¾“å‡ºï¼šç»†èƒç±»å‹ã€è½¨è¿¹
   - å‘ç°ï¼šæ–°çš„ç»†èƒäºšç¾¤
    """)
    
    # ç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼ˆä½¿ç”¨sklearnï¼‰
    from sklearn.neural_network import MLPClassifier
    
    print("\nğŸ“Š ç®€å•ç¥ç»ç½‘ç»œç¤ºä¾‹")
    
    # åˆ›å»ºXORé—®é¢˜ï¼ˆéçº¿æ€§å¯åˆ†ï¼‰
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XORè¾“å‡º
    
    # è®­ç»ƒç¥ç»ç½‘ç»œ
    mlp = MLPClassifier(hidden_layer_sizes=(4,), 
                        activation='relu',
                        max_iter=1000,
                        random_state=42)
    mlp.fit(X, y)
    
    # é¢„æµ‹
    predictions = mlp.predict(X)
    
    print("XORé—®é¢˜ï¼ˆéœ€è¦éçº¿æ€§è¾¹ç•Œï¼‰ï¼š")
    print("è¾“å…¥ -> é¢„æœŸè¾“å‡º -> é¢„æµ‹è¾“å‡º")
    for i in range(len(X)):
        print(f"{X[i]} -> {y[i]} -> {predictions[i]}")
    
    # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # çº¿æ€§æ¨¡å‹ï¼ˆå¤±è´¥ï¼‰
    ax = axes[0]
    log_reg = LogisticRegression()
    log_reg.fit(X, y)
    
    xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),
                         np.linspace(-0.5, 1.5, 100))
    Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Linear Model (Fails)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    # ç¥ç»ç½‘ç»œï¼ˆæˆåŠŸï¼‰
    ax = axes[1]
    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Neural Network (Success)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    plt.tight_layout()
    plt.savefig('neural_network_xor.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# ä¸»å‡½æ•°
# ====================

def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ æ•™ç¨‹"""
    
    print("="*60)
    print("ğŸ§¬ Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯")
    print("="*60)
    print("""
æ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ çš„ä¸–ç•Œï¼
åœ¨ç”Ÿç‰©å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦ä»å¤æ‚çš„æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼ã€‚
æœºå™¨å­¦ä¹ å°±æ˜¯è®©è®¡ç®—æœºå­¦ä¼šè¿™ç§æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚
    """)
    
    # Part 1: ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹  - æ•™ä¼šè®¡ç®—æœºè¯†åˆ«")
    print("="*60)
    
    # åˆ›å»ºåŸºå› åˆ†ç±»æ•°æ®
    df_genes, feature_names = create_gene_classification_data()
    
    # å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ
    visualize_feature_distributions(df_genes, feature_names)
    
    # è®­ç»ƒåˆ†ç±»æ¨¡å‹
    results, X_test, y_test, scaler = train_classification_models(df_genes, feature_names)
    
    # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
    visualize_model_comparison(results)
    
    # æ¼”ç¤ºè¿‡æ‹Ÿåˆ
    demonstrate_overfitting()
    
    # Part 2: æ— ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹  - å‘ç°éšè—æ¨¡å¼")
    print("="*60)
    
    # åˆ›å»ºç»†èƒè¡¨è¾¾æ•°æ®
    df_cells, true_labels = create_cell_expression_data()
    
    # ç¡®å®šæœ€ä¼˜èšç±»æ•°
    optimal_k = determine_optimal_clusters(StandardScaler().fit_transform(np.log1p(df_cells)))
    
    # æ‰§è¡Œèšç±»åˆ†æ
    clustering_results, df_scaled, pca = perform_clustering_analysis(df_cells, true_labels)
    
    # åˆ›å»ºèšç±»çƒ­å›¾
    best_clustering = 'K-Means'
    create_cluster_heatmap(df_cells, clustering_results[best_clustering]['labels'])
    
    # Part 3: ç‰¹å¾å·¥ç¨‹ä¸ä¼˜åŒ–
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–")
    print("="*60)
    
    # ç‰¹å¾å·¥ç¨‹æ¼”ç¤º
    demonstrate_feature_engineering()
    
    # è¶…å‚æ•°è°ƒä¼˜
    hyperparameter_tuning_demo()
    
    # Part 4: æ·±åº¦å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ åˆæ¢")
    print("="*60)
    
    # æ·±åº¦å­¦ä¹ ä»‹ç»
    introduction_to_deep_learning()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“š è¯¾ç¨‹æ€»ç»“")
    print("="*60)
    
    print("""
ğŸ¯ æœ¬ç« æ ¸å¿ƒè¦ç‚¹ï¼š

1. æœºå™¨å­¦ä¹ åŸºç¡€
   âœ… ç›‘ç£å­¦ä¹ ï¼šæœ‰æ ‡ç­¾æ•°æ®çš„å­¦ä¹ ï¼ˆåˆ†ç±»ã€å›å½’ï¼‰
   âœ… æ— ç›‘ç£å­¦ä¹ ï¼šæ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹ ï¼ˆèšç±»ã€é™ç»´ï¼‰
   âœ… æ¨¡å‹è¯„ä¼°ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°

2. å®è·µæŠ€èƒ½
   âœ… æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ã€å¯¹æ•°è½¬æ¢
   âœ… ç‰¹å¾å·¥ç¨‹ï¼šæå–ã€é€‰æ‹©ã€æ„é€ ç‰¹å¾
   âœ… æ¨¡å‹é€‰æ‹©ï¼šæ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½
   âœ… è¶…å‚æ•°è°ƒä¼˜ï¼šç½‘æ ¼æœç´¢ã€äº¤å‰éªŒè¯

3. ç”Ÿç‰©å­¦åº”ç”¨
   âœ… åŸºå› åŠŸèƒ½é¢„æµ‹
   âœ… ç»†èƒç±»å‹è¯†åˆ«
   âœ… ç–¾ç—…åˆ†ç±»è¯Šæ–­
   âœ… è¯ç‰©é¶ç‚¹å‘ç°

4. æ³¨æ„äº‹é¡¹
   âš ï¸ é¿å…è¿‡æ‹Ÿåˆï¼šä½¿ç”¨äº¤å‰éªŒè¯
   âš ï¸ ç±»åˆ«ä¸å¹³è¡¡ï¼šè°ƒæ•´é‡‡æ ·æˆ–æƒé‡
   âš ï¸ ç‰¹å¾é€‰æ‹©ï¼šå»é™¤å™ªå£°ç‰¹å¾
   âš ï¸ ç»“æœè§£é‡Šï¼šç»“åˆç”Ÿç‰©å­¦çŸ¥è¯†
    """)
    
    print("""
ğŸš€ æ­å–œä½ å®Œæˆäº†æœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ï¼

ä½ å·²ç»æŒæ¡äº†ï¼š
- ä½¿ç”¨scikit-learnè¿›è¡Œæœºå™¨å­¦ä¹ 
- ç†è§£ç›‘ç£ä¸æ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«
- è¯„ä¼°å’Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½
- åº”ç”¨æœºå™¨å­¦ä¹ è§£å†³ç”Ÿç‰©å­¦é—®é¢˜

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
1. å°è¯•çœŸå®çš„ç”Ÿç‰©æ•°æ®é›†ï¼ˆGEOã€TCGAï¼‰
2. å­¦ä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆTensorFlowã€PyTorchï¼‰
3. æ¢ç´¢ä¸“ä¸šå·¥å…·ï¼ˆScanpyã€DESeq2ï¼‰
4. å‚ä¸Kaggleç”Ÿç‰©ä¿¡æ¯å­¦ç«èµ›

è®°ä½ï¼šæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚ã€‚
å°†ä¸¤è€…ç»“åˆï¼Œä½ å°±èƒ½åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸå¤§æœ‰ä½œä¸ºï¼

ç¥ä½ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼ğŸ§¬ğŸ’»ğŸ”¬
    """)


if __name__ == "__main__":
    main()