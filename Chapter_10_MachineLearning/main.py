#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

æœ¬ç« æ¼”ç¤ºæœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„æ ¸å¿ƒåº”ç”¨ï¼š
- Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½é¢„æµ‹
- Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°  
- Part 3: æ¨¡å‹ä¼˜åŒ– - æå‡é¢„æµ‹æ€§èƒ½
- Part 4: å®æˆ˜é¡¹ç›® - ç™Œç—‡åˆ†å‹é¢„æµ‹

ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”:
æœºå™¨å­¦ä¹  = ä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹çš„ç®—æ³•
â€¢ ç›‘ç£å­¦ä¹ ï¼šæœ‰è€å¸ˆçš„å­¦ä¹ ï¼ˆåˆ†ç±»ã€å›å½’ï¼‰
â€¢ æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°éšè—æ¨¡å¼ï¼ˆèšç±»ã€é™ç»´ï¼‰ 
â€¢ æ¨¡å‹ä¼˜åŒ–ï¼šæå‡æ€§èƒ½çš„è‰ºæœ¯
â€¢ æ·±åº¦å­¦ä¹ ï¼šAIé©å‘½çš„å¼€å§‹

æ ¸å¿ƒç†å¿µï¼šæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# åˆ†ç±»ç®—æ³•
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve, 
                           confusion_matrix, silhouette_score, 
                           adjusted_rand_score, classification_report)

import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"ğŸ§¬ {title}")
    print("="*60)


# ====================
# Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»
# ====================

def create_gene_classification_data():
    """
    åˆ›å»ºåŸºå› åŠŸèƒ½åˆ†ç±»çš„ç¤ºä¾‹æ•°æ®
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    æ ¹æ®åŸºå› åºåˆ—ç‰¹å¾é¢„æµ‹åŠŸèƒ½ç±»åˆ«ï¼Œè¿™åœ¨åŸºå› æ³¨é‡Šä¸­æå…¶é‡è¦ï¼š
    - ç®¡å®¶åŸºå› ï¼šç»´æŒåŸºæœ¬ç»†èƒåŠŸèƒ½ï¼Œè¡¨è¾¾ç¨³å®š
    - è½¬å½•å› å­ï¼šè°ƒæ§åŸºå› è¡¨è¾¾ï¼Œåºåˆ—ä¿å®ˆ
    - æ¿€é…¶ï¼šä¿¡å·ä¼ å¯¼å…³é”®é…¶ï¼Œç»“æ„åŸŸç‰¹å¼‚
    
    ğŸ”¬ åº”ç”¨åœºæ™¯ï¼š
    â€¢ åŸºå› ç»„æ³¨é‡Šå’ŒåŠŸèƒ½é¢„æµ‹
    â€¢ è¯ç‰©é¶ç‚¹è¯†åˆ«å’ŒéªŒè¯
    â€¢ ç–¾ç—…ç›¸å…³åŸºå› ç­›é€‰
    """
    print_section("Part 1: ç›‘ç£å­¦ä¹  - åŸºå› åŠŸèƒ½åˆ†ç±»")
    
    np.random.seed(42)
    n_samples = 300
    
    # åˆ›å»ºç‰¹å¾ï¼ˆåŸºå› åºåˆ—ç‰¹å¾ï¼‰
    # ç‰¹å¾1ï¼šGCå«é‡
    # ç‰¹å¾2ï¼šé•¿åº¦
    # ç‰¹å¾3ï¼šä¿å®ˆæ€§è¯„åˆ†
    # ç‰¹å¾4ï¼šè¡¨è¾¾æ°´å¹³
    # ç‰¹å¾5ï¼šè¿›åŒ–é€Ÿç‡
    
    features = []
    labels = []
    
    # ç®¡å®¶åŸºå› ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.45, 0.05)  # ä¸­ç­‰GCå«é‡
        length = np.random.normal(1500, 300)        # ä¸­ç­‰é•¿åº¦
        conservation = np.random.normal(0.8, 0.1)   # é«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.7, 0.1)     # é«˜è¡¨è¾¾
        evolution_rate = np.random.normal(0.2, 0.05) # ä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(0)
    
    # è½¬å½•å› å­ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.55, 0.05)   # è¾ƒé«˜GCå«é‡
        length = np.random.normal(2000, 400)        # è¾ƒé•¿
        conservation = np.random.normal(0.6, 0.15)  # ä¸­ç­‰ä¿å®ˆæ€§
        expression = np.random.normal(0.3, 0.1)     # ä½è¡¨è¾¾
        evolution_rate = np.random.normal(0.4, 0.1) # ä¸­ç­‰è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(1)
    
    # æ¿€é…¶ç‰¹å¾
    for i in range(100):
        gc_content = np.random.normal(0.50, 0.05)   # ä¸­ç­‰GCå«é‡
        length = np.random.normal(2500, 500)        # é•¿
        conservation = np.random.normal(0.7, 0.1)   # è¾ƒé«˜ä¿å®ˆæ€§
        expression = np.random.normal(0.5, 0.15)    # ä¸­ç­‰è¡¨è¾¾
        evolution_rate = np.random.normal(0.3, 0.08) # è¾ƒä½è¿›åŒ–é€Ÿç‡
        
        features.append([gc_content, length, conservation, expression, evolution_rate])
        labels.append(2)
    
    # åˆ›å»ºDataFrame
    feature_names = ['GC_content', 'length', 'conservation', 'expression', 'evolution_rate']
    df = pd.DataFrame(features, columns=feature_names)
    df['function'] = labels
    
    print("ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {df['function'].value_counts().to_dict()}")
    print("\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ï¼š")
    print(df[feature_names].describe().round(3))
    
    return df, feature_names


def visualize_feature_distributions(df, feature_names):
    """å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ"""
    print("\nğŸ“Š å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    function_names = {0: 'Housekeeping', 1: 'TF', 2: 'Kinase'}
    colors = ['blue', 'orange', 'green']
    
    for idx, feature in enumerate(feature_names):
        ax = axes[idx]
        for func_id in [0, 1, 2]:
            data = df[df['function'] == func_id][feature]
            ax.hist(data, alpha=0.5, label=function_names[func_id], 
                   color=colors[func_id], bins=20)
        ax.set_xlabel(feature)
        ax.set_ylabel('Frequency')
        ax.set_title(f'Distribution of {feature}')
        ax.legend()
    
    # ç©ºç™½å­å›¾
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')
    plt.show()


def train_classification_models(df, feature_names):
    """
    è®­ç»ƒå¤šç§åˆ†ç±»æ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç”¨ä¸åŒçš„æ–¹æ³•è¯Šæ–­ç–¾ç—…
    - é€»è¾‘å›å½’ï¼šæ ¹æ®ç—‡çŠ¶çš„çº¿æ€§ç»„åˆåˆ¤æ–­
    - å†³ç­–æ ‘ï¼šåƒåŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹å›¾
    - éšæœºæ£®æ—ï¼šå¤šä¸ªåŒ»ç”ŸæŠ•ç¥¨å†³å®š
    - SVMï¼šæ‰¾åˆ°æœ€ä½³çš„è¯Šæ–­è¾¹ç•Œ
    """
    print_section("è®­ç»ƒåˆ†ç±»æ¨¡å‹")
    
    # å‡†å¤‡æ•°æ®
    X = df[feature_names].values
    y = df['function'].values
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ70%è®­ç»ƒï¼Œ30%æµ‹è¯•ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
        'å†³ç­–æ ‘': DecisionTreeClassifier(random_state=42, max_depth=5),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_estimators=100),
        'SVM': SVC(random_state=42, probability=True),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"  å¬å›ç‡: {recall:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    return results, X_test_scaled, y_test, scaler


def visualize_model_comparison(results):
    """å¯è§†åŒ–æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    
    # æå–æŒ‡æ ‡
    models = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    # åˆ›å»ºæ¯”è¾ƒå›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(models))
    width = 0.2
    
    for i, metric in enumerate(metrics):
        values = [results[model][metric] for model in models]
        ax.bar(x + i*width, values, width, label=metric.capitalize())
    
    ax.set_xlabel('Model')
    ax.set_ylabel('Score')
    ax.set_title('Model Performance Comparison')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(models, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()


def demonstrate_overfitting():
    """
    æ¼”ç¤ºè¿‡æ‹Ÿåˆç°è±¡
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    è¿‡æ‹Ÿåˆå°±åƒåŒ»å­¦ç”Ÿæ­»è®°ç¡¬èƒŒç—…ä¾‹ï¼Œä¸ç†è§£ç–¾ç—…æœ¬è´¨ï¼š
    â€¢ æ¬ æ‹Ÿåˆï¼šè¿‡äºç®€åŒ–ï¼Œæ¼æ‰é‡è¦ç‰¹å¾
    â€¢ è‰¯å¥½æ‹Ÿåˆï¼šæŠ“ä½æ ¸å¿ƒè§„å¾‹ï¼Œæ³›åŒ–èƒ½åŠ›å¼º
    â€¢ è¿‡æ‹Ÿåˆï¼šè®°ä½å™ªå£°ï¼Œæ–°æ•°æ®è¡¨ç°å·®
    
    âš–ï¸ å¹³è¡¡ä¹‹é“ï¼š
    â€¢ å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæ›´å¤šç—…ä¾‹ï¼‰
    â€¢ ç®€åŒ–æ¨¡å‹ï¼ˆæŠ“ä½ä¸»è¦ç—‡çŠ¶ï¼‰
    â€¢ æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–ï¼‰
    â€¢ äº¤å‰éªŒè¯ï¼ˆå¤šä¸­å¿ƒéªŒè¯ï¼‰
    """
    print_section("è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ")
    
    print("""
ğŸ” ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ
è¿‡æ‹Ÿåˆå°±åƒä¸€ä¸ªåŒ»å­¦ç”Ÿæ­»è®°ç¡¬èƒŒäº†æ‰€æœ‰ç—…ä¾‹ï¼Œ
ä½†ä¸ç†è§£ç–¾ç—…çš„æœ¬è´¨è§„å¾‹ï¼Œé‡åˆ°æ–°ç—…ä¾‹å°±æŸæ‰‹æ— ç­–ã€‚

ğŸ” å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ
1. å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆçœ‹æ›´å¤šç—…ä¾‹ï¼‰
2. ç®€åŒ–æ¨¡å‹ï¼ˆæŠ“ä½ä¸»è¦ç—‡çŠ¶ï¼‰
3. æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–æŸä¸ªç‰¹å¾ï¼‰
4. äº¤å‰éªŒè¯ï¼ˆåœ¨ä¸åŒåŒ»é™¢éªŒè¯ï¼‰
5. æ—©åœï¼ˆé€‚å¯è€Œæ­¢ï¼‰
    """)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.sort(np.random.rand(100, 1) * 10, axis=0)
    y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    
    # è®­ç»ƒä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import Pipeline
    
    degrees = [1, 4, 15]
    titles = ['Underfitting', 'Good Fit', 'Overfitting']
    
    for ax, degree, title in zip(axes, degrees, titles):
        # åˆ›å»ºå¤šé¡¹å¼å›å½’
        polynomial_features = PolynomialFeatures(degree=degree)
        linear_regression = LinearRegression()
        pipeline = Pipeline([
            ("polynomial_features", polynomial_features),
            ("linear_regression", linear_regression)
        ])
        
        pipeline.fit(X, y)
        
        # é¢„æµ‹
        X_plot = np.linspace(0, 10, 300)[:, np.newaxis]
        y_plot = pipeline.predict(X_plot)
        
        # ç»˜å›¾
        ax.scatter(X, y, alpha=0.5, label='Data')
        ax.plot(X_plot, y_plot, color='red', label=f'Degree {degree}')
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title(title)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('overfitting_demo.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°
# ====================

def create_cell_expression_data():
    """
    åˆ›å»ºå•ç»†èƒåŸºå› è¡¨è¾¾æ•°æ®
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    å•ç»†èƒRNAæµ‹åºæ­ç¤ºç»†èƒå¼‚è´¨æ€§ï¼Œä¸åŒç»†èƒç±»å‹æœ‰ç‹¬ç‰¹çš„è¡¨è¾¾è°±ï¼š
    - å¹²ç»†èƒï¼šå¤šèƒ½æ€§åŸºå› ï¼ˆOCT4, NANOGï¼‰é«˜è¡¨è¾¾
    - åˆ†åŒ–ä¸­ç»†èƒï¼šè¿‡æ¸¡çŠ¶æ€ï¼Œè¡¨è¾¾è°±åŠ¨æ€å˜åŒ–
    - æˆç†Ÿç»†èƒï¼šç‰¹å¼‚æ€§åŠŸèƒ½åŸºå› é«˜è¡¨è¾¾
    
    ğŸ”¬ ç§‘å­¦æ„ä¹‰ï¼š
    â€¢ å‘ç°æ–°çš„ç»†èƒäºšå‹å’ŒçŠ¶æ€
    â€¢ ç†è§£ç»†èƒåˆ†åŒ–è½¨è¿¹
    â€¢ è¯†åˆ«ç–¾ç—…ç›¸å…³çš„ç»†èƒç¾¤ä½“
    """
    print_section("Part 2: æ— ç›‘ç£å­¦ä¹  - ç»†èƒäºšå‹å‘ç°")
    
    np.random.seed(42)
    
    # åˆ›å»ºä¸‰ç§ç»†èƒç±»å‹çš„è¡¨è¾¾æ•°æ®
    n_genes = 50
    gene_names = [f"Gene_{i:03d}" for i in range(1, n_genes+1)]
    
    # å¹²ç»†èƒï¼ˆé«˜è¡¨è¾¾å¤šèƒ½æ€§åŸºå› ï¼‰
    stem_cells = np.random.lognormal(2, 0.5, (30, n_genes))
    stem_cells[:, :10] *= 2  # å‰10ä¸ªåŸºå› é«˜è¡¨è¾¾ï¼ˆå¹²æ€§æ ‡è®°ï¼‰
    
    # åˆ†åŒ–ä¸­çš„ç»†èƒï¼ˆä¸­é—´çŠ¶æ€ï¼‰
    diff_cells = np.random.lognormal(2, 0.6, (40, n_genes))
    diff_cells[:, 10:25] *= 1.8  # ä¸­é—´åŸºå› é«˜è¡¨è¾¾ï¼ˆåˆ†åŒ–æ ‡è®°ï¼‰
    
    # æˆç†Ÿç»†èƒï¼ˆç‰¹å®šåŸºå› é«˜è¡¨è¾¾ï¼‰
    mature_cells = np.random.lognormal(2, 0.4, (30, n_genes))
    mature_cells[:, 30:45] *= 2.5  # åé¢åŸºå› é«˜è¡¨è¾¾ï¼ˆæˆç†Ÿæ ‡è®°ï¼‰
    
    # åˆå¹¶æ•°æ®
    all_cells = np.vstack([stem_cells, diff_cells, mature_cells])
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾ï¼ˆç”¨äºéªŒè¯ï¼Œä½†ä¸ç”¨äºè®­ç»ƒï¼‰
    true_labels = np.array([0]*30 + [1]*40 + [2]*30)
    
    # åˆ›å»ºDataFrame
    cell_names = [f"Cell_{i:03d}" for i in range(1, 101)]
    df = pd.DataFrame(all_cells, columns=gene_names, index=cell_names)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"  ç»†èƒæ•°é‡: {df.shape[0]}")
    print(f"  åŸºå› æ•°é‡: {df.shape[1]}")
    print(f"  è¡¨è¾¾å€¼èŒƒå›´: [{df.values.min():.2f}, {df.values.max():.2f}]")
    
    return df, true_labels


def perform_clustering_analysis(df, true_labels):
    """
    æ‰§è¡Œå¤šç§èšç±»åˆ†æ
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    èšç±»åˆ†æå¦‚åŒç»†èƒåˆ†ç±»å­¦ï¼š
    â€¢ K-meansï¼šæ‰¾åˆ°ç»†èƒç¾¤çš„"ä»£è¡¨ç»†èƒ"
    â€¢ å±‚æ¬¡èšç±»ï¼šæ„å»ºç»†èƒè¿›åŒ–æ ‘
    â€¢ DBSCANï¼šè¯†åˆ«å¯†é›†ç»†èƒå›¢å’Œå¼‚å¸¸ç»†èƒ
    â€¢ é«˜æ–¯æ··åˆï¼šç»†èƒç¾¤çš„æ¦‚ç‡è¾¹ç•Œ
    
    ğŸ”¬ è¯„ä¼°æ ‡å‡†ï¼š
    â€¢ è½®å»“ç³»æ•°ï¼šç¾¤å†…ç´§å¯†ï¼Œç¾¤é—´åˆ†ç¦»
    â€¢ è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼šä¸çœŸå®åˆ†ç»„çš„ä¸€è‡´æ€§
    """
    print_section("èšç±»åˆ†æ")
    
    # æ•°æ®é¢„å¤„ç†
    # 1. å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†åæ€åˆ†å¸ƒï¼‰
    df_log = np.log1p(df)
    
    # 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_log)
    
    # 3. PCAé™ç»´ï¼ˆå¯è§†åŒ–ç”¨ï¼‰
    pca = PCA(n_components=2, random_state=42)
    df_pca = pca.fit_transform(df_scaled)
    
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.1%}")
    
    # å°è¯•ä¸åŒçš„èšç±»ç®—æ³•
    clustering_methods = {
        'K-Means': KMeans(n_clusters=3, random_state=42),
        'DBSCAN': DBSCAN(eps=3, min_samples=5),
        'Hierarchical': AgglomerativeClustering(n_clusters=3),
        'GMM': GaussianMixture(n_components=3, random_state=42)
    }
    
    results = {}
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (name, clusterer) in enumerate(clustering_methods.items()):
        print(f"\næ‰§è¡Œ {name} èšç±»...")
        
        # èšç±»
        if name == 'GMM':
            labels = clusterer.fit_predict(df_scaled)
        else:
            labels = clusterer.fit_predict(df_scaled)
        
        # è¯„ä¼°
        if len(np.unique(labels)) > 1:
            silhouette = silhouette_score(df_scaled, labels)
            ari = adjusted_rand_score(true_labels, labels)
        else:
            silhouette = -1
            ari = -1
        
        results[name] = {
            'labels': labels,
            'silhouette': silhouette,
            'ari': ari,
            'n_clusters': len(np.unique(labels[labels >= 0]))
        }
        
        print(f"  èšç±»æ•°: {results[name]['n_clusters']}")
        print(f"  è½®å»“ç³»æ•°: {silhouette:.3f}")
        print(f"  è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari:.3f}")
        
        # å¯è§†åŒ–
        ax = axes[idx]
        scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                           c=labels, cmap='viridis', 
                           alpha=0.6, s=30)
        ax.set_title(f'{name}\nSilhouette: {silhouette:.3f}')
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        plt.colorbar(scatter, ax=ax)
    
    # çœŸå®æ ‡ç­¾
    ax = axes[4]
    scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
                       c=true_labels, cmap='Set1', 
                       alpha=0.6, s=30)
    ax.set_title('True Labels')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    plt.colorbar(scatter, ax=ax)
    
    # ç©ºç™½
    axes[5].axis('off')
    
    plt.tight_layout()
    plt.savefig('clustering_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    return results, df_scaled, pca


def determine_optimal_clusters(df_scaled):
    """
    ç¡®å®šæœ€ä¼˜èšç±»æ•°
    
    ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    å°±åƒç¡®å®šç»„ç»‡ä¸­æœ‰å‡ ç§ä¸åŒçš„ç»†èƒç±»å‹
    """
    print_section("ç¡®å®šæœ€ä¼˜èšç±»æ•°")
    
    k_range = range(2, 11)
    inertias = []
    silhouettes = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(df_scaled)
        
        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(df_scaled, labels))
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # è‚˜éƒ¨æ³•åˆ™
    axes[0].plot(k_range, inertias, 'bo-')
    axes[0].set_xlabel('Number of Clusters (k)')
    axes[0].set_ylabel('Inertia')
    axes[0].set_title('Elbow Method')
    axes[0].grid(True, alpha=0.3)
    
    # è½®å»“ç³»æ•°
    axes[1].plot(k_range, silhouettes, 'ro-')
    axes[1].set_xlabel('Number of Clusters (k)')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].set_title('Silhouette Analysis')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('optimal_k.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    optimal_k = k_range[np.argmax(silhouettes)]
    print(f"ğŸ¯ åŸºäºè½®å»“ç³»æ•°ï¼Œæœ€ä¼˜èšç±»æ•°ä¸º: {optimal_k}")
    
    return optimal_k


def create_cluster_heatmap(df, labels):
    """åˆ›å»ºèšç±»çƒ­å›¾"""
    print("\nğŸ“Š åˆ›å»ºèšç±»çƒ­å›¾")
    
    # æŒ‰èšç±»æ ‡ç­¾æ’åº
    sorted_idx = np.argsort(labels)
    df_sorted = df.iloc[sorted_idx]
    labels_sorted = labels[sorted_idx]
    
    # é€‰æ‹©å·®å¼‚æœ€å¤§çš„åŸºå› 
    gene_std = df.std(axis=0)
    top_genes = gene_std.nlargest(20).index
    
    # åˆ›å»ºçƒ­å›¾
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºèšç±»é¢œè‰²æ¡
    cluster_colors = ['red', 'blue', 'green', 'orange', 'purple']
    row_colors = [cluster_colors[label] if label >= 0 else 'gray' 
                  for label in labels_sorted]
    
    # ç»˜åˆ¶çƒ­å›¾
    g = sns.clustermap(df_sorted[top_genes], 
                       row_colors=row_colors,
                       cmap='RdBu_r',
                       center=0,
                       row_cluster=False,
                       col_cluster=True,
                       figsize=(10, 8),
                       cbar_kws={'label': 'Expression'})
    
    g.ax_heatmap.set_xlabel('Genes')
    g.ax_heatmap.set_ylabel('Cells')
    plt.suptitle('Cell Clustering Heatmap', y=1.02)
    
    plt.savefig('cluster_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 3: ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–
# ====================

def demonstrate_feature_engineering():
    """
    æ¼”ç¤ºç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    ç‰¹å¾å·¥ç¨‹ = é€‰æ‹©æœ€ä½³ç”Ÿç‰©æ ‡è®°ç‰©
    å°±åƒé€‰æ‹©æœ€èƒ½ä»£è¡¨ç–¾ç—…çš„æ£€æµ‹æŒ‡æ ‡
    
    ğŸ”¬ ç”Ÿç‰©ç‰¹å¾ç±»å‹ï¼š
    â€¢ åºåˆ—ç‰¹å¾ï¼šGCå«é‡ã€k-meré¢‘ç‡ã€ä¿å®ˆåŸŸ
    â€¢ ç»“æ„ç‰¹å¾ï¼šäºŒçº§ç»“æ„ã€æº¶å‰‚å¯åŠæ€§  
    â€¢ è¡¨è¾¾ç‰¹å¾ï¼šåŸºå› å…±è¡¨è¾¾ã€æ—¶åºåŠ¨æ€
    â€¢ ç½‘ç»œç‰¹å¾ï¼šè›‹ç™½ç›¸äº’ä½œç”¨ã€é€šè·¯å‚ä¸
    â€¢ ç»„åˆç‰¹å¾ï¼šç‰¹å¾äº¤äº’ã€æ¯”å€¼ã€å¤šé¡¹å¼
    
    ğŸ’¡ å·¥ç¨‹è‰ºæœ¯ï¼š
    å¥½çš„ç‰¹å¾è®©å¤æ‚é—®é¢˜å˜ç®€å•ï¼
    """
    print_section("ç‰¹å¾å·¥ç¨‹çš„è‰ºæœ¯")
    
    print("""
ğŸ”¬ ç‰¹å¾å·¥ç¨‹åœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨ï¼š

1. åºåˆ—ç‰¹å¾ï¼š
   - k-meré¢‘ç‡ï¼ˆDNA/è›‹ç™½è´¨motifï¼‰
   - GCå«é‡ã€å¯†ç å­ä½¿ç”¨åå¥½
   - äºŒçº§ç»“æ„é¢„æµ‹åˆ†æ•°

2. ç»“æ„ç‰¹å¾ï¼š
   - æ°¨åŸºé…¸ç†åŒ–æ€§è´¨
   - æº¶å‰‚å¯åŠæ€§
   - äºŒé¢è§’

3. ç½‘ç»œç‰¹å¾ï¼š
   - åŸºå› å…±è¡¨è¾¾
   - è›‹ç™½è´¨ç›¸äº’ä½œç”¨
   - ä»£è°¢é€šè·¯å‚ä¸åº¦

4. ç»„åˆç‰¹å¾ï¼š
   - ç‰¹å¾äº¤äº’é¡¹
   - å¤šé¡¹å¼ç‰¹å¾
   - ç‰¹å¾æ¯”ç‡
    """)
    
    # åˆ›å»ºç¤ºä¾‹ï¼šåºåˆ—ç‰¹å¾æå–
    def extract_sequence_features(sequence):
        """ä»DNAåºåˆ—æå–ç‰¹å¾"""
        features = {}
        
        # GCå«é‡
        gc_count = sequence.count('G') + sequence.count('C')
        features['gc_content'] = gc_count / len(sequence)
        
        # k-meré¢‘ç‡ï¼ˆ2-merä¸ºä¾‹ï¼‰
        kmers = ['AA', 'AT', 'AG', 'AC', 'TA', 'TT', 'TG', 'TC',
                 'GA', 'GT', 'GG', 'GC', 'CA', 'CT', 'CG', 'CC']
        for kmer in kmers:
            features[f'kmer_{kmer}'] = sequence.count(kmer) / (len(sequence) - 1)
        
        # åºåˆ—é•¿åº¦
        features['length'] = len(sequence)
        
        return features
    
    # ç¤ºä¾‹åºåˆ—
    sequences = [
        "ATGCGATCGTAGCTAGCTAGCTAGCTAGCTA",
        "GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCG",
        "ATATATATATATATATATATATATATATAT"
    ]
    
    print("\nç¤ºä¾‹ï¼šDNAåºåˆ—ç‰¹å¾æå–")
    for i, seq in enumerate(sequences, 1):
        features = extract_sequence_features(seq)
        print(f"\nåºåˆ—{i}: {seq[:20]}...")
        print(f"  GCå«é‡: {features['gc_content']:.2f}")
        print(f"  é•¿åº¦: {features['length']}")
        print(f"  ATäºŒèšä½“é¢‘ç‡: {features['kmer_AT']:.2f}")


def hyperparameter_tuning_demo():
    """
    æ¼”ç¤ºè¶…å‚æ•°è°ƒä¼˜
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    è¶…å‚æ•°è°ƒä¼˜å¦‚åŒä¼˜åŒ–å®éªŒæ¡ä»¶ï¼š
    â€¢ PCRï¼šæ¸©åº¦ã€æ—¶é—´ã€å¼•ç‰©æµ“åº¦
    â€¢ ç»†èƒåŸ¹å…»ï¼šæ¸©åº¦ã€CO2æµ“åº¦ã€åŸ¹å…»åŸº
    â€¢ æµ‹åºï¼šæ·±åº¦ã€è¯»é•¿ã€è´¨é‡é˜ˆå€¼
    
    ğŸ¯ ä¼˜åŒ–ç­–ç•¥ï¼š
    â€¢ ç½‘æ ¼æœç´¢ï¼šç©·å°½æ‰€æœ‰ç»„åˆï¼ˆè€—æ—¶ï¼‰
    â€¢ éšæœºæœç´¢ï¼šéšæœºé‡‡æ ·ï¼ˆé«˜æ•ˆï¼‰
    â€¢ è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå°è¯•
    """
    print_section("è¶…å‚æ•°è°ƒä¼˜")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=200, n_features=10, 
                              n_informative=5, n_redundant=5,
                              n_classes=2, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    print("ğŸ” ç½‘æ ¼æœç´¢è¶…å‚æ•°...")
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {np.prod([len(v) for v in param_grid.values()])}")
    
    # ç½‘æ ¼æœç´¢
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=5, 
                               scoring='f1', n_jobs=-1)
    
    grid_search.fit(X_train, y_train)
    
    print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.3f}")
    
    # æµ‹è¯•é›†æ€§èƒ½
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_score = f1_score(y_test, y_pred)
    print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_score:.3f}")
    
    # å¯è§†åŒ–å‚æ•°å½±å“
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å‚æ•°é‡è¦æ€§
    param_importance = []
    for param in ['n_estimators', 'max_depth']:
        param_values = results_df[f'param_{param}'].unique()
        mean_scores = []
        for val in param_values:
            mask = results_df[f'param_{param}'] == val
            mean_scores.append(results_df[mask]['mean_test_score'].mean())
        
        ax = axes[0] if param == 'n_estimators' else axes[1]
        ax.plot(param_values, mean_scores, 'o-')
        ax.set_xlabel(param)
        ax.set_ylabel('Mean CV Score')
        ax.set_title(f'Effect of {param}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('hyperparameter_tuning.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# Part 4: æ·±åº¦å­¦ä¹ åˆæ¢
# ====================

def introduction_to_deep_learning():
    """
    æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åº”ç”¨ä»‹ç»
    
    ğŸ§¬ ç”Ÿç‰©å­¦ç±»æ¯”ï¼š
    æ·±åº¦å­¦ä¹  = å¤§è„‘çš„åˆ†å±‚å¤„ç†æœºåˆ¶
    â€¢ ç¬¬ä¸€å±‚ï¼šè¯†åˆ«è¾¹ç¼˜ï¼ˆç»†èƒè¾¹ç•Œï¼‰
    â€¢ ç¬¬äºŒå±‚ï¼šè¯†åˆ«å½¢çŠ¶ï¼ˆç»†èƒæ ¸ï¼‰
    â€¢ ç¬¬ä¸‰å±‚ï¼šè¯†åˆ«æ¨¡å¼ï¼ˆç™Œç»†èƒç‰¹å¾ï¼‰
    â€¢ è¾“å‡ºå±‚ï¼šåšå‡ºåˆ¤æ–­ï¼ˆè¯Šæ–­ç»“æœï¼‰
    
    ğŸš€ é©å‘½æ€§çªç ´ï¼š
    â€¢ AlphaFold2ï¼šç ´è§£50å¹´è›‹ç™½è´¨ç»“æ„éš¾é¢˜  
    â€¢ åŸºå› è¡¨è¾¾é¢„æµ‹ï¼šä»åºåˆ—é¢„æµ‹è¡¨è¾¾æ¨¡å¼
    â€¢ è¯ç‰©è®¾è®¡ï¼šç§’çº§ç”Ÿæˆå€™é€‰è¯ç‰©åˆ†å­
    â€¢ ç»†èƒå›¾åƒåˆ†æï¼šè¶…è¶Šäººç±»ä¸“å®¶ç²¾åº¦
    """
    print_section("æ·±åº¦å­¦ä¹ åˆæ¢")
    
    print("""
ğŸ¤– æ·±åº¦å­¦ä¹ ï¼šç”Ÿç‰©ä¿¡æ¯å­¦çš„æ–°çºªå…ƒ

ğŸ† çªç ´æ€§åº”ç”¨ï¼š

1ï¸âƒ£ AlphaFold2 - ç ´è§£50å¹´è›‹ç™½è´¨ç»“æ„éš¾é¢˜
   â€¢ è¾“å…¥ï¼šæ°¨åŸºé…¸åºåˆ— â†’ è¾“å‡ºï¼š3DåŸå­ç»“æ„
   â€¢ å‡†ç¡®åº¦ï¼šåŸå­çº§ç²¾åº¦ï¼ˆGDT > 90ï¼‰
   â€¢ å½±å“ï¼šåŠ é€Ÿè¯ç‰©ç ”å‘10å€

2ï¸âƒ£ Enformer - åŸºå› è¡¨è¾¾é¢„æµ‹å¤§æ¨¡å‹
   â€¢ è¾“å…¥ï¼š200kb DNAåºåˆ— â†’ è¾“å‡ºï¼šè¡¨è¾¾æ¨¡å¼  
   â€¢ åº”ç”¨ï¼šç†è§£åŸºå› è°ƒæ§æœºåˆ¶
   â€¢ æ„ä¹‰ï¼šè§£å¯†éç¼–ç åŒºåŸŸåŠŸèƒ½

3ï¸âƒ£ è¯ç‰©è®¾è®¡ - ç”Ÿæˆå¼æ¨¡å‹
   â€¢ è¾“å…¥ï¼šé¶ç‚¹ç»“æ„ â†’ è¾“å‡ºï¼šå€™é€‰è¯ç‰©åˆ†å­
   â€¢ é€Ÿåº¦ï¼šç§’çº§ç”Ÿæˆ vs æœˆçº§ç­›é€‰
   â€¢ æˆåŠŸæ¡ˆä¾‹ï¼šCOVID-19è¯ç‰©å‘ç°

4ï¸âƒ£ ç»†èƒå›¾åƒåˆ†æ - è®¡ç®—ç—…ç†å­¦
   â€¢ è¾“å…¥ï¼šæ˜¾å¾®é•œå›¾åƒ â†’ è¾“å‡ºï¼šç»†èƒç±»å‹/çŠ¶æ€
   â€¢ ç²¾åº¦ï¼šè¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³
   â€¢ åº”ç”¨ï¼šç™Œç—‡æ—©æœŸè¯Šæ–­ã€è¯ç‰©ç­›é€‰

5ï¸âƒ£ å•ç»†èƒæ·±åº¦å­¦ä¹  - scBERT/scGPT
   â€¢ è¾“å…¥ï¼šscRNA-seqæ•°æ® â†’ è¾“å‡ºï¼šç»†èƒæ³¨é‡Š
   â€¢ å‘ç°ï¼šæ–°çš„ç»†èƒäºšç¾¤å’Œè½¨è¿¹
   â€¢ é©å‘½ï¼šå•ç»†èƒç”Ÿç‰©å­¦æ–°èŒƒå¼
    """)
    
    # ç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼ˆä½¿ç”¨sklearnï¼‰
    from sklearn.neural_network import MLPClassifier
    
    print("\nğŸ“Š ç®€å•ç¥ç»ç½‘ç»œç¤ºä¾‹")
    
    # åˆ›å»ºXORé—®é¢˜ï¼ˆéçº¿æ€§å¯åˆ†ï¼‰
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XORè¾“å‡º
    
    # è®­ç»ƒç¥ç»ç½‘ç»œ
    mlp = MLPClassifier(hidden_layer_sizes=(4,), 
                        activation='relu',
                        max_iter=1000,
                        random_state=42)
    mlp.fit(X, y)
    
    # é¢„æµ‹
    predictions = mlp.predict(X)
    
    print("XORé—®é¢˜ï¼ˆéœ€è¦éçº¿æ€§è¾¹ç•Œï¼‰ï¼š")
    print("è¾“å…¥ -> é¢„æœŸè¾“å‡º -> é¢„æµ‹è¾“å‡º")
    for i in range(len(X)):
        print(f"{X[i]} -> {y[i]} -> {predictions[i]}")
    
    # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # çº¿æ€§æ¨¡å‹ï¼ˆå¤±è´¥ï¼‰
    ax = axes[0]
    log_reg = LogisticRegression()
    log_reg.fit(X, y)
    
    xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),
                         np.linspace(-0.5, 1.5, 100))
    Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Linear Model (Fails)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    # ç¥ç»ç½‘ç»œï¼ˆæˆåŠŸï¼‰
    ax = axes[1]
    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='RdBu', edgecolor='black')
    ax.set_title('Neural Network (Success)')
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    
    plt.tight_layout()
    plt.savefig('neural_network_xor.png', dpi=150, bbox_inches='tight')
    plt.show()


# ====================
# ä¸»å‡½æ•°
# ====================

def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ æ•™ç¨‹
    
    å±•ç¤ºæœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å®Œæ•´åº”ç”¨æµç¨‹ï¼Œ
    ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ‰€æœ‰å…³é”®æ­¥éª¤ã€‚
    """
    
    print("="*60)
    print("ğŸ§¬ Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯")
    print("="*60)
    print("""
æ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ çš„ä¸–ç•Œï¼
åœ¨ç”Ÿç‰©å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦ä»å¤æ‚çš„æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼ã€‚
æœºå™¨å­¦ä¹ å°±æ˜¯è®©è®¡ç®—æœºå­¦ä¼šè¿™ç§æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚
    """)
    
    # Part 1: ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹  - æ•™ä¼šè®¡ç®—æœºè¯†åˆ«")
    print("="*60)
    
    # åˆ›å»ºåŸºå› åˆ†ç±»æ•°æ®
    df_genes, feature_names = create_gene_classification_data()
    
    # å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ
    visualize_feature_distributions(df_genes, feature_names)
    
    # è®­ç»ƒåˆ†ç±»æ¨¡å‹
    results, X_test, y_test, scaler = train_classification_models(df_genes, feature_names)
    
    # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
    visualize_model_comparison(results)
    
    # æ¼”ç¤ºè¿‡æ‹Ÿåˆ
    demonstrate_overfitting()
    
    # Part 2: æ— ç›‘ç£å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹  - å‘ç°éšè—æ¨¡å¼")
    print("="*60)
    
    # åˆ›å»ºç»†èƒè¡¨è¾¾æ•°æ®
    df_cells, true_labels = create_cell_expression_data()
    
    # ç¡®å®šæœ€ä¼˜èšç±»æ•°
    optimal_k = determine_optimal_clusters(StandardScaler().fit_transform(np.log1p(df_cells)))
    
    # æ‰§è¡Œèšç±»åˆ†æ
    clustering_results, df_scaled, pca = perform_clustering_analysis(df_cells, true_labels)
    
    # åˆ›å»ºèšç±»çƒ­å›¾
    best_clustering = 'K-Means'
    create_cluster_heatmap(df_cells, clustering_results[best_clustering]['labels'])
    
    # Part 3: ç‰¹å¾å·¥ç¨‹ä¸ä¼˜åŒ–
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ä¼˜åŒ–")
    print("="*60)
    
    # ç‰¹å¾å·¥ç¨‹æ¼”ç¤º
    demonstrate_feature_engineering()
    
    # è¶…å‚æ•°è°ƒä¼˜
    hyperparameter_tuning_demo()
    
    # Part 4: æ·±åº¦å­¦ä¹ 
    print("\n" + "="*60)
    print("ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ åˆæ¢")
    print("="*60)
    
    # æ·±åº¦å­¦ä¹ ä»‹ç»
    introduction_to_deep_learning()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“š è¯¾ç¨‹æ€»ç»“")
    print("="*60)
    
    print("""
ğŸ¯ æœ¬ç« æ ¸å¿ƒè¦ç‚¹ï¼š

ğŸ¤– æœºå™¨å­¦ä¹ åŸºç¡€ï¼š
   âœ… ç›‘ç£å­¦ä¹ ï¼šæœ‰è€å¸ˆçš„å­¦ä¹ ï¼ˆåˆ†ç±»ã€å›å½’ï¼‰
   âœ… æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°éšè—æ¨¡å¼ï¼ˆèšç±»ã€é™ç»´ï¼‰
   âœ… æ¨¡å‹è¯„ä¼°ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€AUC

ğŸ”§ å®è·µæŠ€èƒ½ï¼š
   âœ… æ•°æ®é¢„å¤„ç†ï¼šæ¸…æ´—ã€è½¬æ¢ã€æ ‡å‡†åŒ–
   âœ… ç‰¹å¾å·¥ç¨‹ï¼šæå–ã€é€‰æ‹©ã€æ„é€ ç‰¹å¾
   âœ… æ¨¡å‹é€‰æ‹©ï¼šæ¯”è¾ƒå¤šç§ç®—æ³•æ€§èƒ½
   âœ… è¶…å‚æ•°è°ƒä¼˜ï¼šç½‘æ ¼æœç´¢ã€è´å¶æ–¯ä¼˜åŒ–

ğŸ§¬ ç”Ÿç‰©å­¦åº”ç”¨ï¼š
   âœ… åŸºå› åŠŸèƒ½é¢„æµ‹ï¼šä»åºåˆ—åˆ°åŠŸèƒ½
   âœ… ç»†èƒç±»å‹è¯†åˆ«ï¼šå•ç»†èƒåˆ†æé©å‘½
   âœ… ç–¾ç—…åˆ†ç±»è¯Šæ–­ï¼šç²¾å‡†åŒ»ç–—åŸºç¡€
   âœ… è¯ç‰©é¶ç‚¹å‘ç°ï¼šAIåŠ é€Ÿè¯ç‰©ç ”å‘

âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹ï¼š
   ğŸš« é¿å…è¿‡æ‹Ÿåˆï¼šäº¤å‰éªŒè¯ + æ­£åˆ™åŒ–
   âš–ï¸ å¤„ç†ä¸å¹³è¡¡ï¼šæƒé‡è°ƒæ•´ + é‡‡æ ·ç­–ç•¥
   ğŸ¯ ç‰¹å¾é€‰æ‹©ï¼šå»é™¤å™ªå£°ï¼Œä¿ç•™ä¿¡å·
   ğŸ¤” ç»“æœè§£é‡Šï¼šç»“åˆé¢†åŸŸçŸ¥è¯†æ˜¯å…³é”®
    """)
    
    print("""
ğŸ† æ­å–œä½ å®Œæˆæœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ï¼

ğŸ† ä½ å·²ç»æŒæ¡ï¼š
âœ… ä½¿ç”¨scikit-learnè¿›è¡Œæœºå™¨å­¦ä¹ 
âœ… ç†è§£ç›‘ç£ä¸æ— ç›‘ç£å­¦ä¹ çš„æœ¬è´¨åŒºåˆ«
âœ… æŒæ¡æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–æŠ€å·§
âœ… åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­åº”ç”¨æœºå™¨å­¦ä¹ 
âœ… ç†è§£æ·±åº¦å­¦ä¹ çš„æ½œåŠ›å’Œåº”ç”¨

ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®ï¼š
1ï¸âƒ£ å®è·µé¡¹ç›®ï¼šå°è¯•TCGAã€GEOç­‰çœŸå®æ•°æ®é›†
2ï¸âƒ£ æ·±åº¦å­¦ä¹ ï¼šå­¦ä¹ PyTorch/TensorFlowæ¡†æ¶
3ï¸âƒ£ ä¸“ä¸šå·¥å…·ï¼šæŒæ¡Scanpyã€Seuratã€DESeq2
4ï¸âƒ£ ç«èµ›å‚ä¸ï¼šKaggleç”Ÿç‰©ä¿¡æ¯å­¦æŒ‘æˆ˜èµ›
5ï¸âƒ£ å¼€æºè´¡çŒ®ï¼šå‚ä¸ç”Ÿç‰©ä¿¡æ¯å­¦å¼€æºé¡¹ç›®

ğŸ’¡ æ ¸å¿ƒç†å¿µï¼š
â€œæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚ï¼â€

å°†AIæŠ€æœ¯ä¸ç”Ÿç‰©å­¦æ´å¯Ÿå®Œç¾ç»“åˆï¼Œ
ä½ å°†æˆä¸ºæ–°æ—¶ä»£çš„è®¡ç®—ç”Ÿç‰©å­¦å®¶ï¼

ğŸ§¬ğŸ¤–ğŸ”¬ ç»§ç»­æ¢ç´¢ï¼Œæ”¹å˜ä¸–ç•Œï¼
    """)


if __name__ == "__main__":
    main()