#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chapter 10 ç»ƒä¹ é¢˜ç­”æ¡ˆ: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

æœ¬æ–‡ä»¶åŒ…å«ç¬¬åç« æ‰€æœ‰ç»ƒä¹ é¢˜çš„å®Œæ•´ç­”æ¡ˆï¼Œ
å±•ç¤ºäº†æœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å®é™…åº”ç”¨ã€‚

ç­”æ¡ˆåŒ…æ‹¬ï¼š
1. æ•°æ®é¢„å¤„ç†å’Œæ¢ç´¢æ€§åˆ†æ
2. ç›‘ç£å­¦ä¹  - åˆ†ç±»ä»»åŠ¡  
3. æ— ç›‘ç£å­¦ä¹  - èšç±»åˆ†æ
4. æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
5. ç»¼åˆå®æˆ˜é¡¹ç›®

æ¯ä¸ªç­”æ¡ˆéƒ½åŒ…å«è¯¦ç»†çš„ç”Ÿç‰©å­¦è§£é‡Šå’Œæœ€ä½³å®è·µã€‚
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, make_blobs
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.model_selection import StratifiedKFold, validation_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# åˆ†ç±»ç®—æ³•
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve, 
                           confusion_matrix, silhouette_score, 
                           adjusted_rand_score, classification_report)

# ç‰¹å¾é€‰æ‹©
from sklearn.feature_selection import SelectKBest, f_classif, RFE

import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def solution_1_data_preprocessing():
    """
    ç­”æ¡ˆ1: æ•°æ®é¢„å¤„ç†å’Œæ¢ç´¢æ€§åˆ†æ
    
    è¿™ä¸ªç»ƒä¹ å±•ç¤ºäº†ç”Ÿç‰©æ•°æ®é¢„å¤„ç†çš„å®Œæ•´æµç¨‹ï¼Œ
    åŒ…æ‹¬ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹å’Œæ•°æ®æ ‡å‡†åŒ–ã€‚
    """
    print("=" * 60)
    print("ğŸ§¬ ç­”æ¡ˆ1: è›‹ç™½è´¨æ•°æ®é¢„å¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè›‹ç™½è´¨æ•°æ®
    np.random.seed(42)
    n_proteins = 100
    
    # ç”Ÿæˆç¬¦åˆç”Ÿç‰©å­¦ç‰¹å¾çš„è›‹ç™½è´¨æ•°æ®
    protein_data = {
        # åˆ†å­é‡: 10-100 kDaï¼Œå¯¹æ•°æ­£æ€åˆ†å¸ƒ
        'molecular_weight': np.random.lognormal(np.log(30000), 0.5, n_proteins),
        
        # ç­‰ç”µç‚¹: 4-10ï¼Œæ­£æ€åˆ†å¸ƒåå‘é…¸æ€§
        'isoelectric_point': np.random.normal(6.5, 1.5, n_proteins),
        
        # æ°¨åŸºé…¸æ•°é‡: ä¸åˆ†å­é‡ç›¸å…³ï¼Œå¹³å‡åˆ†å­é‡110 Da/æ°¨åŸºé…¸
        'amino_acid_count': None,  # ç¨åè®¡ç®—
        
        # ç–æ°´æ€§æŒ‡æ•°: -2åˆ°2ï¼Œæ­£æ€åˆ†å¸ƒ
        'hydrophobicity': np.random.normal(0, 0.8, n_proteins),
        
        # ä¸ç¨³å®šæ€§æŒ‡æ•°: 0-100ï¼ŒæŒ‡æ•°åˆ†å¸ƒ
        'instability': np.random.exponential(25, n_proteins)
    }
    
    # è®¡ç®—æ°¨åŸºé…¸æ•°é‡ï¼ˆåŸºäºåˆ†å­é‡ï¼ŒåŠ å…¥å™ªå£°ï¼‰
    protein_data['amino_acid_count'] = (protein_data['molecular_weight'] / 110 + 
                                       np.random.normal(0, 20, n_proteins)).astype(int)
    
    # ç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´å†…
    protein_data['isoelectric_point'] = np.clip(protein_data['isoelectric_point'], 3, 12)
    protein_data['amino_acid_count'] = np.clip(protein_data['amino_acid_count'], 50, 2000)
    protein_data['instability'] = np.clip(protein_data['instability'], 0, 100)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(protein_data)
    
    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼ï¼ˆæ¨¡æ‹Ÿå®é™…æ•°æ®æƒ…å†µï¼‰
    missing_indices = np.random.choice(df.index, 8, replace=False)
    missing_columns = np.random.choice(df.columns, 8, replace=True)
    for idx, col in zip(missing_indices, missing_columns):
        df.loc[idx, col] = np.nan
    
    # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼ï¼ˆæ¨¡æ‹Ÿæ•°æ®è´¨é‡é—®é¢˜ï¼‰
    outlier_indices = np.random.choice(df.index, 3, replace=False)
    df.loc[outlier_indices[0], 'molecular_weight'] = 200000  # å¼‚å¸¸å¤§çš„è›‹ç™½è´¨
    df.loc[outlier_indices[1], 'instability'] = 150  # å¼‚å¸¸é«˜çš„ä¸ç¨³å®šæ€§
    df.loc[outlier_indices[2], 'hydrophobicity'] = 5  # å¼‚å¸¸é«˜çš„ç–æ°´æ€§
    
    print("ğŸ” æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"å‰5è¡Œæ•°æ®:")
    print(df.head().round(2))
    
    print(f"\nğŸ“Š æè¿°æ€§ç»Ÿè®¡:")
    print(df.describe().round(2))
    
    print(f"\nâš ï¸ ç¼ºå¤±å€¼æ£€æŸ¥:")
    missing_summary = df.isnull().sum()
    print(missing_summary[missing_summary > 0])
    print(f"æ€»ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    
    # å¤„ç†ç¼ºå¤±å€¼ - ç”¨å‡å€¼å¡«å……æ•°å€¼å‹ç‰¹å¾
    print(f"\nğŸ”§ å¤„ç†ç¼ºå¤±å€¼...")
    df_clean = df.copy()
    for column in df_clean.columns:
        if df_clean[column].dtype in ['float64', 'int64']:
            mean_value = df_clean[column].mean()
            df_clean[column].fillna(mean_value, inplace=True)
            print(f"  {column}: ç”¨å‡å€¼ {mean_value:.2f} å¡«å……")
    
    # å¼‚å¸¸å€¼æ£€æµ‹ - ä½¿ç”¨IQRæ–¹æ³•
    print(f"\nğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹ (IQRæ–¹æ³•):")
    outliers_info = {}
    
    for column in df_clean.select_dtypes(include=[np.number]).columns:
        Q1 = df_clean[column].quantile(0.25)
        Q3 = df_clean[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df_clean[(df_clean[column] < lower_bound) | 
                           (df_clean[column] > upper_bound)]
        
        if len(outliers) > 0:
            outliers_info[column] = len(outliers)
            print(f"  {column}: {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
    
    # æ•°æ®æ ‡å‡†åŒ–
    print(f"\nâš–ï¸ æ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(
        scaler.fit_transform(df_clean),
        columns=df_clean.columns,
        index=df_clean.index
    )
    
    print("æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡:")
    print(df_scaled.describe().round(3))
    
    # ç»˜åˆ¶ç›¸å…³æ€§çƒ­å›¾
    print(f"\nğŸ“ˆ ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾...")
    plt.figure(figsize=(10, 8))
    
    correlation_matrix = df_clean.corr()
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    
    sns.heatmap(correlation_matrix, 
                mask=mask,
                annot=True, 
                cmap='coolwarm', 
                center=0,
                fmt='.2f',
                square=True)
    plt.title('è›‹ç™½è´¨ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
    plt.tight_layout()
    plt.savefig('protein_correlation_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ç”Ÿç‰©å­¦è§£é‡Š
    print(f"\nğŸ§¬ ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š:")
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.5:
                high_corr_pairs.append((
                    correlation_matrix.columns[i],
                    correlation_matrix.columns[j],
                    corr_val
                ))
    
    for feat1, feat2, corr in high_corr_pairs:
        print(f"  {feat1} ä¸ {feat2}: r={corr:.3f}")
        
        if 'molecular_weight' in [feat1, feat2] and 'amino_acid_count' in [feat1, feat2]:
            print("    â†’ ç”Ÿç‰©å­¦è§£é‡Š: åˆ†å­é‡ä¸æ°¨åŸºé…¸æ•°é‡æ­£ç›¸å…³æ˜¯åˆç†çš„")
        elif 'instability' in [feat1, feat2]:
            print("    â†’ ç”Ÿç‰©å­¦è§£é‡Š: ä¸ç¨³å®šæ€§å¯èƒ½ä¸è›‹ç™½è´¨ç»“æ„ç›¸å…³")
    
    print("âœ… ç»ƒä¹ 1å®Œæˆï¼æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æˆåŠŸçš„å…³é”®æ­¥éª¤ã€‚")
    return df_clean, df_scaled


def solution_2_gene_function_classifier():
    """
    ç­”æ¡ˆ2: åŸºå› åŠŸèƒ½åˆ†ç±»å™¨
    
    è¿™ä¸ªç»ƒä¹ å±•ç¤ºäº†ç›‘ç£å­¦ä¹ åœ¨åŸºå› åŠŸèƒ½é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œ
    æ¯”è¾ƒä¸åŒåˆ†ç±»ç®—æ³•çš„æ€§èƒ½ã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç­”æ¡ˆ2: åŸºå› åŠŸèƒ½åˆ†ç±»å™¨")
    print("=" * 60)
    
    # åˆ›å»ºåŸºå› åˆ†ç±»æ•°æ®
    np.random.seed(42)
    
    X, y = make_classification(
        n_samples=300, 
        n_features=8, 
        n_classes=3, 
        n_informative=6,
        n_redundant=2,
        n_clusters_per_class=1,
        flip_y=0.02,  # 2%çš„æ ‡ç­¾å™ªå£°
        random_state=42
    )
    
    # æ·»åŠ ç‰¹å¾åç§°ï¼ˆç”Ÿç‰©å­¦ç›¸å…³ï¼‰
    feature_names = [
        'gc_content',           # GCå«é‡
        'gene_length',          # åŸºå› é•¿åº¦
        'exon_count',           # å¤–æ˜¾å­æ•°é‡
        'intron_length',        # å†…å«å­é•¿åº¦
        'promoter_strength',    # å¯åŠ¨å­å¼ºåº¦
        'conservation_score',   # ä¿å®ˆæ€§è¯„åˆ†
        'expression_level',     # è¡¨è¾¾æ°´å¹³
        'methylation_level'     # ç”²åŸºåŒ–æ°´å¹³
    ]
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(X, columns=feature_names)
    df['function_class'] = y
    
    # æ·»åŠ ç±»åˆ«åç§°
    class_names = {0: 'ç»“æ„è›‹ç™½', 1: 'é…¶', 2: 'è°ƒèŠ‚è›‹ç™½'}
    df['function_name'] = df['function_class'].map(class_names)
    
    print("ğŸ“Š åŸºå› åˆ†ç±»æ•°æ®ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:")
    for class_id, count in df['function_class'].value_counts().items():
        print(f"  {class_names[class_id]}: {count} ({count/len(df)*100:.1f}%)")
    
    # å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
    print(f"\nğŸ“Š æ•°æ®å¯è§†åŒ–...")
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    # å­å›¾1: ç±»åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾
    ax = axes[0]
    class_counts = df['function_class'].value_counts()
    bars = ax.bar([class_names[i] for i in class_counts.index], 
                  class_counts.values,
                  color=['skyblue', 'lightcoral', 'lightgreen'])
    ax.set_title('åŸºå› åŠŸèƒ½ç±»åˆ«åˆ†å¸ƒ')
    ax.set_ylabel('æ ·æœ¬æ•°é‡')
    for bar, count in zip(bars, class_counts.values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                str(count), ha='center', va='bottom')
    
    # å­å›¾2-3: ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾
    for i, features_group in enumerate([feature_names[:4], feature_names[4:]]):
        ax = axes[i+1]
        df[features_group].boxplot(ax=ax)
        ax.set_title(f'ç‰¹å¾åˆ†å¸ƒç»„ {i+1}')
        ax.tick_params(axis='x', rotation=45)
    
    # å­å›¾4: PCAå¯è§†åŒ–
    ax = axes[3]
    pca_vis = PCA(n_components=2)
    X_pca = pca_vis.fit_transform(X)
    
    for class_id in range(3):
        mask = y == class_id
        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                  label=class_names[class_id], alpha=0.6, s=30)
    
    ax.set_xlabel(f'PC1 ({pca_vis.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca_vis.explained_variance_ratio_[1]:.1%})')
    ax.set_title('PCAå¯è§†åŒ–')
    ax.legend()
    
    # å­å›¾5: ä¸åŒç±»åˆ«ä¸‹GCå«é‡çš„åˆ†å¸ƒ
    ax = axes[4]
    for class_id in range(3):
        data = df[df['function_class'] == class_id]['gc_content']
        ax.hist(data, alpha=0.5, label=class_names[class_id], bins=15)
    ax.set_xlabel('GCå«é‡')
    ax.set_ylabel('é¢‘ç‡')
    ax.set_title('GCå«é‡åœ¨ä¸åŒç±»åˆ«ä¸­çš„åˆ†å¸ƒ')
    ax.legend()
    
    # å­å›¾6: ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾
    ax = axes[5]
    corr_subset = df[feature_names[:5]].corr()
    im = ax.imshow(corr_subset, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
    ax.set_xticks(range(5))
    ax.set_yticks(range(5))
    ax.set_xticklabels(corr_subset.columns, rotation=45, ha='right')
    ax.set_yticklabels(corr_subset.columns)
    ax.set_title('ç‰¹å¾ç›¸å…³æ€§')
    
    plt.tight_layout()
    plt.savefig('gene_classification_eda.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # æ•°æ®åˆ†å‰²å’Œé¢„å¤„ç†
    print(f"\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒå¤šä¸ªåˆ†ç±»æ¨¡å‹
    print(f"\nğŸ¤– è®­ç»ƒåˆ†ç±»æ¨¡å‹...")
    
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
        'å†³ç­–æ ‘': DecisionTreeClassifier(random_state=42, max_depth=10),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_estimators=100),
        'SVM': SVC(random_state=42, probability=True),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1_weighted')
        
        results[name] = {
            'accuracy': accuracy,
            'precision': precision, 
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba,
            'model': model
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"  å¬å›ç‡: {recall:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯F1: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    # å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ...")
    plt.figure(figsize=(12, 6))
    
    model_names = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_labels = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
    
    x = np.arange(len(model_names))
    width = 0.2
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        values = [results[model][metric] for model in model_names]
        plt.bar(x + i*width, values, width, label=label, alpha=0.8)
    
    plt.xlabel('æ¨¡å‹')
    plt.ylabel('åˆ†æ•°')
    plt.title('æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ')
    plt.xticks(x + width * 1.5, model_names, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('model_performance_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['f1'])
    best_result = results[best_model_name]
    best_pred = best_result['y_pred']
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"æœ€ä½³F1åˆ†æ•°: {best_result['f1']:.3f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, best_pred, 
                              target_names=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½']))
    
    # æ··æ·†çŸ©é˜µ
    print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ...")
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, best_pred)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½'],
                yticklabels=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½'])
    plt.title(f'{best_model_name} - æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    
    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ ç™¾åˆ†æ¯”
    total = cm.sum()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            percentage = cm[i, j] / total * 100
            plt.text(j+0.5, i+0.7, f'({percentage:.1f}%)', 
                    ha='center', va='center', fontsize=10, color='gray')
    
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    if hasattr(best_result['model'], 'feature_importances_'):
        print(f"\nğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        importances = best_result['model'].feature_importances_
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        print("ç‰¹å¾é‡è¦æ€§æ’å:")
        for idx, row in feature_importance_df.iterrows():
            print(f"  {row['feature']}: {row['importance']:.3f}")
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(feature_names)), importances[np.argsort(importances)])
        plt.yticks(range(len(feature_names)), 
                   [feature_names[i] for i in np.argsort(importances)])
        plt.xlabel('é‡è¦æ€§åˆ†æ•°')
        plt.title(f'{best_model_name} - ç‰¹å¾é‡è¦æ€§')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    # ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š
    print(f"\nğŸ§¬ ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š:")
    print(f"åœ¨åŸºå› åŠŸèƒ½åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œ{best_model_name}è¡¨ç°æœ€ä½³:")
    print(f"â€¢ ç»“æ„è›‹ç™½åŸºå› é€šå¸¸å…·æœ‰ç›¸å¯¹ç¨³å®šçš„è¡¨è¾¾æ¨¡å¼")
    print(f"â€¢ é…¶åŸºå› å¾€å¾€æœ‰ç‰¹å®šçš„åºåˆ—ä¿å®ˆåŸŸ")  
    print(f"â€¢ è°ƒèŠ‚è›‹ç™½åŸºå› è¡¨è¾¾å—åˆ°ä¸¥æ ¼çš„æ—¶ç©ºè°ƒæ§")
    print(f"â€¢ GCå«é‡ã€åŸºå› é•¿åº¦ç­‰åºåˆ—ç‰¹å¾æ˜¯é‡è¦çš„é¢„æµ‹å› å­")
    
    print("âœ… ç»ƒä¹ 2å®Œæˆï¼ç›‘ç£å­¦ä¹ å¯ä»¥æœ‰æ•ˆé¢„æµ‹åŸºå› åŠŸèƒ½ã€‚")
    return results, feature_names


def solution_3_cell_clustering():
    """
    ç­”æ¡ˆ3: å•ç»†èƒèšç±»åˆ†æ
    
    è¿™ä¸ªç»ƒä¹ å±•ç¤ºäº†æ— ç›‘ç£å­¦ä¹ åœ¨å•ç»†èƒæ•°æ®åˆ†æä¸­çš„åº”ç”¨ï¼Œ
    å¸®åŠ©å‘ç°ä¸åŒçš„ç»†èƒäºšå‹ã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç­”æ¡ˆ3: å•ç»†èƒèšç±»åˆ†æ")
    print("=" * 60)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå•ç»†èƒæ•°æ®
    np.random.seed(42)
    
    # åˆ›å»ºä¸‰ä¸ªç»†èƒç¾¤ï¼Œæ¨¡æ‹Ÿä¸åŒçš„è¡¨è¾¾æ¨¡å¼
    X, true_labels = make_blobs(
        n_samples=100, 
        n_features=50, 
        centers=3,
        cluster_std=1.5,
        random_state=42
    )
    
    # æ¨¡æ‹ŸåŸºå› è¡¨è¾¾æ•°æ®ç‰¹ç‚¹ - è½¬æ¢ä¸ºå¯¹æ•°æ­£æ€åˆ†å¸ƒ
    X = np.exp(X)  # ä½¿æ•°æ®å‘ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒ
    X = np.maximum(X, 0.1)  # ç¡®ä¿æ‰€æœ‰å€¼ä¸ºæ­£
    
    # åˆ›å»ºåŸºå› å’Œç»†èƒåç§°
    gene_names = [f'Gene_{i:02d}' for i in range(1, 51)]
    cell_names = [f'Cell_{i:03d}' for i in range(1, 101)]
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(X, columns=gene_names, index=cell_names)
    
    # ä¸ºäº†æ›´å¥½åœ°æ¨¡æ‹Ÿå•ç»†èƒæ•°æ®ï¼Œæ·»åŠ ä¸€äº›ç‰¹æ®Šè¡¨è¾¾æ¨¡å¼
    # è®©ä¸åŒç»†èƒç¾¤åœ¨ç‰¹å®šåŸºå› ä¸Šæœ‰å·®å¼‚è¡¨è¾¾
    cell_types = {0: 'Stem_cells', 1: 'Differentiated', 2: 'Mature_cells'}
    
    # å¹²ç»†èƒç‰¹å¼‚æ€§åŸºå› ï¼ˆå‰10ä¸ªåŸºå› ï¼‰
    stem_mask = true_labels == 0
    df.loc[df.index[stem_mask], gene_names[:10]] *= 3
    
    # åˆ†åŒ–ç»†èƒç‰¹å¼‚æ€§åŸºå› ï¼ˆä¸­é—´10ä¸ªåŸºå› ï¼‰
    diff_mask = true_labels == 1
    df.loc[df.index[diff_mask], gene_names[20:30]] *= 2.5
    
    # æˆç†Ÿç»†èƒç‰¹å¼‚æ€§åŸºå› ï¼ˆå10ä¸ªåŸºå› ï¼‰
    mature_mask = true_labels == 2
    df.loc[df.index[mature_mask], gene_names[40:50]] *= 4
    
    print("ğŸ”¬ å•ç»†èƒæ•°æ®ä¿¡æ¯:")
    print(f"ç»†èƒæ•°é‡: {df.shape[0]}")
    print(f"åŸºå› æ•°é‡: {df.shape[1]}")
    print(f"è¡¨è¾¾å€¼èŒƒå›´: [{df.values.min():.2f}, {df.values.max():.2f}]")
    
    # çœŸå®ç»†èƒç±»å‹åˆ†å¸ƒ
    true_distribution = pd.Series(true_labels).value_counts().sort_index()
    print(f"\nçœŸå®ç»†èƒç±»å‹åˆ†å¸ƒ:")
    for cell_id, count in true_distribution.items():
        print(f"  {cell_types[cell_id]}: {count} ({count/len(df)*100:.1f}%)")
    
    # æ•°æ®é¢„å¤„ç†
    print(f"\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
    
    # 1. å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†åæ€åˆ†å¸ƒï¼‰
    df_log = np.log1p(df)
    print(f"å¯¹æ•°è½¬æ¢åèŒƒå›´: [{df_log.values.min():.2f}, {df_log.values.max():.2f}]")
    
    # 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_log)
    print(f"æ ‡å‡†åŒ–åå‡å€¼: {df_scaled.mean():.3f}, æ ‡å‡†å·®: {df_scaled.std():.3f}")
    
    # PCAé™ç»´åˆ†æ
    print(f"\nğŸ” ä¸»æˆåˆ†åˆ†æ...")
    pca = PCA(n_components=10)  # å…ˆçœ‹å‰10ä¸ªä¸»æˆåˆ†
    df_pca_full = pca.fit_transform(df_scaled)
    
    # é€‰æ‹©å‰ä¸¤ä¸ªä¸»æˆåˆ†ç”¨äºå¯è§†åŒ–
    df_pca = df_pca_full[:, :2]
    
    explained_var_ratio = pca.explained_variance_ratio_
    print(f"å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”: {explained_var_ratio}")
    print(f"PC1è§£é‡Šæ–¹å·®: {explained_var_ratio[0]:.1%}")
    print(f"PC2è§£é‡Šæ–¹å·®: {explained_var_ratio[1]:.1%}")
    print(f"å‰ä¸¤ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {explained_var_ratio[:2].sum():.1%}")
    
    # ç»˜åˆ¶æ–¹å·®è§£é‡Šæ¯”ä¾‹
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.bar(range(1, 11), explained_var_ratio[:10])
    plt.xlabel('ä¸»æˆåˆ†')
    plt.ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    plt.title('å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(range(1, 11), np.cumsum(explained_var_ratio[:10]), 'bo-')
    plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
    plt.ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    plt.title('ç´¯è®¡è§£é‡Šæ–¹å·®')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('pca_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # å°è¯•ä¸åŒçš„èšç±»ç®—æ³•
    print(f"\nğŸ¯ èšç±»åˆ†æ...")
    
    clustering_methods = {
        'K-Means': KMeans(n_clusters=3, random_state=42, n_init=10),
        'DBSCAN': DBSCAN(eps=1.0, min_samples=5),
        'å±‚æ¬¡èšç±»': AgglomerativeClustering(n_clusters=3),
        'é«˜æ–¯æ··åˆ': GaussianMixture(n_components=3, random_state=42)
    }
    
    clustering_results = {}
    
    for name, clusterer in clustering_methods.items():
        print(f"\næ‰§è¡Œ {name} èšç±»...")
        
        # èšç±»
        if name == 'é«˜æ–¯æ··åˆ':
            clusterer.fit(df_scaled)
            labels = clusterer.predict(df_scaled)
        else:
            labels = clusterer.fit_predict(df_scaled)
        
        # è¯„ä¼°æŒ‡æ ‡
        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)
        
        if n_clusters > 1 and n_clusters < len(df):
            silhouette = silhouette_score(df_scaled, labels)
            ari = adjusted_rand_score(true_labels, labels)
        else:
            silhouette = -1
            ari = -1
        
        clustering_results[name] = {
            'labels': labels,
            'silhouette': silhouette,
            'ari': ari,
            'n_clusters': n_clusters
        }
        
        print(f"  å‘ç°èšç±»æ•°: {n_clusters}")
        print(f"  è½®å»“ç³»æ•°: {silhouette:.3f}")
        print(f"  è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari:.3f}")
        
        if n_clusters > 0:
            cluster_sizes = np.bincount(labels[labels >= 0])
            print(f"  èšç±»å¤§å°: {cluster_sizes}")
    
    # å¯è§†åŒ–èšç±»ç»“æœ
    print(f"\nğŸ“Š å¯è§†åŒ–èšç±»ç»“æœ...")
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (name, result) in enumerate(clustering_results.items()):
        ax = axes[idx]
        labels = result['labels']
        
        if result['n_clusters'] > 0:
            # å¤„ç†DBSCANçš„å™ªå£°ç‚¹
            unique_labels = set(labels)
            colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))
            
            for label_id, color in zip(unique_labels, colors):
                if label_id == -1:
                    # å™ªå£°ç‚¹ç”¨é»‘è‰²è¡¨ç¤º
                    mask = labels == label_id
                    ax.scatter(df_pca[mask, 0], df_pca[mask, 1], 
                             c='black', marker='x', s=20, alpha=0.5, label='å™ªå£°')
                else:
                    mask = labels == label_id
                    ax.scatter(df_pca[mask, 0], df_pca[mask, 1], 
                             c=[color], alpha=0.6, s=30, label=f'ç¾¤{label_id}')
        
        ax.set_title(f'{name}\nè½®å»“ç³»æ•°: {result["silhouette"]:.3f}')
        ax.set_xlabel(f'PC1 ({explained_var_ratio[0]:.1%})')
        ax.set_ylabel(f'PC2 ({explained_var_ratio[1]:.1%})')
        
        if idx < 4:  # åªåœ¨å‰4ä¸ªå­å›¾æ·»åŠ å›¾ä¾‹
            ax.legend(fontsize=8, loc='best')
    
    # çœŸå®æ ‡ç­¾
    ax = axes[4]
    for true_label in range(3):
        mask = true_labels == true_label
        ax.scatter(df_pca[mask, 0], df_pca[mask, 1], 
                  label=cell_types[true_label], alpha=0.6, s=30)
    ax.set_title('çœŸå®ç»†èƒç±»å‹')
    ax.set_xlabel(f'PC1 ({explained_var_ratio[0]:.1%})')
    ax.set_ylabel(f'PC2 ({explained_var_ratio[1]:.1%})')
    ax.legend(fontsize=8)
    
    # ç©ºç™½
    axes[5].axis('off')
    
    plt.tight_layout()
    plt.savefig('clustering_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # é€‰æ‹©æœ€ä½³èšç±»æ–¹æ³•
    valid_methods = {k: v for k, v in clustering_results.items() 
                    if v['silhouette'] > 0}
    
    if valid_methods:
        best_method = max(valid_methods.keys(), 
                         key=lambda k: valid_methods[k]['silhouette'])
        best_labels = clustering_results[best_method]['labels']
        
        print(f"\nğŸ† æœ€ä½³èšç±»æ–¹æ³•: {best_method}")
        print(f"è½®å»“ç³»æ•°: {clustering_results[best_method]['silhouette']:.3f}")
        print(f"è°ƒæ•´å…°å¾·æŒ‡æ•°: {clustering_results[best_method]['ari']:.3f}")
        
        # åˆ›å»ºèšç±»çƒ­å›¾
        print(f"\nğŸ¨ ç»˜åˆ¶èšç±»çƒ­å›¾...")
        
        # é€‰æ‹©è¡¨è¾¾å˜å¼‚æœ€å¤§çš„20ä¸ªåŸºå› 
        gene_std = df.std(axis=0)
        top_genes = gene_std.nlargest(20).index
        
        # æŒ‰èšç±»æ ‡ç­¾æ’åºç»†èƒ
        if best_labels is not None:
            # å¤„ç†å™ªå£°ç‚¹ï¼ˆDBSCANäº§ç”Ÿçš„-1æ ‡ç­¾ï¼‰
            valid_cells = best_labels >= 0
            sorted_indices = []
            
            for cluster_id in sorted(set(best_labels[best_labels >= 0])):
                cluster_cells = np.where((best_labels == cluster_id) & valid_cells)[0]
                sorted_indices.extend(cluster_cells)
            
            # æ·»åŠ å™ªå£°ç‚¹åˆ°æœ«å°¾
            if -1 in best_labels:
                noise_cells = np.where(best_labels == -1)[0]
                sorted_indices.extend(noise_cells)
            
            df_sorted = df.iloc[sorted_indices]
            labels_sorted = best_labels[sorted_indices]
            
            # åˆ›å»ºèšç±»çƒ­å›¾
            plt.figure(figsize=(12, 8))
            
            # åˆ›å»ºè¡Œé¢œè‰²ï¼ˆä»£è¡¨èšç±»ï¼‰
            unique_clusters = sorted(set(labels_sorted[labels_sorted >= 0]))
            if -1 in labels_sorted:
                unique_clusters.append(-1)
            
            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_clusters)))
            color_map = dict(zip(unique_clusters, colors))
            
            row_colors = [color_map[label] for label in labels_sorted]
            
            # ç»˜åˆ¶çƒ­å›¾
            g = sns.clustermap(
                df_sorted[top_genes], 
                row_colors=row_colors,
                cmap='RdBu_r',
                center=df_sorted[top_genes].median().median(),
                row_cluster=False,  # ä¸å¯¹è¡Œèšç±»ï¼Œä¿æŒæˆ‘ä»¬çš„æ’åº
                col_cluster=True,   # å¯¹åˆ—ï¼ˆåŸºå› ï¼‰èšç±»
                figsize=(12, 8),
                cbar_kws={'label': 'è¡¨è¾¾æ°´å¹³'},
                yticklabels=False  # éšè—ç»†èƒåç§°
            )
            
            g.ax_heatmap.set_xlabel('åŸºå› ')
            g.ax_heatmap.set_ylabel('ç»†èƒ')
            plt.suptitle(f'{best_method} - ç»†èƒèšç±»çƒ­å›¾', y=1.02)
            
            plt.savefig('cell_clustering_heatmap.png', dpi=150, bbox_inches='tight')
            plt.show()
            
            # åˆ†ææ¯ä¸ªèšç±»çš„ç‰¹å¾åŸºå› 
            print(f"\nğŸ§¬ èšç±»ç”Ÿç‰©å­¦ç‰¹å¾åˆ†æ:")
            
            for cluster_id in unique_clusters:
                if cluster_id >= 0:  # è·³è¿‡å™ªå£°ç‚¹
                    cluster_cells = best_labels == cluster_id
                    cluster_size = sum(cluster_cells)
                    
                    # è®¡ç®—è¯¥èšç±»çš„å¹³å‡è¡¨è¾¾
                    cluster_mean = df.loc[df.index[cluster_cells]].mean()
                    
                    # æ‰¾å‡ºé«˜è¡¨è¾¾åŸºå› ï¼ˆå‰5ä¸ªï¼‰
                    top_expressed_genes = cluster_mean.nlargest(5)
                    
                    print(f"\nèšç±» {cluster_id} ({cluster_size} ä¸ªç»†èƒ):")
                    print(f"  é«˜è¡¨è¾¾åŸºå› :")
                    for gene, expr in top_expressed_genes.items():
                        print(f"    {gene}: {expr:.2f}")
                    
                    # æ¨æµ‹ç»†èƒç±»å‹
                    if gene in gene_names[:10]:  # å¹²ç»†èƒç‰¹å¼‚æ€§åŸºå› 
                        print(f"  å¯èƒ½çš„ç»†èƒç±»å‹: å¹²ç»†èƒ")
                    elif gene in gene_names[20:30]:  # åˆ†åŒ–ç»†èƒç‰¹å¼‚æ€§åŸºå› 
                        print(f"  å¯èƒ½çš„ç»†èƒç±»å‹: åˆ†åŒ–ä¸­ç»†èƒ")
                    elif gene in gene_names[40:50]:  # æˆç†Ÿç»†èƒç‰¹å¼‚æ€§åŸºå› 
                        print(f"  å¯èƒ½çš„ç»†èƒç±»å‹: æˆç†Ÿç»†èƒ")
    
    # ç”Ÿç‰©å­¦æ„ä¹‰æ€»ç»“
    print(f"\nğŸ§¬ ç”Ÿç‰©å­¦æ„ä¹‰æ€»ç»“:")
    print(f"å•ç»†èƒèšç±»åˆ†æå¸®åŠ©æˆ‘ä»¬:")
    print(f"â€¢ è¯†åˆ«ä¸åŒçš„ç»†èƒäºšå‹å’ŒçŠ¶æ€")
    print(f"â€¢ å‘ç°ç»†èƒåˆ†åŒ–è½¨è¿¹å’Œè°±ç³»å…³ç³»")
    print(f"â€¢ æ‰¾åˆ°ç‰¹å¼‚æ€§è¡¨è¾¾çš„markeråŸºå› ")
    print(f"â€¢ ç†è§£ç»†èƒå¼‚è´¨æ€§çš„ç”Ÿç‰©å­¦åŸºç¡€")
    print(f"â€¢ ä¸ºè¿›ä¸€æ­¥çš„åŠŸèƒ½å®éªŒæä¾›å€™é€‰ç›®æ ‡")
    
    print("âœ… ç»ƒä¹ 3å®Œæˆï¼æ— ç›‘ç£å­¦ä¹ æ˜¯æ¢ç´¢ç”Ÿç‰©æ•°æ®ç»“æ„çš„å¼ºå¤§å·¥å…·ã€‚")
    return clustering_results, df, df_scaled


def solution_4_model_optimization():
    """
    ç­”æ¡ˆ4: æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
    
    è¿™ä¸ªç»ƒä¹ å±•ç¤ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„å®Œæ•´æµç¨‹ï¼Œ
    åŒ…æ‹¬äº¤å‰éªŒè¯ã€è¶…å‚æ•°è°ƒä¼˜å’Œè¿‡æ‹Ÿåˆåˆ†æã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç­”æ¡ˆ4: æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„ç”Ÿç‰©åˆ†ç±»æ•°æ®
    np.random.seed(42)
    
    X, y = make_classification(
        n_samples=500, 
        n_features=20, 
        n_informative=12, 
        n_redundant=4,
        n_classes=2, 
        flip_y=0.05,  # 5%æ ‡ç­¾å™ªå£°
        class_sep=0.8,  # é€‚ä¸­çš„ç±»åˆ«åˆ†ç¦»åº¦
        random_state=42
    )
    
    print("ğŸ“Š æ•°æ®ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {np.bincount(y)} ({np.bincount(y)[1]/(np.bincount(y)[0]+np.bincount(y)[1])*100:.1f}% é˜³æ€§)")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"\nåˆ†å‰²å:")
    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"æ•°æ®å·²æ ‡å‡†åŒ– (è®­ç»ƒé›†å‡å€¼: {X_train_scaled.mean():.3f}, æ ‡å‡†å·®: {X_train_scaled.std():.3f})")
    
    # ä»»åŠ¡4.1: äº¤å‰éªŒè¯
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.1: äº¤å‰éªŒè¯è¯„ä¼°")
    print("-" * 40)
    
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # ä½¿ç”¨åˆ†å±‚kæŠ˜äº¤å‰éªŒè¯
    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # å¤šç§è¯„ä¼°æŒ‡æ ‡çš„äº¤å‰éªŒè¯
    scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
    cv_results = {}
    
    for metric in scoring_metrics:
        scores = cross_val_score(rf, X_train_scaled, y_train, 
                                cv=cv_strategy, scoring=metric)
        cv_results[metric] = scores
        print(f"{metric:>10}: {scores.mean():.3f} Â± {scores.std():.3f} {scores}")
    
    # å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    metric_names = list(cv_results.keys())
    means = [cv_results[m].mean() for m in metric_names]
    stds = [cv_results[m].std() for m in metric_names]
    
    plt.bar(metric_names, means, yerr=stds, capsize=5, alpha=0.7)
    plt.title('5æŠ˜äº¤å‰éªŒè¯ç»“æœ')
    plt.ylabel('åˆ†æ•°')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.boxplot([cv_results[m] for m in metric_names], labels=metric_names)
    plt.title('äº¤å‰éªŒè¯åˆ†æ•°åˆ†å¸ƒ')
    plt.ylabel('åˆ†æ•°')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('cross_validation_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ä»»åŠ¡4.2: è¶…å‚æ•°è°ƒä¼˜
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.2: è¶…å‚æ•°è°ƒä¼˜")
    print("-" * 40)
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2', None]
    }
    
    total_combinations = np.prod([len(v) for v in param_grid.values()])
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {total_combinations}")
    print(f"ä½¿ç”¨3æŠ˜äº¤å‰éªŒè¯è¿›è¡Œç½‘æ ¼æœç´¢...")
    
    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42), 
        param_grid, 
        cv=3,  # ä½¿ç”¨3æŠ˜ä»¥èŠ‚çœæ—¶é—´ 
        scoring='f1', 
        n_jobs=-1,
        verbose=0
    )
    
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³CVåˆ†æ•°: {grid_search.best_score_:.3f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
    best_model = grid_search.best_estimator_
    y_pred_best = best_model.predict(X_test_scaled)
    test_f1 = f1_score(y_test, y_pred_best)
    test_accuracy = accuracy_score(y_test, y_pred_best)
    
    print(f"æµ‹è¯•é›†æ€§èƒ½:")
    print(f"  F1åˆ†æ•°: {test_f1:.3f}")
    print(f"  å‡†ç¡®ç‡: {test_accuracy:.3f}")
    
    # åˆ†æå‚æ•°å½±å“
    print(f"\nå‚æ•°å½±å“åˆ†æ...")
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # åˆ†æn_estimatorsçš„å½±å“
    ax = axes[0]
    n_est_values = param_grid['n_estimators']
    n_est_scores = []
    
    for n_est in n_est_values:
        mask = results_df['param_n_estimators'] == n_est
        mean_score = results_df[mask]['mean_test_score'].mean()
        n_est_scores.append(mean_score)
    
    ax.plot(n_est_values, n_est_scores, 'o-')
    ax.set_xlabel('n_estimators')
    ax.set_ylabel('å¹³å‡F1åˆ†æ•°')
    ax.set_title('æ ‘çš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“')
    ax.grid(True, alpha=0.3)
    
    # åˆ†æmax_depthçš„å½±å“
    ax = axes[1]
    depth_values = [d for d in param_grid['max_depth'] if d is not None]
    depth_scores = []
    
    for depth in depth_values:
        mask = results_df['param_max_depth'] == depth
        mean_score = results_df[mask]['mean_test_score'].mean()
        depth_scores.append(mean_score)
    
    ax.plot(depth_values, depth_scores, 'o-')
    ax.set_xlabel('max_depth')
    ax.set_ylabel('å¹³å‡F1åˆ†æ•°')
    ax.set_title('æ ‘çš„æ·±åº¦å¯¹æ€§èƒ½çš„å½±å“')
    ax.grid(True, alpha=0.3)
    
    # åˆ†æmin_samples_splitçš„å½±å“
    ax = axes[2]
    split_values = param_grid['min_samples_split']
    split_scores = []
    
    for split in split_values:
        mask = results_df['param_min_samples_split'] == split
        mean_score = results_df[mask]['mean_test_score'].mean()
        split_scores.append(mean_score)
    
    ax.plot(split_values, split_scores, 'o-')
    ax.set_xlabel('min_samples_split')
    ax.set_ylabel('å¹³å‡F1åˆ†æ•°')
    ax.set_title('æœ€å°åˆ†å‰²æ ·æœ¬æ•°å¯¹æ€§èƒ½çš„å½±å“')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('hyperparameter_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ä»»åŠ¡4.3: è¿‡æ‹Ÿåˆåˆ†æ
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.3: è¿‡æ‹Ÿåˆåˆ†æ")
    print("-" * 40)
    
    # ä½¿ç”¨éªŒè¯æ›²çº¿åˆ†æè¿‡æ‹Ÿåˆ
    depths = [1, 3, 5, 10, 15, 20, 30, None]
    
    print("è®¡ç®—éªŒè¯æ›²çº¿...")
    train_scores, val_scores = validation_curve(
        DecisionTreeClassifier(random_state=42), 
        X_train_scaled, y_train,
        param_name='max_depth', 
        param_range=[d for d in depths if d is not None] + [100],  # Noneç”¨100ä»£æ›¿
        cv=5, 
        scoring='accuracy',
        n_jobs=-1
    )
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    # ç»˜åˆ¶éªŒè¯æ›²çº¿
    plt.figure(figsize=(10, 6))
    
    x_values = [d if d is not None else 100 for d in depths[:-1]] + [100]
    
    plt.plot(x_values, train_mean, 'o-', label='è®­ç»ƒåˆ†æ•°', color='blue', alpha=0.8)
    plt.fill_between(x_values, train_mean-train_std, train_mean+train_std, 
                     alpha=0.2, color='blue')
    
    plt.plot(x_values, val_mean, 'o-', label='éªŒè¯åˆ†æ•°', color='red', alpha=0.8)
    plt.fill_between(x_values, val_mean-val_std, val_mean+val_std, 
                     alpha=0.2, color='red')
    
    plt.xlabel('æœ€å¤§æ ‘æ·±åº¦')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('éªŒè¯æ›²çº¿ - è¿‡æ‹Ÿåˆåˆ†æ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ ‡æ³¨è¿‡æ‹ŸåˆåŒºåŸŸ
    overfit_threshold = 0.05
    for i, (train_acc, val_acc) in enumerate(zip(train_mean, val_mean)):
        gap = train_acc - val_acc
        if gap > overfit_threshold:
            plt.axvline(x_values[i], color='orange', alpha=0.5, linestyle='--')
            plt.text(x_values[i], val_acc-0.02, f'è¿‡æ‹Ÿåˆ\nå·®è·:{gap:.3f}', 
                    ha='center', fontsize=8, color='orange')
    
    plt.tight_layout()
    plt.savefig('overfitting_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # åˆ†æç»“æœ
    print(f"\nè¿‡æ‹Ÿåˆåˆ†æç»“æœ:")
    for i, depth in enumerate(x_values):
        gap = train_mean[i] - val_mean[i]
        if gap > overfit_threshold:
            print(f"æ·±åº¦ {depth}: è®­ç»ƒåˆ†æ•°={train_mean[i]:.3f}, éªŒè¯åˆ†æ•°={val_mean[i]:.3f}, å·®è·={gap:.3f} (è¿‡æ‹Ÿåˆ)")
        else:
            print(f"æ·±åº¦ {depth}: è®­ç»ƒåˆ†æ•°={train_mean[i]:.3f}, éªŒè¯åˆ†æ•°={val_mean[i]:.3f}, å·®è·={gap:.3f}")
    
    optimal_depth_idx = np.argmax(val_mean)
    optimal_depth = x_values[optimal_depth_idx]
    print(f"\næ¨èçš„æœ€ä¼˜æ·±åº¦: {optimal_depth} (éªŒè¯åˆ†æ•°æœ€é«˜: {val_mean[optimal_depth_idx]:.3f})")
    
    # ä»»åŠ¡4.4: ç‰¹å¾é€‰æ‹©
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.4: ç‰¹å¾é€‰æ‹©")
    print("-" * 40)
    
    print("åŸå§‹ç‰¹å¾æ•°:", X_train_scaled.shape[1])
    
    # æ–¹æ³•1: åŸºäºç»Ÿè®¡çš„ç‰¹å¾é€‰æ‹©
    print("\næ–¹æ³•1: åŸºäºFç»Ÿè®¡é‡çš„ç‰¹å¾é€‰æ‹©")
    selector_stats = SelectKBest(score_func=f_classif, k=10)
    X_train_stats = selector_stats.fit_transform(X_train_scaled, y_train)
    X_test_stats = selector_stats.transform(X_test_scaled)
    
    # è·å–é€‰æ‹©çš„ç‰¹å¾
    selected_features_stats = selector_stats.get_support()
    feature_scores = selector_stats.scores_
    
    print(f"é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {np.where(selected_features_stats)[0]}")
    print(f"ç‰¹å¾åˆ†æ•°å‰5å:")
    top_features = np.argsort(feature_scores)[::-1][:5]
    for i, feat_idx in enumerate(top_features):
        print(f"  ç‰¹å¾{feat_idx}: Fåˆ†æ•°={feature_scores[feat_idx]:.2f}")
    
    # æ–¹æ³•2: é€’å½’ç‰¹å¾æ¶ˆé™¤
    print("\næ–¹æ³•2: é€’å½’ç‰¹å¾æ¶ˆé™¤(RFE)")
    rfe = RFE(LogisticRegression(random_state=42, max_iter=1000), n_features_to_select=10)
    X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)
    X_test_rfe = rfe.transform(X_test_scaled)
    
    selected_features_rfe = rfe.support_
    feature_ranking = rfe.ranking_
    
    print(f"RFEé€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {np.where(selected_features_rfe)[0]}")
    print(f"ç‰¹å¾é‡è¦æ€§æ’å:")
    for i in range(len(feature_ranking)):
        if feature_ranking[i] == 1:  # é€‰ä¸­çš„ç‰¹å¾æ’åä¸º1
            print(f"  ç‰¹å¾{i}: æ’å{feature_ranking[i]} (é€‰ä¸­)")
    
    # æ–¹æ³•3: åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    print("\næ–¹æ³•3: åŸºäºéšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§")
    rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_selector.fit(X_train_scaled, y_train)
    
    feature_importances = rf_selector.feature_importances_
    important_features = np.argsort(feature_importances)[::-1][:10]
    
    print(f"é‡è¦æ€§æœ€é«˜çš„10ä¸ªç‰¹å¾:")
    for i, feat_idx in enumerate(important_features):
        print(f"  ç‰¹å¾{feat_idx}: é‡è¦æ€§={feature_importances[feat_idx]:.3f}")
    
    # æ¯”è¾ƒç‰¹å¾é€‰æ‹©å‰åçš„æ€§èƒ½
    print("\nç‰¹å¾é€‰æ‹©æ•ˆæœæ¯”è¾ƒ:")
    
    models_to_test = {
        'åŸå§‹ç‰¹å¾': (X_train_scaled, X_test_scaled),
        'Fç»Ÿè®¡é‡é€‰æ‹©': (X_train_stats, X_test_stats),
        'RFEé€‰æ‹©': (X_train_rfe, X_test_rfe)
    }
    
    feature_selection_results = {}
    
    for method_name, (X_tr, X_te) in models_to_test.items():
        # ä½¿ç”¨é€»è¾‘å›å½’æµ‹è¯•
        lr = LogisticRegression(random_state=42, max_iter=1000)
        lr.fit(X_tr, y_train)
        y_pred = lr.predict(X_te)
        
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        feature_selection_results[method_name] = {
            'accuracy': accuracy,
            'f1': f1,
            'n_features': X_tr.shape[1]
        }
        
        print(f"{method_name:>15}: å‡†ç¡®ç‡={accuracy:.3f}, F1={f1:.3f}, ç‰¹å¾æ•°={X_tr.shape[1]}")
    
    # å¯è§†åŒ–ç‰¹å¾é€‰æ‹©æ•ˆæœ
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    methods = list(feature_selection_results.keys())
    accuracies = [feature_selection_results[m]['accuracy'] for m in methods]
    f1s = [feature_selection_results[m]['f1'] for m in methods]
    
    x = np.arange(len(methods))
    width = 0.35
    
    plt.bar(x - width/2, accuracies, width, label='å‡†ç¡®ç‡', alpha=0.8)
    plt.bar(x + width/2, f1s, width, label='F1åˆ†æ•°', alpha=0.8)
    
    plt.xlabel('ç‰¹å¾é€‰æ‹©æ–¹æ³•')
    plt.ylabel('æ€§èƒ½åˆ†æ•°')
    plt.title('ç‰¹å¾é€‰æ‹©æ•ˆæœæ¯”è¾ƒ')
    plt.xticks(x, methods, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    n_features = [feature_selection_results[m]['n_features'] for m in methods]
    colors = ['blue', 'orange', 'green']
    
    plt.bar(methods, n_features, color=colors, alpha=0.7)
    plt.xlabel('ç‰¹å¾é€‰æ‹©æ–¹æ³•')
    plt.ylabel('ç‰¹å¾æ•°é‡')
    plt.title('é€‰æ‹©çš„ç‰¹å¾æ•°é‡')
    plt.xticks(rotation=45)
    
    for i, (method, n_feat) in enumerate(zip(methods, n_features)):
        plt.text(i, n_feat + 0.5, str(n_feat), ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('feature_selection_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ç”Ÿç‰©å­¦è§£é‡Š
    print(f"\nğŸ§¬ æ¨¡å‹ä¼˜åŒ–çš„ç”Ÿç‰©å­¦æ„ä¹‰:")
    print(f"â€¢ äº¤å‰éªŒè¯ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šçš„ç¨³å®šæ€§")
    print(f"â€¢ è¶…å‚æ•°è°ƒä¼˜å¸®åŠ©æ‰¾åˆ°æœ€é€‚åˆç”Ÿç‰©æ•°æ®ç‰¹å¾çš„æ¨¡å‹é…ç½®")
    print(f"â€¢ è¿‡æ‹Ÿåˆæ£€æµ‹é˜²æ­¢æ¨¡å‹è¿‡åº¦è®°å¿†è®­ç»ƒæ•°æ®çš„å™ªå£°")
    print(f"â€¢ ç‰¹å¾é€‰æ‹©è¯†åˆ«çœŸæ­£æœ‰é¢„æµ‹ä»·å€¼çš„ç”Ÿç‰©æ ‡è®°ç‰©")
    print(f"â€¢ è¿™äº›æŠ€æœ¯ç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œä¸´åºŠåº”ç”¨ä»·å€¼")
    
    print("âœ… ç»ƒä¹ 4å®Œæˆï¼æ¨¡å‹ä¼˜åŒ–æ˜¯æ„å»ºå¯é AIç³»ç»Ÿçš„å…³é”®ã€‚")
    return grid_search, feature_selection_results


def solution_5_cancer_subtype_project():
    """
    ç­”æ¡ˆ5: ç»¼åˆé¡¹ç›® - ç™Œç—‡äºšå‹åˆ†ç±»
    
    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œæ•´åˆäº†å‰é¢å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯ï¼Œ
    æ¨¡æ‹ŸçœŸå®çš„ç™Œç—‡äºšå‹åˆ†ç±»ä»»åŠ¡ã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç»¼åˆé¡¹ç›®: ç™Œç—‡äºšå‹åˆ†ç±»")
    print("=" * 60)
    
    print("""
ğŸ”¬ é¡¹ç›®èƒŒæ™¯:
åŸºäºåŸºå› è¡¨è¾¾æ•°æ®çš„ä¹³è…ºç™Œåˆ†å­äºšå‹åˆ†ç±»æ˜¯ç²¾å‡†åŒ»ç–—çš„é‡è¦åº”ç”¨ã€‚
ä¸åŒäºšå‹æœ‰ä¸åŒçš„æ²»ç–—ç­–ç•¥å’Œé¢„åï¼š

â€¢ Luminal A: æ¿€ç´ å—ä½“é˜³æ€§ï¼Œé¢„åæœ€å¥½ï¼Œå†…åˆ†æ³Œæ²»ç–—æ•æ„Ÿ
â€¢ Luminal B: æ¿€ç´ å—ä½“é˜³æ€§ï¼ŒKi-67é«˜ï¼Œå¯èƒ½éœ€è¦åŒ–ç–—
â€¢ HER2+: HER2æ‰©å¢ï¼Œé¶å‘æ²»ç–—(å¦‚æ›²å¦¥ç å•æŠ—)æœ‰æ•ˆ
â€¢ Basal-like: ä¸‰é˜´æ€§ï¼Œé¢„åæœ€å·®ï¼Œä¸»è¦ä¾é åŒ–ç–—

æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ªè‡ªåŠ¨åˆ†ç±»å™¨æ¥è¾…åŠ©ä¸´åºŠè¯Šæ–­ã€‚
    """)
    
    # æ­¥éª¤1: æ•°æ®ç”Ÿæˆå’Œæ¢ç´¢
    print("\n" + "-" * 50)
    print("æ­¥éª¤1: æ•°æ®ç”Ÿæˆå’Œæ¢ç´¢")
    print("-" * 50)
    
    np.random.seed(42)
    
    # æ¨¡æ‹Ÿ4ç§äºšå‹çš„åŸºå› è¡¨è¾¾æ¨¡å¼
    n_samples_per_class = [120, 100, 80, 100]  # ä¸å¹³è¡¡æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®æƒ…å†µ
    n_genes = 100
    subtype_names = ['Luminal A', 'Luminal B', 'HER2+', 'Basal-like']
    
    all_data = []
    all_labels = []
    
    print("ç”Ÿæˆå„äºšå‹çš„åŸºå› è¡¨è¾¾æ•°æ®...")
    
    for subtype_id, (n_samples, subtype_name) in enumerate(zip(n_samples_per_class, subtype_names)):
        print(f"  ç”Ÿæˆ {subtype_name}: {n_samples} ä¸ªæ ·æœ¬")
        
        # åŸºç¡€è¡¨è¾¾æ°´å¹³ (å¯¹æ•°æ­£æ€åˆ†å¸ƒ)
        base_expression = np.random.lognormal(2, 0.8, (n_samples, n_genes))
        
        # æ·»åŠ äºšå‹ç‰¹å¼‚æ€§è¡¨è¾¾æ¨¡å¼
        if subtype_id == 0:  # Luminal A
            # ESR1, PGRç›¸å…³åŸºå› é«˜è¡¨è¾¾ (å‰25ä¸ªåŸºå› )
            base_expression[:, :25] *= np.random.uniform(2, 3, (n_samples, 25))
            # å¢æ®–ç›¸å…³åŸºå› ä½è¡¨è¾¾
            base_expression[:, 75:] *= np.random.uniform(0.3, 0.7, (n_samples, 25))
            
        elif subtype_id == 1:  # Luminal B  
            # ESR1, PGRç›¸å…³åŸºå› ä¸­ç­‰è¡¨è¾¾
            base_expression[:, :25] *= np.random.uniform(1.5, 2.5, (n_samples, 25))
            # å¢æ®–ç›¸å…³åŸºå› é«˜è¡¨è¾¾ (25-50ä¸ªåŸºå› )
            base_expression[:, 25:50] *= np.random.uniform(2.5, 4, (n_samples, 25))
            
        elif subtype_id == 2:  # HER2+
            # HER2åŠç›¸å…³åŸºå› æé«˜è¡¨è¾¾ (50-75ä¸ªåŸºå› )
            base_expression[:, 50:75] *= np.random.uniform(3, 5, (n_samples, 25))
            # å¢æ®–åŸºå› é«˜è¡¨è¾¾
            base_expression[:, 25:50] *= np.random.uniform(2, 3, (n_samples, 25))
            
        elif subtype_id == 3:  # Basal-like
            # åŸºåº•æ ·ç‰¹å¼‚åŸºå› é«˜è¡¨è¾¾ (75-100ä¸ªåŸºå› )
            base_expression[:, 75:] *= np.random.uniform(2.5, 4, (n_samples, 25))
            # æ¿€ç´ å—ä½“åŸºå› ä½è¡¨è¾¾
            base_expression[:, :25] *= np.random.uniform(0.2, 0.5, (n_samples, 25))
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, base_expression * 0.1)  # ç›¸å¯¹å™ªå£°
        base_expression += noise
        base_expression = np.maximum(base_expression, 0.1)  # ç¡®ä¿è¡¨è¾¾å€¼ä¸ºæ­£
        
        all_data.append(base_expression)
        all_labels.extend([subtype_id] * n_samples)
    
    X = np.vstack(all_data)
    y = np.array(all_labels)
    
    # åˆ›å»ºç‰¹å¾åç§°ï¼ˆæ¨¡æ‹ŸçœŸå®åŸºå› åï¼‰
    gene_categories = {
        'ESR_related': list(range(0, 25)),
        'Proliferation': list(range(25, 50)), 
        'HER2_pathway': list(range(50, 75)),
        'Basal_markers': list(range(75, 100))
    }
    
    gene_names = []
    for category, indices in gene_categories.items():
        for i, idx in enumerate(indices):
            gene_names.append(f'{category}_{i+1:02d}')
    
    print(f"\næ•°æ®é›†æ¦‚å†µ:")
    print(f"æ€»æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"åŸºå› æ•°é‡: {X.shape[1]}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:")
    for i, (name, count) in enumerate(zip(subtype_names, n_samples_per_class)):
        print(f"  {name}: {count} ({count/sum(n_samples_per_class)*100:.1f}%)")
    
    # æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
    print("\n" + "-" * 50)
    print("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†")
    print("-" * 50)
    
    # åˆ›å»ºDataFrameæ–¹ä¾¿å¤„ç†
    df = pd.DataFrame(X, columns=gene_names)
    df['subtype'] = [subtype_names[i] for i in y]
    df['subtype_id'] = y
    
    # 2.1 å¤„ç†ç¼ºå¤±å€¼(æ¨¡æ‹Ÿä¸€äº›ç¼ºå¤±)
    print("æ¨¡æ‹Ÿå¹¶å¤„ç†ç¼ºå¤±å€¼...")
    missing_indices = np.random.choice(len(df), 20, replace=False)
    missing_genes = np.random.choice(gene_names, 20, replace=True)
    
    for idx, gene in zip(missing_indices, missing_genes):
        df.loc[idx, gene] = np.nan
    
    print(f"  æ·»åŠ äº† {df.isnull().sum().sum()} ä¸ªç¼ºå¤±å€¼")
    
    # ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼ï¼ˆæ¯”å‡å€¼æ›´ç¨³å¥ï¼‰
    for gene in gene_names:
        if df[gene].isnull().any():
            median_value = df[gene].median()
            df[gene].fillna(median_value, inplace=True)
    
    print(f"  ç”¨ä¸­ä½æ•°å¡«å……åï¼Œç¼ºå¤±å€¼: {df[gene_names].isnull().sum().sum()}")
    
    # 2.2 å¯¹æ•°è½¬æ¢å¤„ç†åæ€åˆ†å¸ƒ
    print("å¯¹æ•°è½¬æ¢...")
    df_log = df.copy()
    df_log[gene_names] = np.log1p(df[gene_names])  # log(1+x)
    
    # 2.3 æ ‡å‡†åŒ–
    print("æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_log[gene_names])
    
    # 2.4 å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    print(f"ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹: {dict(zip(*np.unique(y, return_counts=True)))}")
    print("å°†åœ¨æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨class_weight='balanced'æ¥å¤„ç†ä¸å¹³è¡¡")
    
    # æ­¥éª¤3: æ¢ç´¢æ€§æ•°æ®åˆ†æ
    print("\n" + "-" * 50)
    print("æ­¥éª¤3: æ¢ç´¢æ€§æ•°æ®åˆ†æ") 
    print("-" * 50)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # 3.1 ç±»åˆ«åˆ†å¸ƒ
    ax = axes[0]
    subtype_counts = df['subtype'].value_counts()
    bars = ax.bar(subtype_counts.index, subtype_counts.values, 
                  color=['lightblue', 'lightcoral', 'lightgreen', 'gold'])
    ax.set_title('ç™Œç—‡äºšå‹åˆ†å¸ƒ', fontsize=14)
    ax.set_ylabel('æ ·æœ¬æ•°é‡')
    plt.setp(ax.get_xticklabels(), rotation=45)
    
    for bar, count in zip(bars, subtype_counts.values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
                str(count), ha='center', va='bottom')
    
    # 3.2 ä¸»æˆåˆ†åˆ†æå¯è§†åŒ–
    ax = axes[1]
    pca_vis = PCA(n_components=2)
    X_pca = pca_vis.fit_transform(X_scaled)
    
    colors = ['blue', 'red', 'green', 'orange']
    for i, subtype in enumerate(subtype_names):
        mask = df['subtype'] == subtype
        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                  c=colors[i], label=subtype, alpha=0.6, s=30)
    
    ax.set_xlabel(f'PC1 ({pca_vis.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca_vis.explained_variance_ratio_[1]:.1%})')
    ax.set_title('PCAå¯è§†åŒ–')
    ax.legend(fontsize=8)
    
    # 3.3 å·®å¼‚è¡¨è¾¾åŸºå› çƒ­å›¾
    ax = axes[2]
    # è®¡ç®—æ¯ä¸ªäºšå‹çš„å¹³å‡è¡¨è¾¾
    subtype_means = df_log.groupby('subtype')[gene_names].mean()
    
    # é€‰æ‹©å·®å¼‚æœ€å¤§çš„20ä¸ªåŸºå› 
    gene_vars = df_log[gene_names].var()
    top_var_genes = gene_vars.nlargest(20).index
    
    im = ax.imshow(subtype_means[top_var_genes].T, aspect='auto', cmap='RdBu_r')
    ax.set_xticks(range(len(subtype_names)))
    ax.set_xticklabels(subtype_names, rotation=45)
    ax.set_ylabel('åŸºå› ')
    ax.set_title('å·®å¼‚è¡¨è¾¾çƒ­å›¾(Top 20)')
    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    # 3.4 åŸºå› è¡¨è¾¾åˆ†å¸ƒ
    ax = axes[3]
    sample_genes = ['ESR_related_01', 'Proliferation_01', 'HER2_pathway_01', 'Basal_markers_01']
    for gene in sample_genes:
        if gene in df_log.columns:
            ax.hist(df_log[gene], alpha=0.5, bins=30, label=gene.replace('_', ' '))
    ax.set_xlabel('log(è¡¨è¾¾æ°´å¹³)')
    ax.set_ylabel('é¢‘ç‡')
    ax.set_title('å…³é”®åŸºå› è¡¨è¾¾åˆ†å¸ƒ')
    ax.legend(fontsize=8)
    
    # 3.5 t-SNEå¯è§†åŒ–
    ax = axes[4]
    print("è®¡ç®—t-SNE...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    X_tsne = tsne.fit_transform(X_scaled[:300])  # ä½¿ç”¨éƒ¨åˆ†æ•°æ®åŠ é€Ÿ
    
    for i, subtype in enumerate(subtype_names):
        mask = df['subtype'].iloc[:300] == subtype
        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], 
                  c=colors[i], label=subtype, alpha=0.6, s=30)
    
    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2') 
    ax.set_title('t-SNEå¯è§†åŒ–')
    ax.legend(fontsize=8)
    
    # 3.6 æ–¹å·®åˆ†æ
    ax = axes[5]
    pca_full = PCA()
    pca_full.fit(X_scaled)
    explained_var = pca_full.explained_variance_ratio_[:20]
    
    ax.bar(range(1, 21), explained_var)
    ax.set_xlabel('ä¸»æˆåˆ†')
    ax.set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    ax.set_title('ä¸»æˆåˆ†è§£é‡Šæ–¹å·®')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('cancer_eda_comprehensive.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # æ­¥éª¤4: ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©
    print("\n" + "-" * 50)
    print("æ­¥éª¤4: ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©")
    print("-" * 50)
    
    X_processed = X_scaled.copy()
    
    # 4.1 å•å˜é‡ç‰¹å¾é€‰æ‹©
    print("å•å˜é‡ç‰¹å¾é€‰æ‹©...")
    selector_univariate = SelectKBest(score_func=f_classif, k=50)
    X_univariate = selector_univariate.fit_transform(X_processed, y)
    selected_genes_univariate = np.array(gene_names)[selector_univariate.get_support()]
    
    print(f"  é€‰æ‹©äº† {len(selected_genes_univariate)} ä¸ªåŸºå› ")
    print(f"  å‰5ä¸ªæœ€é‡è¦çš„åŸºå› :")
    scores = selector_univariate.scores_
    top_indices = np.argsort(scores)[::-1][:5]
    for i, idx in enumerate(top_indices):
        print(f"    {gene_names[idx]}: Fåˆ†æ•° {scores[idx]:.2f}")
    
    # 4.2 é€’å½’ç‰¹å¾æ¶ˆé™¤
    print("\né€’å½’ç‰¹å¾æ¶ˆé™¤...")
    rf_for_rfe = RandomForestClassifier(n_estimators=50, random_state=42)
    rfe = RFE(rf_for_rfe, n_features_to_select=40)
    X_rfe = rfe.fit_transform(X_processed, y)
    selected_genes_rfe = np.array(gene_names)[rfe.support_]
    
    print(f"  RFEé€‰æ‹©äº† {len(selected_genes_rfe)} ä¸ªåŸºå› ")
    
    # 4.3 åŸºäºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
    print("\nåŸºäºéšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§...")
    rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_importance.fit(X_processed, y)
    
    importances = rf_importance.feature_importances_
    important_indices = np.argsort(importances)[::-1][:30]
    selected_genes_importance = [gene_names[i] for i in important_indices]
    
    print(f"  é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„ {len(selected_genes_importance)} ä¸ªåŸºå› ")
    print(f"  å‰5ä¸ªæœ€é‡è¦çš„åŸºå› :")
    for i, idx in enumerate(important_indices[:5]):
        print(f"    {gene_names[idx]}: é‡è¦æ€§ {importances[idx]:.3f}")
    
    # 4.4 åˆ›å»ºç»„åˆç‰¹å¾ï¼ˆåŸºå› æ¯”å€¼ï¼‰
    print("\nåˆ›å»ºç»„åˆç‰¹å¾...")
    # ESRç›¸å…³åŸºå› ä¸HER2é€šè·¯åŸºå› çš„æ¯”å€¼
    esr_genes = [g for g in gene_names if g.startswith('ESR')]
    her2_genes = [g for g in gene_names if g.startswith('HER2')]
    
    if esr_genes and her2_genes:
        esr_mean = df_log[esr_genes].mean(axis=1)
        her2_mean = df_log[her2_genes].mean(axis=1)
        esr_her2_ratio = esr_mean / (her2_mean + 1e-6)  # é¿å…é™¤é›¶
        
        print(f"  åˆ›å»ºESR/HER2æ¯”å€¼ç‰¹å¾")
        print(f"  ä¸åŒäºšå‹çš„ESR/HER2æ¯”å€¼:")
        for subtype in subtype_names:
            mask = df['subtype'] == subtype
            ratio_mean = esr_her2_ratio[mask].mean()
            print(f"    {subtype}: {ratio_mean:.2f}")
    
    # ä½¿ç”¨æœ€ä½³ç‰¹å¾é€‰æ‹©ç»“æœ
    X_selected = X_univariate  # ä½¿ç”¨å•å˜é‡é€‰æ‹©çš„ç»“æœ
    
    # æ­¥éª¤5: æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
    print("\n" + "-" * 50)
    print("æ­¥éª¤5: æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–")
    print("-" * 50)
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    # 5.1 è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹
    models_to_try = {
        'LogisticRegression': LogisticRegression(
            random_state=42, max_iter=1000, class_weight='balanced'
        ),
        'RandomForest': RandomForestClassifier(
            n_estimators=100, random_state=42, class_weight='balanced'
        ),
        'SVM': SVC(
            random_state=42, probability=True, class_weight='balanced'
        ),
        'XGBoost': RandomForestClassifier(  # ç”¨éšæœºæ£®æ—ä»£æ›¿XGBoost
            n_estimators=200, max_depth=6, random_state=42, class_weight='balanced'
        ),
        'NeuralNetwork': MLPClassifier(
            hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42
        )
    }
    
    model_results = {}
    
    print("è®­ç»ƒåŸºç¡€æ¨¡å‹...")
    for model_name, model in models_to_try.items():
        print(f"\nè®­ç»ƒ {model_name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')
        
        model_results[model_name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯F1: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    # 5.2 è¶…å‚æ•°è°ƒä¼˜ï¼ˆé’ˆå¯¹æœ€ä½³æ¨¡å‹ï¼‰
    best_basic_model = max(model_results.keys(), key=lambda k: model_results[k]['f1'])
    print(f"\næœ€ä½³åŸºç¡€æ¨¡å‹: {best_basic_model}")
    
    if best_basic_model == 'RandomForest':
        print("å¯¹éšæœºæ£®æ—è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        param_grid_rf = {
            'n_estimators': [100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [5, 10],
            'min_samples_leaf': [2, 4]
        }
        
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=42, class_weight='balanced'),
            param_grid_rf, cv=3, scoring='f1_weighted', n_jobs=-1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³CVåˆ†æ•°: {grid_search.best_score_:.3f}")
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        best_model = grid_search.best_estimator_
        y_pred_final = best_model.predict(X_test)
        y_proba_final = best_model.predict_proba(X_test)
        
        final_accuracy = accuracy_score(y_test, y_pred_final)
        final_f1 = f1_score(y_test, y_pred_final, average='weighted')
        
        print(f"ä¼˜åŒ–åæµ‹è¯•é›†æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {final_accuracy:.3f}")
        print(f"  F1åˆ†æ•°: {final_f1:.3f}")
    else:
        best_model = model_results[best_basic_model]['model']
        y_pred_final = model_results[best_basic_model]['y_pred']
        y_proba_final = model_results[best_basic_model]['y_proba']
        final_accuracy = model_results[best_basic_model]['accuracy']
        final_f1 = model_results[best_basic_model]['f1']
    
    # æ­¥éª¤6: æ¨¡å‹è¯„ä¼°
    print("\n" + "-" * 50)
    print("æ­¥éª¤6: æ¨¡å‹è¯„ä¼°")
    print("-" * 50)
    
    # 6.1 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred_final, target_names=subtype_names))
    
    # 6.2 æ··æ·†çŸ©é˜µ
    print("\næ··æ·†çŸ©é˜µåˆ†æ:")
    cm = confusion_matrix(y_test, y_pred_final)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=subtype_names, yticklabels=subtype_names)
    plt.title('ç™Œç—‡äºšå‹åˆ†ç±»æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®äºšå‹')
    plt.xlabel('é¢„æµ‹äºšå‹')
    
    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯
    for i in range(len(subtype_names)):
        for j in range(len(subtype_names)):
            if i == j:  # å¯¹è§’çº¿å…ƒç´ 
                accuracy_per_class = cm[i, j] / cm[i, :].sum()
                plt.text(j+0.5, i+0.7, f'({accuracy_per_class:.1%})', 
                        ha='center', va='center', color='red', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('cancer_confusion_matrix.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # åˆ†ææ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    for i, subtype in enumerate(subtype_names):
        class_mask = y_test == i
        class_predictions = y_pred_final[class_mask]
        class_accuracy = accuracy_score([i] * sum(class_mask), class_predictions)
        print(f"{subtype}: å‡†ç¡®ç‡ {class_accuracy:.1%}, æ ·æœ¬æ•° {sum(class_mask)}")
    
    # 6.3 ROCæ›²çº¿ (å¤šåˆ†ç±»)
    if y_proba_final is not None:
        print("\nç»˜åˆ¶ROCæ›²çº¿...")
        plt.figure(figsize=(10, 8))
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶ROCæ›²çº¿
        for i, subtype in enumerate(subtype_names):
            # å°†å¤šåˆ†ç±»é—®é¢˜è½¬æ¢ä¸ºäºŒåˆ†ç±» (one-vs-rest)
            y_binary = (y_test == i).astype(int)
            y_score = y_proba_final[:, i]
            
            fpr, tpr, _ = roc_curve(y_binary, y_score)
            auc_score = roc_auc_score(y_binary, y_score)
            
            plt.plot(fpr, tpr, label=f'{subtype} (AUC = {auc_score:.3f})', 
                    linewidth=2)
        
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)
        plt.xlabel('å‡é˜³æ€§ç‡ (1 - ç‰¹å¼‚åº¦)')
        plt.ylabel('çœŸé˜³æ€§ç‡ (æ•æ„Ÿåº¦)')
        plt.title('å¤šåˆ†ç±»ROCæ›²çº¿ (One-vs-Rest)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('cancer_roc_curves.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    # æ­¥éª¤7: æ¨¡å‹è§£é‡Š
    print("\n" + "-" * 50)
    print("æ­¥éª¤7: æ¨¡å‹è§£é‡Š")
    print("-" * 50)
    
    # 7.1 ç‰¹å¾é‡è¦æ€§åˆ†æ
    if hasattr(best_model, 'feature_importances_'):
        print("ç‰¹å¾é‡è¦æ€§åˆ†æ:")
        feature_importance = best_model.feature_importances_
        important_features = np.argsort(feature_importance)[::-1]
        
        print("æœ€é‡è¦çš„10ä¸ªåŸºå› :")
        for i, feat_idx in enumerate(important_features[:10]):
            gene_name = selected_genes_univariate[feat_idx]
            importance = feature_importance[feat_idx]
            print(f"  {i+1:2d}. {gene_name}: {importance:.3f}")
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 6))
        top_features = important_features[:15]
        top_importance = feature_importance[top_features]
        top_names = [selected_genes_univariate[i] for i in top_features]
        
        plt.barh(range(len(top_names)), top_importance)
        plt.yticks(range(len(top_names)), top_names)
        plt.xlabel('ç‰¹å¾é‡è¦æ€§')
        plt.title('Top 15 ç‰¹å¾é‡è¦æ€§')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig('cancer_feature_importance.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    # 7.2 é”™è¯¯æ¡ˆä¾‹åˆ†æ
    print("\né”™è¯¯æ¡ˆä¾‹åˆ†æ:")
    misclassified = y_test != y_pred_final
    if sum(misclassified) > 0:
        print(f"æ€»å…± {sum(misclassified)} ä¸ªè¯¯åˆ†ç±»æ¡ˆä¾‹:")
        
        error_analysis = {}
        for true_class, pred_class in zip(y_test[misclassified], y_pred_final[misclassified]):
            key = f"{subtype_names[true_class]} â†’ {subtype_names[pred_class]}"
            error_analysis[key] = error_analysis.get(key, 0) + 1
        
        for error_type, count in error_analysis.items():
            print(f"  {error_type}: {count} ä¾‹")
    
    # 7.3 ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š
    print(f"\nğŸ§¬ ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š:")
    print(f"åŸºäºåŸºå› è¡¨è¾¾çš„ç™Œç—‡äºšå‹åˆ†ç±»å…·æœ‰é‡è¦ä¸´åºŠæ„ä¹‰:")
    print(f"")
    print(f"åˆ†ç±»æ€§èƒ½:")
    print(f"â€¢ æ€»ä½“å‡†ç¡®ç‡: {final_accuracy:.1%}")
    print(f"â€¢ åŠ æƒF1åˆ†æ•°: {final_f1:.3f}")
    print(f"")
    print(f"ä¸´åºŠåº”ç”¨ä»·å€¼:")
    print(f"â€¢ Luminal Aäºšå‹é¢„æµ‹å‡†ç¡®ï¼Œå¯æŒ‡å¯¼å†…åˆ†æ³Œæ²»ç–—")
    print(f"â€¢ HER2+äºšå‹è¯†åˆ«å…³é”®ï¼Œå†³å®šé¶å‘æ²»ç–—æ–¹æ¡ˆ")
    print(f"â€¢ Basal-likeäºšå‹é¢„åæœ€å·®ï¼Œéœ€ç§¯æåŒ–ç–—")
    print(f"â€¢ å¯ä½œä¸ºç—…ç†è¯Šæ–­çš„é‡è¦è¡¥å……å·¥å…·")
    print(f"")
    print(f"å…³é”®é¢„æµ‹åŸºå› :")
    if hasattr(best_model, 'feature_importances_'):
        top_3_genes = [selected_genes_univariate[i] for i in important_features[:3]]
        for gene in top_3_genes:
            if gene.startswith('ESR'):
                print(f"â€¢ {gene}: æ¿€ç´ å—ä½“é€šè·¯ï¼ŒåŒºåˆ†Luminaläºšå‹")
            elif gene.startswith('HER2'):
                print(f"â€¢ {gene}: HER2é€šè·¯ï¼Œè¯†åˆ«HER2+äºšå‹")
            elif gene.startswith('Basal'):
                print(f"â€¢ {gene}: åŸºåº•æ ·æ ‡è®°ï¼Œè¯†åˆ«ä¸‰é˜´æ€§ä¹³è…ºç™Œ")
            elif gene.startswith('Proliferation'):
                print(f"â€¢ {gene}: å¢æ®–ç›¸å…³ï¼ŒåŒºåˆ†Luminal A/B")
    
    # æ­¥éª¤8: ç»“æœæ€»ç»“å’ŒæŠ¥å‘Š
    print("\n" + "-" * 50)
    print("æ­¥éª¤8: é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("-" * 50)
    
    # è®¡ç®—ä¸€äº›é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(y)
    n_features_original = len(gene_names)
    n_features_selected = X_selected.shape[1]
    
    # è®¡ç®—æ¯ç±»çš„AUC
    class_aucs = []
    if y_proba_final is not None:
        for i in range(len(subtype_names)):
            y_binary = (y_test == i).astype(int)
            y_score = y_proba_final[:, i]
            auc = roc_auc_score(y_binary, y_score)
            class_aucs.append(auc)
    
    mean_auc = np.mean(class_aucs) if class_aucs else 0
    
    # æ‰¾å‡ºæœ€éš¾åŒºåˆ†çš„äºšå‹ç»„åˆ
    most_confused = ""
    max_confusion = 0
    for i in range(len(subtype_names)):
        for j in range(len(subtype_names)):
            if i != j and cm[i, j] > max_confusion:
                max_confusion = cm[i, j]
                most_confused = f"{subtype_names[i]} ä¸ {subtype_names[j]}"
    
    report = f"""
    
ğŸ¥ ç™Œç—‡äºšå‹åˆ†ç±»é¡¹ç›®æ€»ç»“æŠ¥å‘Š
=====================================

ğŸ“Š æ•°æ®æ¦‚å†µ:
   â€¢ æ€»æ ·æœ¬æ•°: {total_samples}
   â€¢ åŸºå› æ•°é‡: {n_features_original} â†’ {n_features_selected} (ç‰¹å¾é€‰æ‹©å)  
   â€¢ ç™Œç—‡äºšå‹: {len(subtype_names)} ç±»
   â€¢ ç±»åˆ«åˆ†å¸ƒ: ä¸å¹³è¡¡ (æœ€å¤š{max(n_samples_per_class)}, æœ€å°‘{min(n_samples_per_class)})

ğŸ¤– æœ€ä½³æ¨¡å‹:
   â€¢ ç®—æ³•: {best_basic_model}
   â€¢ æ€»ä½“å‡†ç¡®ç‡: {final_accuracy:.1%}
   â€¢ åŠ æƒF1åˆ†æ•°: {final_f1:.3f}
   â€¢ å¹³å‡AUC: {mean_auc:.3f}

ğŸ” å…³é”®å‘ç°:
   â€¢ æœ€é‡è¦çš„é¢„æµ‹åŸºå› : {', '.join([selected_genes_univariate[i] for i in important_features[:3]]) if hasattr(best_model, 'feature_importances_') else 'N/A'}
   â€¢ æœ€éš¾åŒºåˆ†çš„äºšå‹: {most_confused} ({max_confusion}ä¾‹è¯¯åˆ†ç±»)
   â€¢ æ¨¡å‹ä¼˜åŠ¿: ç‰¹å¾é€‰æ‹©æœ‰æ•ˆï¼Œå¤„ç†ä¸å¹³è¡¡æ•°æ®

âš•ï¸ ä¸´åºŠæ„ä¹‰:
   â€¢ è¯¥æ¨¡å‹å¯è¾…åŠ©ä¹³è…ºç™Œåˆ†å­äºšå‹è¯Šæ–­
   â€¢ ç»“åˆå¸¸è§„ç—…ç†æ£€æŸ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§
   â€¢ ä¸ºä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆæä¾›åˆ†å­ä¾æ®
   â€¢ ç‰¹åˆ«é€‚ç”¨äºç–‘éš¾ç—…ä¾‹çš„è¾…åŠ©è¯Šæ–­

ğŸ“‹ æŠ€æœ¯ç»†èŠ‚:
   â€¢ ç‰¹å¾é€‰æ‹©: å•å˜é‡Fç»Ÿè®¡é‡é€‰æ‹© (å‰50ä¸ªåŸºå› )
   â€¢ æ•°æ®é¢„å¤„ç†: å¯¹æ•°è½¬æ¢ + æ ‡å‡†åŒ–
   â€¢ ç±»åˆ«ä¸å¹³è¡¡: ä½¿ç”¨class_weight='balanced'
   â€¢ æ¨¡å‹éªŒè¯: 5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯

âš ï¸ å±€é™æ€§:
   â€¢ æ ·æœ¬é‡ç›¸å¯¹è¾ƒå°ï¼Œéœ€æ›´å¤§è§„æ¨¡éªŒè¯
   â€¢ æ¨¡æ‹Ÿæ•°æ®å¯èƒ½ä¸çœŸå®æ•°æ®å­˜åœ¨å·®å¼‚
   â€¢ ç¼ºä¹å¤–éƒ¨éªŒè¯é›†è¯„ä¼°æ³›åŒ–èƒ½åŠ›
   â€¢ æœªè€ƒè™‘æ‰¹æ¬¡æ•ˆåº”å’ŒæŠ€æœ¯å˜å¼‚

ğŸš€ ä¸‹ä¸€æ­¥å·¥ä½œ:
   â€¢ æ”¶é›†æ›´å¤§è§„æ¨¡çš„çœŸå®ä¸´åºŠæ•°æ®
   â€¢ å¼•å…¥æ·±åº¦å­¦ä¹ æ–¹æ³• (å¦‚CNNã€Transformer)
   â€¢ æ•´åˆå¤šç»„å­¦æ•°æ® (åŸºå› ç»„ã€è›‹ç™½ç»„ã€ä»£è°¢ç»„)
   â€¢ å¼€å‘ç”¨æˆ·å‹å¥½çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿ
   â€¢ è¿›è¡Œå‰ç»æ€§ä¸´åºŠè¯•éªŒéªŒè¯

ğŸ“ˆ æ€§èƒ½åŸºå‡†:
   â€¢ Luminal A: {(cm[0,0]/cm[0,:].sum() if len(cm) > 0 else 0):.1%} å‡†ç¡®ç‡
   â€¢ Luminal B: {(cm[1,1]/cm[1,:].sum() if len(cm) > 1 else 0):.1%} å‡†ç¡®ç‡  
   â€¢ HER2+: {(cm[2,2]/cm[2,:].sum() if len(cm) > 2 else 0):.1%} å‡†ç¡®ç‡
   â€¢ Basal-like: {(cm[3,3]/cm[3,:].sum() if len(cm) > 3 else 0):.1%} å‡†ç¡®ç‡

ğŸ’¡ åˆ›æ–°ç‚¹:
   â€¢ æ•´åˆå¤šç§ç‰¹å¾é€‰æ‹©æ–¹æ³•
   â€¢ è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
   â€¢ æä¾›è¯¦ç»†çš„ç”Ÿç‰©å­¦è§£é‡Š
   â€¢ å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æµç¨‹

=====================================
é¡¹ç›®å®Œæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
å»ºè®®å¼•ç”¨: "åŸºäºæœºå™¨å­¦ä¹ çš„ä¹³è…ºç™Œåˆ†å­äºšå‹åˆ†ç±»ç ”ç©¶"
    """
    
    print(report)
    
    print("\n" + "="*60)
    print("ğŸ‰ ç»¼åˆé¡¹ç›®å®Œæˆï¼")
    print("="*60)
    print("""
ğŸ† æ­å–œä½ å®Œæˆäº†å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼

ä½ å·²ç»æŒæ¡äº†ï¼š
âœ… ç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ é¡¹ç›®æµç¨‹
âœ… ç”Ÿç‰©åŒ»å­¦æ•°æ®çš„ç‰¹æ®Šå¤„ç†æ–¹æ³•
âœ… å¤šåˆ†ç±»é—®é¢˜çš„å»ºæ¨¡å’Œè¯„ä¼°
âœ… ä¸å¹³è¡¡æ•°æ®çš„å¤„ç†ç­–ç•¥
âœ… ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹ä¼˜åŒ–æŠ€æœ¯
âœ… ç»“æœè§£é‡Šå’Œä¸´åºŠæ„ä¹‰åˆ†æ

è¿™ä¸ªé¡¹ç›®å±•ç¤ºäº†æœºå™¨å­¦ä¹ åœ¨ç²¾å‡†åŒ»ç–—ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚
ä½ ç°åœ¨å…·å¤‡äº†åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸåº”ç”¨AIæŠ€æœ¯çš„èƒ½åŠ›ï¼

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
ğŸš€ å°è¯•çœŸå®çš„ç™Œç—‡æ•°æ®é›† (TCGA, GEO)
ğŸš€ å­¦ä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶ (PyTorch, TensorFlow)
ğŸš€ æ¢ç´¢å•ç»†èƒåˆ†æå·¥å…· (Scanpy, Seurat)
ğŸš€ å‚ä¸ç”Ÿç‰©ä¿¡æ¯å­¦å¼€æºé¡¹ç›®

è®°ä½ï¼šæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦æ´å¯Ÿæ˜¯çµé­‚ï¼
    """)
    
    return {
        'best_model': best_model,
        'results': model_results,
        'final_accuracy': final_accuracy,
        'final_f1': final_f1,
        'selected_genes': selected_genes_univariate,
        'report': report
    }


def main():
    """è¿è¡Œæ‰€æœ‰ç»ƒä¹ çš„ç­”æ¡ˆ"""
    print("ğŸ§¬ Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ç»ƒä¹  - å®Œæ•´ç­”æ¡ˆ")
    print("è¿™äº›ç­”æ¡ˆå±•ç¤ºäº†æœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å®é™…åº”ç”¨")
    print("="*80)
    
    try:
        print("å¼€å§‹è¿è¡Œæ‰€æœ‰ç»ƒä¹ ç­”æ¡ˆ...")
        
        # ç­”æ¡ˆ1: æ•°æ®é¢„å¤„ç†
        print("\nğŸ”¬ è¿è¡Œç­”æ¡ˆ1: æ•°æ®é¢„å¤„ç†...")
        df_clean, df_scaled = solution_1_data_preprocessing()
        
        # ç­”æ¡ˆ2: åŸºå› åŠŸèƒ½åˆ†ç±»
        print("\nğŸ”¬ è¿è¡Œç­”æ¡ˆ2: åŸºå› åŠŸèƒ½åˆ†ç±»...")
        classification_results, feature_names = solution_2_gene_function_classifier()
        
        # ç­”æ¡ˆ3: å•ç»†èƒèšç±»
        print("\nğŸ”¬ è¿è¡Œç­”æ¡ˆ3: å•ç»†èƒèšç±»...")
        clustering_results, cell_df, cell_scaled = solution_3_cell_clustering()
        
        # ç­”æ¡ˆ4: æ¨¡å‹ä¼˜åŒ–
        print("\nğŸ”¬ è¿è¡Œç­”æ¡ˆ4: æ¨¡å‹ä¼˜åŒ–...")
        grid_search, feature_selection_results = solution_4_model_optimization()
        
        # ç­”æ¡ˆ5: ç»¼åˆé¡¹ç›®
        print("\nğŸ”¬ è¿è¡Œç­”æ¡ˆ5: ç»¼åˆé¡¹ç›®...")
        project_results = solution_5_cancer_subtype_project()
        
        print("\n" + "="*80)
        print("ğŸ¯ æ‰€æœ‰ç»ƒä¹ ç­”æ¡ˆè¿è¡Œå®Œæˆï¼")
        print("="*80)
        
        print("""
ğŸ“š å­¦ä¹ æˆæœæ€»ç»“:

é€šè¿‡è¿™5ä¸ªç»ƒä¹ ï¼Œä½ å·²ç»å®Œæ•´æŒæ¡äº†ï¼š

1ï¸âƒ£ æ•°æ®é¢„å¤„ç† (ç­”æ¡ˆ1):
   â€¢ ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼å¤„ç†
   â€¢ æ•°æ®æ ‡å‡†åŒ–å’Œè½¬æ¢
   â€¢ ç›¸å…³æ€§åˆ†æå’Œå¯è§†åŒ–

2ï¸âƒ£ ç›‘ç£å­¦ä¹  (ç­”æ¡ˆ2): 
   â€¢ å¤šåˆ†ç±»é—®é¢˜å»ºæ¨¡
   â€¢ æ¨¡å‹æ¯”è¾ƒå’Œé€‰æ‹©
   â€¢ ç‰¹å¾é‡è¦æ€§åˆ†æ

3ï¸âƒ£ æ— ç›‘ç£å­¦ä¹  (ç­”æ¡ˆ3):
   â€¢ èšç±»ç®—æ³•æ¯”è¾ƒ
   â€¢ é™ç»´å’Œå¯è§†åŒ–
   â€¢ ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š

4ï¸âƒ£ æ¨¡å‹ä¼˜åŒ– (ç­”æ¡ˆ4):
   â€¢ äº¤å‰éªŒè¯è¯„ä¼°
   â€¢ è¶…å‚æ•°è°ƒä¼˜
   â€¢ è¿‡æ‹Ÿåˆæ£€æµ‹å’Œç‰¹å¾é€‰æ‹©

5ï¸âƒ£ ç»¼åˆåº”ç”¨ (ç­”æ¡ˆ5):
   â€¢ å®Œæ•´é¡¹ç›®æµç¨‹
   â€¢ å¤šç»„å­¦æ•°æ®æ•´åˆ
   â€¢ ä¸´åºŠåº”ç”¨ä»·å€¼è¯„ä¼°

ğŸ‰ ä½ ç°åœ¨å·²ç»å…·å¤‡äº†åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸ
   åº”ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯çš„å®Œæ•´æŠ€èƒ½ï¼

ç»§ç»­ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œåœ¨å®è·µä¸­ä¸æ–­æå‡ï¼ ğŸ§¬ğŸ’»ğŸš€
        """)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç¯å¢ƒä¾èµ–å’Œæ•°æ®ç”Ÿæˆè¿‡ç¨‹")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()