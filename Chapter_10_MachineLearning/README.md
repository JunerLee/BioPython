# Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

> æœºå™¨å­¦ä¹  = ä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹çš„ç®—æ³•

## ğŸ§¬ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†ï¼š
- æŒæ¡ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µ
- å­¦ä¼šä½¿ç”¨scikit-learnè¿›è¡ŒåŸºå› åŠŸèƒ½é¢„æµ‹å’Œç»†èƒåˆ†å‹
- ç†è§£æ¨¡å‹è¯„ä¼°ã€ç‰¹å¾å·¥ç¨‹å’Œè¶…å‚æ•°è°ƒä¼˜
- åˆæ­¥äº†è§£æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åº”ç”¨

## ğŸš€ ä¸ºä»€ä¹ˆç”Ÿç‰©å­¦éœ€è¦æœºå™¨å­¦ä¹ ï¼Ÿ

- **æ•°æ®çˆ†ç‚¸**ï¼šNGSäº§ç”ŸGBçº§æ•°æ®ï¼Œéœ€è¦è‡ªåŠ¨åŒ–åˆ†æ
- **ç²¾å‡†åŒ»ç–—**ï¼šåŸºäºä¸ªä½“åŸºå› ç»„çš„ä¸ªæ€§åŒ–æ²»ç–—
- **è¯ç‰©ç ”å‘**ï¼šAIåŠ é€Ÿé¶ç‚¹å‘ç°å’ŒåŒ–åˆç‰©ç­›é€‰
- **ç»†èƒå‘ç°**ï¼šå•ç»†èƒæ•°æ®ä¸­è¯†åˆ«æ–°çš„ç»†èƒäºšç¾¤

## ğŸ“š æ ¸å¿ƒå†…å®¹

### 1. ç›‘ç£å­¦ä¹  - æœ‰è€å¸ˆçš„å­¦ä¹ 
- **åŸºå› åŠŸèƒ½åˆ†ç±»**ï¼šæ ¹æ®åºåˆ—ç‰¹å¾é¢„æµ‹åŸºå› åŠŸèƒ½
- **ç®—æ³•å¯¹æ¯”**ï¼šé€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€SVM
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°

### 2. æ— ç›‘ç£å­¦ä¹  - å‘ç°éšè—æ¨¡å¼  
- **ç»†èƒäºšå‹å‘ç°**ï¼šä»è¡¨è¾¾è°±æ•°æ®è¯†åˆ«ç»†èƒç±»å‹
- **èšç±»ç®—æ³•**ï¼šK-meansã€DBSCANã€å±‚æ¬¡èšç±»
- **é™ç»´å¯è§†åŒ–**ï¼šPCAã€t-SNEå¯è§†åŒ–é«˜ç»´æ•°æ®

### 3. æ¨¡å‹ä¼˜åŒ– - æå‡æ€§èƒ½çš„è‰ºæœ¯
- **ç‰¹å¾å·¥ç¨‹**ï¼šåºåˆ—ç‰¹å¾ã€ç»“æ„ç‰¹å¾ã€ç½‘ç»œç‰¹å¾
- **è¶…å‚æ•°è°ƒä¼˜**ï¼šç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°
- **é¿å…è¿‡æ‹Ÿåˆ**ï¼šäº¤å‰éªŒè¯å’Œæ­£åˆ™åŒ–

### 4. æ·±åº¦å­¦ä¹ å±•æœ› - AIçš„æœªæ¥
- **AlphaFold**ï¼šè›‹ç™½è´¨ç»“æ„é¢„æµ‹é©å‘½
- **åŸºå› è°ƒæ§**ï¼šä»åºåˆ—é¢„æµ‹è¡¨è¾¾æ¨¡å¼
- **è¯ç‰©å‘ç°**ï¼šAIåŠ é€Ÿæ–°è¯å¼€å‘

## ğŸ”¬ å®æˆ˜é¡¹ç›®

### é¡¹ç›®1ï¼šåŸºå› åŠŸèƒ½é¢„æµ‹å™¨
æ ¹æ®åŸºå› åºåˆ—ç‰¹å¾ï¼ˆGCå«é‡ã€é•¿åº¦ã€ä¿å®ˆæ€§ç­‰ï¼‰é¢„æµ‹åŸºå› åŠŸèƒ½ç±»åˆ«ï¼š
- ç®¡å®¶åŸºå› ï¼ˆhousekeepingï¼‰
- è½¬å½•å› å­ï¼ˆtranscription factorï¼‰  
- æ¿€é…¶ï¼ˆkinaseï¼‰

### é¡¹ç›®2ï¼šç»†èƒäºšå‹å‘ç°
ä»å•ç»†èƒåŸºå› è¡¨è¾¾æ•°æ®ä¸­å‘ç°éšè—çš„ç»†èƒç±»å‹ï¼š
- å¹²ç»†èƒ
- åˆ†åŒ–ä¸­çš„ç»†èƒ
- æˆç†Ÿç»†èƒ

## ğŸ“– ç”Ÿç‰©å­¦ç±»æ¯”

| æœºå™¨å­¦ä¹ æ¦‚å¿µ | ç”Ÿç‰©å­¦ç±»æ¯” |
|-------------|----------|
| ç›‘ç£å­¦ä¹  | æœ‰æ ‡å‡†ç­”æ¡ˆçš„ç—…ä¾‹å­¦ä¹  |
| æ— ç›‘ç£å­¦ä¹  | åœ¨æ˜¾å¾®é•œä¸‹åˆ†ç±»æœªçŸ¥ç»†èƒ |
| è¿‡æ‹Ÿåˆ | æ­»è®°ç¡¬èƒŒç—…ä¾‹ä½†ä¸ç†è§£æœ¬è´¨ |
| ç‰¹å¾å·¥ç¨‹ | é€‰æ‹©åˆé€‚çš„ç”Ÿç‰©æ ‡è®°ç‰© |
| äº¤å‰éªŒè¯ | å¤šä¸­å¿ƒä¸´åºŠè¯•éªŒ |
| é›†æˆå­¦ä¹  | å¤šä¸ªåŒ»ç”Ÿä¼šè¯ŠæŠ•ç¥¨ |

---

## å¿«é€Ÿå…¥é—¨

### 1.1 ç”Ÿç‰©å¤§æ•°æ®çš„æŒ‘æˆ˜

è®©æˆ‘ä»¬å…ˆçœ‹çœ‹ç°ä»£ç”Ÿç‰©å­¦é¢ä¸´çš„æ•°æ®è§„æ¨¡ï¼š

```python
# ç”Ÿç‰©æ•°æ®è§„æ¨¡ç¤ºä¾‹
data_scales = {
    "äººç±»åŸºå› ç»„": "3.2 Ã— 10^9 ç¢±åŸºå¯¹",
    "äººç±»åŸºå› æ•°é‡": "~20,000 ä¸ª",
    "è›‹ç™½è´¨ç¼–ç å˜å¼‚": "~4-5 ç™¾ä¸‡ä¸ª/äºº",
    "å•ç»†èƒRNA-seq": "~20,000 åŸºå›  Ã— 100,000 ç»†èƒ",
    "è›‹ç™½è´¨ç»“æ„æ•°æ®åº“": "> 200,000 ä¸ªç»“æ„",
    "ä»£è°¢ç‰©ç§ç±»": "> 100,000 ç§",
    "è¯ç‰©-é¶ç‚¹ç›¸äº’ä½œç”¨": "> 1,000,000 å¯¹"
}

for data_type, scale in data_scales.items():
    print(f"{data_type}: {scale}")
```

é¢å¯¹å¦‚æ­¤æµ·é‡çš„æ•°æ®ï¼Œä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å·²ç»åŠ›ä¸ä»å¿ƒã€‚è¿™å°±åƒç”¨æ”¾å¤§é•œå»è§‚å¯Ÿæ•´ä¸ªåœ°çƒ - ä½ éœ€è¦æ›´å¼ºå¤§çš„å·¥å…·ã€‚

### 1.2 æ¨¡å¼è¯†åˆ«çš„ç”Ÿç‰©å­¦æ„ä¹‰

åœ¨ç”Ÿç‰©å­¦ä¸­ï¼Œ**æ¨¡å¼**æ— å¤„ä¸åœ¨ï¼š

1. **åºåˆ—æ¨¡å¼**ï¼šå¯åŠ¨å­åºåˆ—ã€å‰ªæ¥ä½ç‚¹ã€è›‹ç™½è´¨ç»“æ„åŸŸ
2. **è¡¨è¾¾æ¨¡å¼**ï¼šç»„ç»‡ç‰¹å¼‚æ€§è¡¨è¾¾ã€æ˜¼å¤œèŠ‚å¾‹ã€å‘è‚²é˜¶æ®µ
3. **ç›¸äº’ä½œç”¨æ¨¡å¼**ï¼šè›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œã€ä»£è°¢é€šè·¯
4. **è¿›åŒ–æ¨¡å¼**ï¼šä¿å®ˆåºåˆ—ã€ååŒè¿›åŒ–ã€é€‰æ‹©å‹åŠ›
5. **ç–¾ç—…æ¨¡å¼**ï¼šåŸºå› çªå˜ç‰¹å¾ã€è¡¨è¾¾è°±æ”¹å˜ã€ä»£è°¢å¼‚å¸¸

æœºå™¨å­¦ä¹ å°±æ˜¯å‘ç°å’Œåˆ©ç”¨è¿™äº›æ¨¡å¼çš„å¼ºå¤§å·¥å…·ã€‚

### 1.3 æˆåŠŸæ¡ˆä¾‹ - AIæ”¹å˜ç”Ÿç‰©å­¦

#### æ¡ˆä¾‹1ï¼šAlphaFold - ç ´è§£è›‹ç™½è´¨æŠ˜å å¯†ç 

```python
# AlphaFoldçš„å½±å“
alphafold_impact = """
ğŸ† 2020å¹´CASP14ç«èµ›ï¼šå‡†ç¡®åº¦è¾¾åˆ°92.4 GDT
ğŸ“Š 2022å¹´7æœˆï¼šé¢„æµ‹äº†2.14äº¿ä¸ªè›‹ç™½è´¨ç»“æ„
â±ï¸ é¢„æµ‹é€Ÿåº¦ï¼šå‡ åˆ†é’Ÿ vs å®éªŒæµ‹å®šçš„å‡ ä¸ªæœˆ
ğŸ’° èŠ‚çœæˆæœ¬ï¼šæ¯ä¸ªç»“æ„èŠ‚çœ10-100ä¸‡ç¾å…ƒ
ğŸ”¬ ç§‘å­¦å½±å“ï¼šåŠ é€Ÿè¯ç‰©ç ”å‘ã€ç†è§£ç–¾ç—…æœºåˆ¶
"""
print(alphafold_impact)
```

#### æ¡ˆä¾‹2ï¼šç™Œç—‡æ—©æœŸè¯Šæ–­

```python
# æœºå™¨å­¦ä¹ åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„åº”ç”¨
cancer_ml_applications = {
    "æ¶²ä½“æ´»æ£€": "ä»è¡€æ¶²ctDNAé¢„æµ‹è‚¿ç˜¤ç±»å‹ï¼Œå‡†ç¡®ç‡>90%",
    "ç—…ç†å›¾åƒ": "è¯†åˆ«ç™Œç»†èƒï¼Œé€Ÿåº¦æ¯”ç—…ç†å­¦å®¶å¿«1000å€",
    "åŸºå› è¡¨è¾¾": "é¢„æµ‹é¢„åå’Œè¯ç‰©ååº”ï¼Œä¸ªæ€§åŒ–æ²»ç–—",
    "å½±åƒè¯Šæ–­": "CT/MRIæ—©æœŸç—…å˜æ£€æµ‹ï¼Œå‡å°‘æ¼è¯Š50%"
}
```

#### æ¡ˆä¾‹3ï¼šæ–°å† è¯ç‰©ç ”å‘

```python
# COVID-19è¯ç‰©ç ”å‘ä¸­çš„AIåº”ç”¨
covid_ai_timeline = """
2020å¹´1æœˆï¼šç—…æ¯’åºåˆ—å…¬å¸ƒ
2020å¹´2æœˆï¼šAIé¢„æµ‹ç—…æ¯’è›‹ç™½ç»“æ„
2020å¹´3æœˆï¼šè™šæ‹Ÿç­›é€‰10äº¿ä¸ªåŒ–åˆç‰©
2020å¹´4æœˆï¼šç¡®å®šå€™é€‰è¯ç‰©
2020å¹´12æœˆï¼šè¿›å…¥ä¸´åºŠè¯•éªŒ

ä¼ ç»Ÿæ–¹æ³•éœ€è¦ï¼š5-10å¹´
AIåŠ é€Ÿåï¼š< 1å¹´
"""
```

---

## ç¬¬2éƒ¨åˆ†ï¼šæœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µ - ç”¨ç”Ÿç‰©å­¦æ€ç»´ç†è§£AI

### 2.1 ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ

æƒ³è±¡ä¸€ä¸ªåŒ»å­¦ç”Ÿçš„æˆé•¿è¿‡ç¨‹ï¼š

1. **å­¦ä¹ é˜¶æ®µ**ï¼šçœ‹æ•™ç§‘ä¹¦ã€è§‚å¯Ÿç—…ä¾‹ã€è·Ÿéšå¯¼å¸ˆ
2. **å®è·µé˜¶æ®µ**ï¼šè¯Šæ–­ç—…äººã€çŠ¯é”™ã€çº æ­£
3. **æˆç†Ÿé˜¶æ®µ**ï¼šç‹¬ç«‹è¯Šæ–­ã€ç»éªŒä¸°å¯Œ

æœºå™¨å­¦ä¹ çš„è¿‡ç¨‹å®Œå…¨ä¸€æ ·ï¼š

```python
# æœºå™¨å­¦ä¹ è¿‡ç¨‹ç±»æ¯”
ml_process = {
    "è®­ç»ƒ(Training)": "åƒåŒ»å­¦ç”Ÿå­¦ä¹ ç—…ä¾‹",
    "éªŒè¯(Validation)": "åƒå®ä¹ åŒ»ç”Ÿçš„ç»ƒä¹ ",  
    "æµ‹è¯•(Testing)": "åƒæ‰§ä¸šåŒ»å¸ˆçš„è€ƒæ ¸",
    "éƒ¨ç½²(Deployment)": "åƒç‹¬ç«‹è¡ŒåŒ»"
}
```

### 2.2 ç›‘ç£å­¦ä¹  - æœ‰æ ‡å‡†ç­”æ¡ˆçš„å­¦ä¹ 

#### ç”Ÿç‰©å­¦ç±»æ¯”
ç›‘ç£å­¦ä¹ å°±åƒ**å¸¦ç­”æ¡ˆçš„ä¹ é¢˜é›†**ã€‚ä½ æœ‰ä¸€æ‰¹å·²çŸ¥è¯Šæ–­ç»“æœçš„ç—…äººæ•°æ®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡è¿™äº›"æ ‡å‡†ç­”æ¡ˆ"å­¦ä¼šè¯Šæ–­è§„å¾‹ã€‚

#### æ ¸å¿ƒæ¦‚å¿µ

```python
# ç›‘ç£å­¦ä¹ çš„è¦ç´ 
supervised_learning = {
    "è¾“å…¥(X)": "ç‰¹å¾ - å¦‚åŸºå› è¡¨è¾¾ã€ä¸´åºŠæŒ‡æ ‡",
    "è¾“å‡º(y)": "æ ‡ç­¾ - å¦‚ç–¾ç—…/å¥åº·ã€è¯ç‰©æ•æ„Ÿ/è€è¯",
    "æ¨¡å‹(f)": "å­¦ä¹ Xåˆ°yçš„æ˜ å°„å…³ç³»",
    "ç›®æ ‡": "æœ€å°åŒ–é¢„æµ‹è¯¯å·®"
}

# ä¸¤å¤§ç±»ä»»åŠ¡
tasks = {
    "åˆ†ç±»(Classification)": {
        "è¾“å‡º": "ç¦»æ•£ç±»åˆ«",
        "ä¾‹å­": ["ç–¾ç—…è¯Šæ–­", "ç»†èƒç±»å‹è¯†åˆ«", "åŸºå› åŠŸèƒ½é¢„æµ‹"],
        "ç®—æ³•": ["é€»è¾‘å›å½’", "å†³ç­–æ ‘", "SVM", "éšæœºæ£®æ—"]
    },
    "å›å½’(Regression)": {
        "è¾“å‡º": "è¿ç»­æ•°å€¼",
        "ä¾‹å­": ["è¯ç‰©å‰‚é‡é¢„æµ‹", "ç”Ÿå­˜æœŸé¢„æµ‹", "åŸºå› è¡¨è¾¾é‡é¢„æµ‹"],
        "ç®—æ³•": ["çº¿æ€§å›å½’", "å¤šé¡¹å¼å›å½’", "å›å½’æ ‘", "ç¥ç»ç½‘ç»œ"]
    }
}
```

#### å®é™…ä¾‹å­ï¼šé¢„æµ‹åŸºå› åŠŸèƒ½

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# æ¨¡æ‹ŸåŸºå› ç‰¹å¾æ•°æ®
np.random.seed(42)
n_genes = 100

# ç‰¹å¾ï¼šGCå«é‡ã€é•¿åº¦ã€è¡¨è¾¾æ°´å¹³ç­‰
features = []
labels = []  # 0: ç®¡å®¶åŸºå› , 1: è½¬å½•å› å­

for i in range(n_genes):
    if i < 50:  # ç®¡å®¶åŸºå› 
        gc_content = np.random.normal(0.45, 0.05)
        expression = np.random.normal(0.8, 0.1)
        length = np.random.normal(1500, 200)
        labels.append(0)
    else:  # è½¬å½•å› å­
        gc_content = np.random.normal(0.55, 0.05)
        expression = np.random.normal(0.3, 0.1)
        length = np.random.normal(2000, 300)
        labels.append(1)
    
    features.append([gc_content, expression, length])

X = np.array(features)
y = np.array(labels)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# è¯„ä¼°
accuracy = model.score(X_test, y_test)
print(f"é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.2%}")

# ç‰¹å¾é‡è¦æ€§
feature_names = ['GCå«é‡', 'è¡¨è¾¾æ°´å¹³', 'åºåˆ—é•¿åº¦']
importances = model.feature_importances_
for name, imp in zip(feature_names, importances):
    print(f"{name}: {imp:.3f}")
```

### 2.3 æ— ç›‘ç£å­¦ä¹  - è‡ªä¸»æ¢ç´¢çš„å­¦ä¹ 

#### ç”Ÿç‰©å­¦ç±»æ¯”
æ— ç›‘ç£å­¦ä¹ å°±åƒ**åœ¨æ˜¾å¾®é•œä¸‹è§‚å¯ŸæœªçŸ¥ç»†èƒ**ã€‚ä½ ä¸çŸ¥é“å®ƒä»¬æ˜¯ä»€ä¹ˆç±»å‹ï¼Œä½†å¯ä»¥æ ¹æ®å½¢æ€ã€å¤§å°ã€æŸ“è‰²ç­‰ç‰¹å¾å°†ç›¸ä¼¼çš„å½’ä¸ºä¸€ç»„ã€‚

#### æ ¸å¿ƒæ¦‚å¿µ

```python
# æ— ç›‘ç£å­¦ä¹ çš„ç‰¹ç‚¹
unsupervised_learning = {
    "è¾“å…¥": "åªæœ‰ç‰¹å¾Xï¼Œæ²¡æœ‰æ ‡ç­¾y",
    "ç›®æ ‡": "å‘ç°æ•°æ®çš„å†…åœ¨ç»“æ„",
    "ä¼˜åŠ¿": "ä¸éœ€è¦æ˜‚è´µçš„æ ‡æ³¨æ•°æ®",
    "æŒ‘æˆ˜": "ç»“æœè§£é‡Šéœ€è¦é¢†åŸŸçŸ¥è¯†"
}

# ä¸»è¦ä»»åŠ¡ç±»å‹
unsupervised_tasks = {
    "èšç±»(Clustering)": {
        "ç›®çš„": "å°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„",
        "åº”ç”¨": ["ç»†èƒç±»å‹å‘ç°", "åŸºå› å…±è¡¨è¾¾æ¨¡å—", "æ‚£è€…åˆ†å‹"],
        "ç®—æ³•": ["K-means", "å±‚æ¬¡èšç±»", "DBSCAN"]
    },
    "é™ç»´(Dimensionality Reduction)": {
        "ç›®çš„": "æå–ä¸»è¦ç‰¹å¾ï¼Œå¯è§†åŒ–",
        "åº”ç”¨": ["å•ç»†èƒæ•°æ®å¯è§†åŒ–", "æ‰¹æ¬¡æ•ˆåº”å»é™¤"],
        "ç®—æ³•": ["PCA", "t-SNE", "UMAP"]
    },
    "å¼‚å¸¸æ£€æµ‹(Anomaly Detection)": {
        "ç›®çš„": "å‘ç°å¼‚å¸¸æ ·æœ¬",
        "åº”ç”¨": ["ç½•è§çªå˜", "æ±¡æŸ“æ ·æœ¬", "å®éªŒå¼‚å¸¸"],
        "ç®—æ³•": ["Isolation Forest", "One-class SVM"]
    }
}
```

#### å®é™…ä¾‹å­ï¼šå‘ç°ç»†èƒäºšå‹

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿå•ç»†èƒåŸºå› è¡¨è¾¾æ•°æ®
np.random.seed(42)
n_cells = 150
n_genes = 50

# ä¸‰ç§ç»†èƒç±»å‹ï¼ˆä½†æˆ‘ä»¬"ä¸çŸ¥é“"ï¼‰
cell_type1 = np.random.lognormal(2, 0.5, (50, n_genes))
cell_type1[:, :10] *= 2  # æŸäº›åŸºå› é«˜è¡¨è¾¾

cell_type2 = np.random.lognormal(2, 0.6, (50, n_genes))
cell_type2[:, 20:30] *= 2.5

cell_type3 = np.random.lognormal(2, 0.4, (50, n_genes))
cell_type3[:, 35:45] *= 2

# åˆå¹¶æ•°æ®
expression_data = np.vstack([cell_type1, cell_type2, cell_type3])

# æ•°æ®é¢„å¤„ç†
scaler = StandardScaler()
data_scaled = scaler.fit_transform(np.log1p(expression_data))

# PCAé™ç»´å¯è§†åŒ–
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# K-meansèšç±»
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(data_scaled)

# å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.6)
plt.title('åŸå§‹æ•°æ®ï¼ˆæ— æ ‡ç­¾ï¼‰')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')

plt.subplot(1, 2, 2)
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.title('K-meansèšç±»ç»“æœ')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.colorbar(label='Cluster')

plt.tight_layout()
plt.show()

print(f"å‘ç°äº† {len(np.unique(clusters))} ä¸ªç»†èƒç¾¤")
```

### 2.4 å¼ºåŒ–å­¦ä¹  - è¯•é”™ä¸­å­¦ä¹ 

#### ç”Ÿç‰©å­¦ç±»æ¯”
å¼ºåŒ–å­¦ä¹ å°±åƒ**è®­ç»ƒå°é¼ èµ°è¿·å®«**ã€‚é€šè¿‡å¥–åŠ±ï¼ˆæ‰¾åˆ°é£Ÿç‰©ï¼‰å’Œæƒ©ç½šï¼ˆç¢°å£ï¼‰ï¼Œå°é¼ å­¦ä¼šæœ€ä¼˜è·¯å¾„ã€‚

```python
# å¼ºåŒ–å­¦ä¹ åœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨
reinforcement_applications = {
    "è¯ç‰©ç»„åˆä¼˜åŒ–": "å¯»æ‰¾æœ€ä½³è¯ç‰©ç»„åˆå’Œå‰‚é‡",
    "å®éªŒè®¾è®¡": "è‡ªåŠ¨ä¼˜åŒ–å®éªŒæ¡ä»¶",
    "æ²»ç–—æ–¹æ¡ˆ": "ä¸ªæ€§åŒ–æ²»ç–—ç­–ç•¥",
    "è›‹ç™½è´¨è®¾è®¡": "ä¼˜åŒ–è›‹ç™½è´¨åºåˆ—"
}
```

### 2.5 æ·±åº¦å­¦ä¹  - å¤šå±‚æ¬¡ç†è§£

#### ç”Ÿç‰©å­¦ç±»æ¯”
æ·±åº¦å­¦ä¹ å°±åƒ**å¤§è„‘çš„åˆ†å±‚å¤„ç†**ï¼š
- ç¬¬ä¸€å±‚ï¼šè¯†åˆ«è¾¹ç¼˜ï¼ˆå¦‚ç»†èƒè¾¹ç•Œï¼‰
- ç¬¬äºŒå±‚ï¼šè¯†åˆ«å½¢çŠ¶ï¼ˆå¦‚ç»†èƒæ ¸ï¼‰
- ç¬¬ä¸‰å±‚ï¼šè¯†åˆ«æ¨¡å¼ï¼ˆå¦‚ç™Œç»†èƒç‰¹å¾ï¼‰
- æœ€ç»ˆå±‚ï¼šåšå‡ºåˆ¤æ–­ï¼ˆè¯Šæ–­ç»“æœï¼‰

```python
# æ·±åº¦å­¦ä¹ çš„å±‚æ¬¡ç»“æ„
deep_learning_layers = {
    "è¾“å…¥å±‚": "åŸå§‹æ•°æ®ï¼ˆå¦‚DNAåºåˆ—ï¼‰",
    "éšè—å±‚1": "è¯†åˆ«åŸºæœ¬æ¨¡å¼ï¼ˆå¦‚ç¢±åŸºç»„åˆï¼‰",
    "éšè—å±‚2": "è¯†åˆ«é«˜çº§ç‰¹å¾ï¼ˆå¦‚å¯åŠ¨å­ï¼‰",
    "éšè—å±‚3": "ç†è§£åŠŸèƒ½å…³ç³»ï¼ˆå¦‚è°ƒæ§ä½œç”¨ï¼‰",
    "è¾“å‡ºå±‚": "æœ€ç»ˆé¢„æµ‹ï¼ˆå¦‚åŸºå› è¡¨è¾¾æ°´å¹³ï¼‰"
}
```

---

## ç¬¬3éƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹ - æ•°æ®çš„"åŸºå› å·¥ç¨‹"

### 3.1 ä»€ä¹ˆæ˜¯ç‰¹å¾ï¼Ÿ

åœ¨ç”Ÿç‰©å­¦ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯åœ¨å¯»æ‰¾**æ ‡è®°ç‰©**ï¼ˆbiomarkerï¼‰- èƒ½å¤ŸæŒ‡ç¤ºç”Ÿç‰©çŠ¶æ€çš„å¯æµ‹é‡æŒ‡æ ‡ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè¿™äº›æ ‡è®°ç‰©å°±æ˜¯**ç‰¹å¾**ï¼ˆfeaturesï¼‰ã€‚

```python
# ç”Ÿç‰©å­¦æ ‡è®°ç‰© vs æœºå™¨å­¦ä¹ ç‰¹å¾
biomarker_to_feature = {
    "ç”Ÿç‰©æ ‡è®°ç‰©": "æœºå™¨å­¦ä¹ ç‰¹å¾",
    "PSAæ°´å¹³": "å‰åˆ—è…ºç™Œé¢„æµ‹ç‰¹å¾",
    "CD4+ç»†èƒè®¡æ•°": "HIVè¿›å±•ç‰¹å¾",
    "HER2è¡¨è¾¾": "ä¹³è…ºç™Œåˆ†å‹ç‰¹å¾",
    "ç”²åŸºåŒ–æ¨¡å¼": "è¡¨è§‚é—ä¼ ç‰¹å¾",
    "å¾®ç”Ÿç‰©ç»„æˆ": "è‚ é“å¥åº·ç‰¹å¾"
}
```

### 3.2 ç‰¹å¾ç±»å‹è¯¦è§£

#### 3.2.1 æ•°å€¼ç‰¹å¾

```python
# å¸¸è§çš„æ•°å€¼ç‰¹å¾
numerical_features = {
    "åŸºå› è¡¨è¾¾é‡": "è¿ç»­å€¼ï¼Œé€šå¸¸éœ€è¦å¯¹æ•°è½¬æ¢",
    "è›‹ç™½è´¨æµ“åº¦": "è¿ç»­å€¼ï¼Œå¯èƒ½éœ€è¦æ ‡å‡†åŒ–",
    "å¹´é¾„": "è¿ç»­å€¼ï¼Œå¯èƒ½éœ€è¦åˆ†ç®±",
    "BMI": "è¿ç»­å€¼ï¼Œèº«é«˜ä½“é‡çš„ç»„åˆç‰¹å¾",
    "çªå˜é¢‘ç‡": "æ¯”ç‡ï¼ŒèŒƒå›´0-1"
}

# ç¤ºä¾‹ï¼šå¤„ç†åŸºå› è¡¨è¾¾æ•°æ®
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# æ¨¡æ‹ŸåŸºå› è¡¨è¾¾æ•°æ®ï¼ˆé€šå¸¸æ˜¯åæ€åˆ†å¸ƒï¼‰
expression = np.random.lognormal(3, 2, 1000)

# æ–¹æ³•1ï¼šå¯¹æ•°è½¬æ¢
expression_log = np.log1p(expression)  # log(1+x) é¿å…log(0)

# æ–¹æ³•2ï¼šæ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
scaler = StandardScaler()
expression_zscore = scaler.fit_transform(expression.reshape(-1, 1))

# æ–¹æ³•3ï¼šå½’ä¸€åŒ–ï¼ˆ0-1èŒƒå›´ï¼‰
minmax = MinMaxScaler()
expression_norm = minmax.fit_transform(expression.reshape(-1, 1))

print(f"åŸå§‹æ•°æ®èŒƒå›´: [{expression.min():.2f}, {expression.max():.2f}]")
print(f"å¯¹æ•°è½¬æ¢å: [{expression_log.min():.2f}, {expression_log.max():.2f}]")
print(f"æ ‡å‡†åŒ–å: [{expression_zscore.min():.2f}, {expression_zscore.max():.2f}]")
print(f"å½’ä¸€åŒ–å: [{expression_norm.min():.2f}, {expression_norm.max():.2f}]")
```

#### 3.2.2 åˆ†ç±»ç‰¹å¾

```python
# ç”Ÿç‰©å­¦ä¸­çš„åˆ†ç±»ç‰¹å¾
categorical_features = {
    "æ€§åˆ«": ["ç”·", "å¥³"],
    "è¡€å‹": ["A", "B", "AB", "O"],
    "åŸºå› å‹": ["AA", "Aa", "aa"],
    "ç™Œç—‡åˆ†æœŸ": ["I", "II", "III", "IV"],
    "ç»†èƒç±»å‹": ["Tç»†èƒ", "Bç»†èƒ", "NKç»†èƒ", "å·¨å™¬ç»†èƒ"]
}

# ç¼–ç æ–¹æ³•
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# ç¤ºä¾‹ï¼šç¼–ç è¡€å‹
blood_types = ['A', 'B', 'O', 'AB', 'A', 'O', 'B']

# æ–¹æ³•1ï¼šæ ‡ç­¾ç¼–ç ï¼ˆæœ‰åºåˆ†ç±»ï¼‰
label_encoder = LabelEncoder()
blood_encoded = label_encoder.fit_transform(blood_types)
print(f"æ ‡ç­¾ç¼–ç : {blood_encoded}")

# æ–¹æ³•2ï¼šç‹¬çƒ­ç¼–ç ï¼ˆæ— åºåˆ†ç±»ï¼‰
onehot = OneHotEncoder(sparse=False)
blood_onehot = onehot.fit_transform(np.array(blood_types).reshape(-1, 1))
print(f"ç‹¬çƒ­ç¼–ç :\n{blood_onehot}")
```

#### 3.2.3 åºåˆ—ç‰¹å¾

```python
# DNA/è›‹ç™½è´¨åºåˆ—ç‰¹å¾æå–
def extract_sequence_features(sequence, seq_type='DNA'):
    """ä»åºåˆ—ä¸­æå–å¤šç§ç‰¹å¾"""
    features = {}
    
    if seq_type == 'DNA':
        # 1. ç¢±åŸºç»„æˆ
        for base in 'ATCG':
            features[f'{base}_content'] = sequence.count(base) / len(sequence)
        
        # 2. GCå«é‡
        gc_count = sequence.count('G') + sequence.count('C')
        features['GC_content'] = gc_count / len(sequence)
        
        # 3. äºŒæ ¸è‹·é…¸é¢‘ç‡
        dinucleotides = ['AA', 'AT', 'AG', 'AC', 
                        'TA', 'TT', 'TG', 'TC',
                        'GA', 'GT', 'GG', 'GC',
                        'CA', 'CT', 'CG', 'CC']
        for di in dinucleotides:
            features[f'di_{di}'] = sequence.count(di) / (len(sequence) - 1)
        
        # 4. å¯†ç å­ä½¿ç”¨åå¥½ï¼ˆå¦‚æœé•¿åº¦æ˜¯3çš„å€æ•°ï¼‰
        if len(sequence) % 3 == 0:
            codons = [sequence[i:i+3] for i in range(0, len(sequence), 3)]
            features['ATG_frequency'] = codons.count('ATG') / len(codons)
        
    elif seq_type == 'protein':
        # æ°¨åŸºé…¸ç»„æˆ
        amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        for aa in amino_acids:
            features[f'{aa}_content'] = sequence.count(aa) / len(sequence)
        
        # ç†åŒ–æ€§è´¨
        hydrophobic = 'AILMFWYV'
        charged = 'DEKR'
        polar = 'STNQ'
        
        features['hydrophobic_ratio'] = sum(sequence.count(aa) for aa in hydrophobic) / len(sequence)
        features['charged_ratio'] = sum(sequence.count(aa) for aa in charged) / len(sequence)
        features['polar_ratio'] = sum(sequence.count(aa) for aa in polar) / len(sequence)
    
    return features

# æµ‹è¯•
dna_seq = "ATGCGATCGTAGCTAGCTAGCTAGCTAGCTA"
features = extract_sequence_features(dna_seq, 'DNA')
print(f"åºåˆ—é•¿åº¦: {len(dna_seq)}")
print(f"GCå«é‡: {features['GC_content']:.2%}")
print(f"ATäºŒæ ¸è‹·é…¸é¢‘ç‡: {features['di_AT']:.3f}")
```

#### 3.2.4 å›¾åƒç‰¹å¾

```python
# ç—…ç†å›¾åƒç‰¹å¾æå–
image_features = {
    "å½¢æ€å­¦ç‰¹å¾": {
        "ç»†èƒé¢ç§¯": "åƒç´ æ•°",
        "ç»†èƒåœ†åº¦": "4Ï€Ã—é¢ç§¯/å‘¨é•¿Â²",
        "æ ¸è´¨æ¯”": "æ ¸é¢ç§¯/ç»†èƒé¢ç§¯",
        "ç»†èƒå¯†åº¦": "å•ä½é¢ç§¯ç»†èƒæ•°"
    },
    "çº¹ç†ç‰¹å¾": {
        "ç°åº¦å…±ç”ŸçŸ©é˜µ": "çº¹ç†ç²—ç³™åº¦",
        "å°æ³¢å˜æ¢": "å¤šå°ºåº¦ç‰¹å¾",
        "Gaboræ»¤æ³¢": "æ–¹å‘æ€§çº¹ç†"
    },
    "é¢œè‰²ç‰¹å¾": {
        "å¹³å‡å¼ºåº¦": "æŸ“è‰²æ·±åº¦",
        "é¢œè‰²ç›´æ–¹å›¾": "æŸ“è‰²åˆ†å¸ƒ",
        "é¢œè‰²çŸ©": "é¢œè‰²ç»Ÿè®¡"
    }
}
```

### 3.3 ç‰¹å¾é€‰æ‹© - æ‰¾åˆ°çœŸæ­£é‡è¦çš„"åŸºå› "

å°±åƒä¸æ˜¯æ‰€æœ‰åŸºå› éƒ½ä¸ç–¾ç—…ç›¸å…³ï¼Œä¸æ˜¯æ‰€æœ‰ç‰¹å¾éƒ½å¯¹é¢„æµ‹æœ‰ç”¨ã€‚ç‰¹å¾é€‰æ‹©å¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æœ€é‡è¦çš„ç‰¹å¾ã€‚

#### 3.3.1 è¿‡æ»¤æ³•ï¼ˆFilter Methodsï¼‰

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import numpy as np

# åˆ›å»ºç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_samples = 100
n_features = 20

# 5ä¸ªç›¸å…³ç‰¹å¾ï¼Œ15ä¸ªå™ªå£°ç‰¹å¾
X = np.random.randn(n_samples, n_features)
y = (X[:, 0] + X[:, 1] - X[:, 2] + 0.5 * X[:, 3] > 0).astype(int)

# æ–¹æ³•1ï¼šANOVA Fæ£€éªŒï¼ˆç”¨äºåˆ†ç±»ï¼‰
selector_f = SelectKBest(score_func=f_classif, k=5)
X_selected_f = selector_f.fit_transform(X, y)
scores_f = selector_f.scores_

# æ–¹æ³•2ï¼šäº’ä¿¡æ¯ï¼ˆæ•æ‰éçº¿æ€§å…³ç³»ï¼‰
selector_mi = SelectKBest(score_func=mutual_info_classif, k=5)
X_selected_mi = selector_mi.fit_transform(X, y)
scores_mi = selector_mi.scores_

# æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
print("ç‰¹å¾é‡è¦æ€§æ’åï¼ˆF-scoreï¼‰:")
feature_importance = sorted(zip(range(n_features), scores_f), 
                          key=lambda x: x[1], reverse=True)
for idx, score in feature_importance[:5]:
    print(f"  ç‰¹å¾{idx}: {score:.2f}")
```

#### 3.3.2 åŒ…è£…æ³•ï¼ˆWrapper Methodsï¼‰

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰
estimator = LogisticRegression(random_state=42)
selector_rfe = RFE(estimator, n_features_to_select=5)
selector_rfe.fit(X, y)

# é€‰ä¸­çš„ç‰¹å¾
selected_features = np.where(selector_rfe.support_)[0]
print(f"\nRFEé€‰æ‹©çš„ç‰¹å¾: {selected_features}")
print(f"ç‰¹å¾æ’å: {selector_rfe.ranking_}")
```

#### 3.3.3 åµŒå…¥æ³•ï¼ˆEmbedded Methodsï¼‰

```python
from sklearn.ensemble import RandomForestClassifier

# ä½¿ç”¨éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# ç‰¹å¾é‡è¦æ€§
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

print("\néšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§:")
for i in range(5):
    print(f"  ç‰¹å¾{indices[i]}: {importances[indices[i]]:.3f}")
```

### 3.4 ç‰¹å¾å·¥ç¨‹å®æˆ˜æ¡ˆä¾‹

#### æ¡ˆä¾‹1ï¼šåŸºå› è¡¨è¾¾æ•°æ®çš„ç‰¹å¾å·¥ç¨‹

```python
# å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹
class GeneExpressionFeatureEngineering:
    """åŸºå› è¡¨è¾¾æ•°æ®çš„ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.selector = None
        
    def fit_transform(self, expression_matrix, labels=None):
        """
        å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹
        
        å‚æ•°:
            expression_matrix: åŸºå› è¡¨è¾¾çŸ©é˜µ (samples Ã— genes)
            labels: æ ·æœ¬æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        features_list = []
        
        # 1. åŸºç¡€ç‰¹å¾ï¼šåŸå§‹è¡¨è¾¾å€¼
        features_list.append(expression_matrix)
        
        # 2. ç»Ÿè®¡ç‰¹å¾
        stats_features = self._extract_stats(expression_matrix)
        features_list.append(stats_features)
        
        # 3. é€šè·¯ç‰¹å¾ï¼ˆå‡è®¾æˆ‘ä»¬æœ‰é€šè·¯ä¿¡æ¯ï¼‰
        pathway_features = self._extract_pathway_features(expression_matrix)
        features_list.append(pathway_features)
        
        # 4. ç›¸äº’ä½œç”¨ç‰¹å¾
        interaction_features = self._extract_interactions(expression_matrix)
        features_list.append(interaction_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features = np.hstack(features_list)
        
        # 5. æ ‡å‡†åŒ–
        all_features_scaled = self.scaler.fit_transform(all_features)
        
        # 6. ç‰¹å¾é€‰æ‹©ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        if labels is not None:
            all_features_selected = self._select_features(all_features_scaled, labels)
            return all_features_selected
        
        return all_features_scaled
    
    def _extract_stats(self, matrix):
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        stats = []
        stats.append(np.mean(matrix, axis=1))  # å¹³å‡è¡¨è¾¾
        stats.append(np.std(matrix, axis=1))   # è¡¨è¾¾å˜å¼‚
        stats.append(np.max(matrix, axis=1))   # æœ€å¤§è¡¨è¾¾
        stats.append(np.min(matrix, axis=1))   # æœ€å°è¡¨è¾¾
        return np.column_stack(stats)
    
    def _extract_pathway_features(self, matrix):
        """æå–é€šè·¯æ°´å¹³ç‰¹å¾ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰"""
        # å‡è®¾å‰10ä¸ªåŸºå› å±äºé€šè·¯1ï¼Œ11-20å±äºé€šè·¯2
        pathway1_mean = np.mean(matrix[:, :10], axis=1)
        pathway2_mean = np.mean(matrix[:, 10:20], axis=1)
        return np.column_stack([pathway1_mean, pathway2_mean])
    
    def _extract_interactions(self, matrix):
        """æå–åŸºå› ç›¸äº’ä½œç”¨ç‰¹å¾"""
        # ç®€å•ç¤ºä¾‹ï¼šè®¡ç®—å‡ ä¸ªå…³é”®åŸºå› çš„ä¹˜ç§¯
        interaction1 = matrix[:, 0] * matrix[:, 1]  # åŸºå› 0å’Œ1çš„ç›¸äº’ä½œç”¨
        interaction2 = matrix[:, 2] * matrix[:, 3]  # åŸºå› 2å’Œ3çš„ç›¸äº’ä½œç”¨
        return np.column_stack([interaction1, interaction2])
    
    def _select_features(self, features, labels, k=50):
        """é€‰æ‹©æœ€é‡è¦çš„kä¸ªç‰¹å¾"""
        from sklearn.feature_selection import SelectKBest, f_classif
        self.selector = SelectKBest(score_func=f_classif, k=min(k, features.shape[1]))
        return self.selector.fit_transform(features, labels)

# ä½¿ç”¨ç¤ºä¾‹
np.random.seed(42)
expression_data = np.random.lognormal(3, 1, (100, 100))  # 100ä¸ªæ ·æœ¬ï¼Œ100ä¸ªåŸºå› 
labels = np.random.randint(0, 2, 100)  # äºŒåˆ†ç±»æ ‡ç­¾

engineer = GeneExpressionFeatureEngineering()
engineered_features = engineer.fit_transform(expression_data, labels)
print(f"åŸå§‹ç‰¹å¾æ•°: {expression_data.shape[1]}")
print(f"å·¥ç¨‹åç‰¹å¾æ•°: {engineered_features.shape[1]}")
```

---

## ç¬¬4éƒ¨åˆ†ï¼šåˆ†ç±»ç®—æ³•è¯¦è§£ - è¯Šæ–­ç–¾ç—…çš„ä¸åŒæ–¹æ³•

### 4.1 é€»è¾‘å›å½’ - æœ€ç®€å•çš„è¯Šæ–­æ¨¡å‹

#### ç”Ÿç‰©å­¦ç±»æ¯”
é€»è¾‘å›å½’å°±åƒä¸€ä¸ªç®€å•çš„è¯Šæ–­æ ‡å‡†ï¼šå¦‚æœæŸä¸ªæŒ‡æ ‡è¶…è¿‡é˜ˆå€¼ï¼Œå°±è¯Šæ–­ä¸ºç–¾ç—…ã€‚

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# åˆ›å»ºä¸€ä¸ªç®€å•çš„äºŒåˆ†ç±»é—®é¢˜
X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,
                          n_informative=2, n_clusters_per_class=1,
                          random_state=42)

# è®­ç»ƒé€»è¾‘å›å½’
log_reg = LogisticRegression()
log_reg.fit(X, y)

# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(model, X, y, title):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolor='black')
    plt.xlabel('ç‰¹å¾1ï¼ˆå¦‚ï¼šåŸºå› Aè¡¨è¾¾é‡ï¼‰')
    plt.ylabel('ç‰¹å¾2ï¼ˆå¦‚ï¼šåŸºå› Bè¡¨è¾¾é‡ï¼‰')
    plt.title(title)
    plt.show()

plot_decision_boundary(log_reg, X, y, 'é€»è¾‘å›å½’ï¼šçº¿æ€§å†³ç­–è¾¹ç•Œ')

# è§£é‡Šç³»æ•°
print("é€»è¾‘å›å½’ç³»æ•°ï¼ˆç‰¹å¾æƒé‡ï¼‰:")
print(f"æˆªè·: {log_reg.intercept_[0]:.3f}")
for i, coef in enumerate(log_reg.coef_[0]):
    print(f"ç‰¹å¾{i+1}: {coef:.3f}")
```

#### ä¼˜ç¼ºç‚¹åˆ†æ

```python
logistic_regression_analysis = {
    "ä¼˜ç‚¹": [
        "ç®€å•æ˜“æ‡‚ï¼Œå¯è§£é‡Šæ€§å¼º",
        "è®¡ç®—é€Ÿåº¦å¿«",
        "ä¸éœ€è¦è°ƒå‚",
        "è¾“å‡ºæ¦‚ç‡ï¼Œä¾¿äºé£é™©è¯„ä¼°"
    ],
    "ç¼ºç‚¹": [
        "åªèƒ½å¤„ç†çº¿æ€§å¯åˆ†é—®é¢˜",
        "å¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿ",
        "éš¾ä»¥å¤„ç†ç‰¹å¾é—´çš„å¤æ‚å…³ç³»"
    ],
    "é€‚ç”¨åœºæ™¯": [
        "çº¿æ€§å¯åˆ†çš„é—®é¢˜",
        "éœ€è¦æ¦‚ç‡è¾“å‡ºçš„åœºæ™¯",
        "ç‰¹å¾å·²ç»è¿‡è‰¯å¥½å·¥ç¨‹çš„æ•°æ®",
        "åŸºçº¿æ¨¡å‹"
    ]
}
```

### 4.2 å†³ç­–æ ‘ - è¯Šæ–­æµç¨‹å›¾

#### ç”Ÿç‰©å­¦ç±»æ¯”
å†³ç­–æ ‘å°±åƒåŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹å›¾ï¼šå…ˆæ£€æŸ¥ä½“æ¸©ï¼Œå¦‚æœå‘çƒ§å†æ£€æŸ¥ç™½ç»†èƒï¼Œä¸€æ­¥æ­¥ç¼©å°è¯Šæ–­èŒƒå›´ã€‚

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# åˆ›å»ºç–¾ç—…è¯Šæ–­ç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_patients = 200

# ç‰¹å¾ï¼šä½“æ¸©ã€ç™½ç»†èƒè®¡æ•°ã€CRPæ°´å¹³
temperature = np.random.normal(37, 1.5, n_patients)
wbc_count = np.random.normal(7000, 2000, n_patients)
crp_level = np.random.normal(5, 3, n_patients)

# æ ‡ç­¾ï¼šæ ¹æ®è§„åˆ™ç”Ÿæˆï¼ˆæ¨¡æ‹ŸåŒ»ç”Ÿè¯Šæ–­é€»è¾‘ï¼‰
labels = []
for t, w, c in zip(temperature, wbc_count, crp_level):
    if t > 38.5 and w > 10000:
        labels.append(1)  # ç»†èŒæ„ŸæŸ“
    elif t > 37.5 and c > 10:
        labels.append(1)  # ç‚ç—‡
    else:
        labels.append(0)  # å¥åº·

X = np.column_stack([temperature, wbc_count, crp_level])
y = np.array(labels)

# è®­ç»ƒå†³ç­–æ ‘
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X, y)

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(15, 10))
plot_tree(tree, 
          feature_names=['ä½“æ¸©', 'ç™½ç»†èƒ', 'CRP'],
          class_names=['å¥åº·', 'ç–¾ç—…'],
          filled=True,
          rounded=True,
          fontsize=10)
plt.title('åŒ»ç–—è¯Šæ–­å†³ç­–æ ‘')
plt.show()

# æå–å†³ç­–è§„åˆ™
def get_rules(tree, feature_names):
    """æå–å†³ç­–æ ‘è§„åˆ™"""
    from sklearn.tree import _tree
    
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]
    
    def recurse(node, depth, parent_rule=""):
        indent = "  " * depth
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            print(f"{indent}å¦‚æœ {name} <= {threshold:.2f}:")
            recurse(tree_.children_left[node], depth + 1, f"{name} <= {threshold:.2f}")
            print(f"{indent}å¦åˆ™ ({name} > {threshold:.2f}):")
            recurse(tree_.children_right[node], depth + 1, f"{name} > {threshold:.2f}")
        else:
            class_idx = np.argmax(tree_.value[node])
            class_name = ['å¥åº·', 'ç–¾ç—…'][class_idx]
            print(f"{indent}è¯Šæ–­: {class_name}")
    
    print("å†³ç­–è§„åˆ™:")
    recurse(0, 1)

get_rules(tree, ['ä½“æ¸©', 'ç™½ç»†èƒ', 'CRP'])
```

### 4.3 éšæœºæ£®æ— - ä¸“å®¶ä¼šè¯Š

#### ç”Ÿç‰©å­¦ç±»æ¯”
éšæœºæ£®æ—å°±åƒå¤šä¸ªåŒ»ç”Ÿä¼šè¯Šï¼šæ¯ä¸ªåŒ»ç”Ÿï¼ˆå†³ç­–æ ‘ï¼‰ç»™å‡ºè¯Šæ–­æ„è§ï¼Œæœ€ç»ˆæŠ•ç¥¨å†³å®šã€‚

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# ä½¿ç”¨æ›´å¤æ‚çš„æ•°æ®
X, y = make_classification(n_samples=500, n_features=20,
                          n_informative=15, n_redundant=5,
                          n_classes=3, random_state=42)

# æ¯”è¾ƒå•æ£µæ ‘å’Œéšæœºæ£®æ—
single_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# äº¤å‰éªŒè¯è¯„ä¼°
tree_scores = cross_val_score(single_tree, X, y, cv=5)
forest_scores = cross_val_score(random_forest, X, y, cv=5)

print("æ€§èƒ½æ¯”è¾ƒ:")
print(f"å•æ£µå†³ç­–æ ‘: {tree_scores.mean():.3f} Â± {tree_scores.std():.3f}")
print(f"éšæœºæ£®æ—: {forest_scores.mean():.3f} Â± {forest_scores.std():.3f}")

# è®­ç»ƒéšæœºæ£®æ—
random_forest.fit(X, y)

# ç‰¹å¾é‡è¦æ€§åˆ†æ
importances = random_forest.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("ç‰¹å¾é‡è¦æ€§ï¼ˆéšæœºæ£®æ—ï¼‰")
plt.bar(range(10), importances[indices[:10]])
plt.xticks(range(10), [f'ç‰¹å¾{i}' for i in indices[:10]], rotation=45)
plt.ylabel('é‡è¦æ€§åˆ†æ•°')
plt.tight_layout()
plt.show()

# åˆ†æå•æ£µæ ‘çš„å¤šæ ·æ€§
print("\néšæœºæ£®æ—ä¸­æ ‘çš„å¤šæ ·æ€§:")
trees = random_forest.estimators_
depths = [tree.get_depth() for tree in trees[:10]]
n_leaves = [tree.get_n_leaves() for tree in trees[:10]]

print(f"å‰10æ£µæ ‘çš„æ·±åº¦: {depths}")
print(f"å‰10æ£µæ ‘çš„å¶èŠ‚ç‚¹æ•°: {n_leaves}")
print(f"å¹³å‡æ·±åº¦: {np.mean(depths):.1f}")
print(f"æ·±åº¦æ ‡å‡†å·®: {np.std(depths):.1f}")
```

#### éšæœºæ£®æ—çš„ä¼˜åŠ¿

```python
# æ¼”ç¤ºéšæœºæ£®æ—çš„é²æ£’æ€§
def test_robustness():
    """æµ‹è¯•å¯¹å™ªå£°çš„é²æ£’æ€§"""
    np.random.seed(42)
    
    # åˆ›å»ºæ•°æ®
    X, y = make_classification(n_samples=200, n_features=10,
                              n_informative=5, n_redundant=5,
                              flip_y=0.1,  # 10%æ ‡ç­¾å™ªå£°
                              random_state=42)
    
    # ä¸åŒæ¨¡å‹
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42),
        'å•æ£µå†³ç­–æ ‘': DecisionTreeClassifier(random_state=42),
        'éšæœºæ£®æ—': RandomForestClassifier(n_estimators=100, random_state=42)
    }
    
    # è¯„ä¼°
    for name, model in models.items():
        scores = cross_val_score(model, X, y, cv=5)
        print(f"{name}: {scores.mean():.3f} Â± {scores.std():.3f}")

test_robustness()
```

### 4.4 æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰- å¯»æ‰¾æœ€ä¼˜è¾¹ç•Œ

#### ç”Ÿç‰©å­¦ç±»æ¯”
SVMå°±åƒåœ¨ä¸¤ç¾¤ç»†èƒä¹‹é—´ç”»ä¸€æ¡æœ€å®½çš„åˆ†ç•Œçº¿ï¼Œä½¿å¾—è¾¹ç•Œä¸¤ä¾§éƒ½æœ‰è¶³å¤Ÿçš„"å®‰å…¨è·ç¦»"ã€‚

```python
from sklearn.svm import SVC

# åˆ›å»ºéçº¿æ€§å¯åˆ†æ•°æ®
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.15, random_state=42)

# æ¯”è¾ƒä¸åŒæ ¸å‡½æ•°
kernels = ['linear', 'rbf', 'poly', 'sigmoid']
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for idx, kernel in enumerate(kernels):
    svm = SVC(kernel=kernel, random_state=42)
    svm.fit(X, y)
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    ax = axes[idx]
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolor='black')
    ax.set_title(f'SVM with {kernel} kernel')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

plt.tight_layout()
plt.show()

# æ”¯æŒå‘é‡çš„æ¦‚å¿µ
print(f"RBFæ ¸SVMçš„æ”¯æŒå‘é‡æ•°: {svm.n_support_}")
print(f"æ€»æ ·æœ¬æ•°: {len(X)}")
print(f"æ”¯æŒå‘é‡æ¯”ä¾‹: {sum(svm.n_support_) / len(X):.1%}")
```

### 4.5 ç®—æ³•é€‰æ‹©æŒ‡å—

```python
# ç®—æ³•é€‰æ‹©å†³ç­–æ ‘
algorithm_selection_guide = {
    "æ•°æ®ç‰¹ç‚¹": {
        "çº¿æ€§å¯åˆ†": ["é€»è¾‘å›å½’", "çº¿æ€§SVM"],
        "éçº¿æ€§å…³ç³»": ["å†³ç­–æ ‘", "éšæœºæ£®æ—", "RBF-SVM", "ç¥ç»ç½‘ç»œ"],
        "é«˜ç»´ç¨€ç–": ["é€»è¾‘å›å½’", "çº¿æ€§SVM", "æœ´ç´ è´å¶æ–¯"],
        "ç‰¹å¾ç›¸å…³æ€§å¼º": ["éšæœºæ£®æ—", "æ¢¯åº¦æå‡æ ‘"],
        "æ•°æ®é‡å°": ["SVM", "å†³ç­–æ ‘"],
        "æ•°æ®é‡å¤§": ["éšæœºæ£®æ—", "ç¥ç»ç½‘ç»œ", "æ¢¯åº¦æå‡"]
    },
    "éœ€æ±‚ç‰¹ç‚¹": {
        "éœ€è¦è§£é‡Šæ€§": ["é€»è¾‘å›å½’", "å†³ç­–æ ‘"],
        "éœ€è¦é«˜ç²¾åº¦": ["éšæœºæ£®æ—", "æ¢¯åº¦æå‡", "ç¥ç»ç½‘ç»œ"],
        "éœ€è¦æ¦‚ç‡è¾“å‡º": ["é€»è¾‘å›å½’", "éšæœºæ£®æ—", "æœ´ç´ è´å¶æ–¯"],
        "å®æ—¶é¢„æµ‹": ["é€»è¾‘å›å½’", "æœ´ç´ è´å¶æ–¯"],
        "é²æ£’æ€§è¦æ±‚é«˜": ["éšæœºæ£®æ—", "æ¢¯åº¦æå‡"]
    }
}

def recommend_algorithm(data_size, n_features, interpretability_needed, 
                        linear_problem, realtime_needed):
    """æ ¹æ®é—®é¢˜ç‰¹ç‚¹æ¨èç®—æ³•"""
    recommendations = []
    
    if data_size < 1000:
        recommendations.append("SVMï¼ˆå°æ•°æ®é›†è¡¨ç°å¥½ï¼‰")
    elif data_size > 10000:
        recommendations.append("éšæœºæ£®æ—ï¼ˆå¤§æ•°æ®é›†æ•ˆç‡é«˜ï¼‰")
    
    if n_features > 1000:
        recommendations.append("é€»è¾‘å›å½’ï¼ˆé«˜ç»´ç¨€ç–æ•°æ®ï¼‰")
    
    if interpretability_needed:
        recommendations.append("å†³ç­–æ ‘ï¼ˆå¯è§†åŒ–è§„åˆ™ï¼‰")
        recommendations.append("é€»è¾‘å›å½’ï¼ˆç³»æ•°å¯è§£é‡Šï¼‰")
    
    if linear_problem:
        recommendations.append("é€»è¾‘å›å½’ï¼ˆçº¿æ€§é—®é¢˜é¦–é€‰ï¼‰")
    else:
        recommendations.append("éšæœºæ£®æ—ï¼ˆå¤„ç†éçº¿æ€§ï¼‰")
    
    if realtime_needed:
        recommendations.append("æœ´ç´ è´å¶æ–¯ï¼ˆé¢„æµ‹é€Ÿåº¦å¿«ï¼‰")
    
    return list(set(recommendations))

# ä½¿ç”¨ç¤ºä¾‹
recs = recommend_algorithm(
    data_size=5000,
    n_features=100,
    interpretability_needed=True,
    linear_problem=False,
    realtime_needed=False
)
print("æ¨èçš„ç®—æ³•:")
for rec in recs:
    print(f"  - {rec}")
```

---

## ç¬¬5éƒ¨åˆ†ï¼šèšç±»åˆ†æ - å‘ç°éšè—çš„ç¾¤ä½“

### 5.1 K-meansèšç±» - å¯»æ‰¾ä¸­å¿ƒç‚¹

#### ç”Ÿç‰©å­¦ç±»æ¯”
K-meanså°±åƒæŠŠç»†èƒåˆ†ç»„ï¼šæ‰¾åˆ°æ¯ç»„çš„"å…¸å‹"ç»†èƒï¼ˆä¸­å¿ƒï¼‰ï¼Œå…¶ä»–ç»†èƒå½’åˆ°æœ€åƒçš„é‚£ç»„ã€‚

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# åˆ›å»ºæ¨¡æ‹Ÿçš„ç»†èƒæ•°æ®
np.random.seed(42)
X, true_labels = make_blobs(n_samples=300, n_features=2, 
                           centers=3, cluster_std=1.0)

# K-meansèšç±»
kmeans = KMeans(n_clusters=3, random_state=42)
predicted_labels = kmeans.fit_predict(X)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# åŸå§‹æ•°æ®
axes[0].scatter(X[:, 0], X[:, 1], alpha=0.6)
axes[0].set_title('åŸå§‹æ•°æ®ï¼ˆæœªæ ‡è®°ï¼‰')

# çœŸå®åˆ†ç»„
axes[1].scatter(X[:, 0], X[:, 1], c=true_labels, cmap='viridis', alpha=0.6)
axes[1].set_title('çœŸå®åˆ†ç»„ï¼ˆå¦‚æœçŸ¥é“çš„è¯ï¼‰')

# K-meansç»“æœ
axes[2].scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='viridis', alpha=0.6)
axes[2].scatter(kmeans.cluster_centers_[:, 0], 
               kmeans.cluster_centers_[:, 1],
               marker='*', s=300, c='red', edgecolor='black')
axes[2].set_title('K-meansèšç±»ç»“æœ')

plt.tight_layout()
plt.show()

# è¯„ä¼°èšç±»è´¨é‡
from sklearn.metrics import silhouette_score, adjusted_rand_score

silhouette = silhouette_score(X, predicted_labels)
ari = adjusted_rand_score(true_labels, predicted_labels)

print(f"è½®å»“ç³»æ•°: {silhouette:.3f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
print(f"è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari:.3f} (1è¡¨ç¤ºå®Œç¾åŒ¹é…)")
```

#### ç¡®å®šæœ€ä¼˜Kå€¼

```python
# è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°æ³•
def find_optimal_k(X, max_k=10):
    """ç¡®å®šæœ€ä¼˜çš„èšç±»æ•°"""
    inertias = []
    silhouettes = []
    K_range = range(2, max_k + 1)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X)
        
        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(X, labels))
    
    # ç»˜å›¾
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # è‚˜éƒ¨æ³•åˆ™
    axes[0].plot(K_range, inertias, 'bo-')
    axes[0].set_xlabel('èšç±»æ•° K')
    axes[0].set_ylabel('SSE (ç»„å†…å¹³æ–¹å’Œ)')
    axes[0].set_title('è‚˜éƒ¨æ³•åˆ™')
    axes[0].grid(True, alpha=0.3)
    
    # è½®å»“ç³»æ•°
    axes[1].plot(K_range, silhouettes, 'ro-')
    axes[1].set_xlabel('èšç±»æ•° K')
    axes[1].set_ylabel('è½®å»“ç³»æ•°')
    axes[1].set_title('è½®å»“ç³»æ•°æ³•')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰¾åˆ°æœ€ä¼˜K
    optimal_k = K_range[np.argmax(silhouettes)]
    print(f"åŸºäºè½®å»“ç³»æ•°ï¼Œæœ€ä¼˜Kå€¼: {optimal_k}")
    
    return optimal_k

optimal_k = find_optimal_k(X)
```

### 5.2 å±‚æ¬¡èšç±» - æ„å»ºå®¶æ—æ ‘

#### ç”Ÿç‰©å­¦ç±»æ¯”
å±‚æ¬¡èšç±»å°±åƒæ„å»ºç‰©ç§è¿›åŒ–æ ‘ï¼šä»ä¸ªä½“å¼€å§‹ï¼Œé€æ­¥åˆå¹¶ç›¸ä¼¼çš„ï¼Œæœ€ç»ˆå½¢æˆå®Œæ•´çš„è°±ç³»ã€‚

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼šä¸åŒåŸºå› çš„è¡¨è¾¾è°±
np.random.seed(42)
n_genes = 30
n_conditions = 10

# ä¸‰ç»„åŸºå› ï¼šæ—©æœŸå“åº”ã€ä¸­æœŸå“åº”ã€æ™šæœŸå“åº”
early_genes = np.random.randn(10, n_conditions) + np.array([3, 2, 1, 0, 0, 0, 0, 0, 0, 0])
mid_genes = np.random.randn(10, n_conditions) + np.array([0, 0, 0, 3, 2, 1, 0, 0, 0, 0])
late_genes = np.random.randn(10, n_conditions) + np.array([0, 0, 0, 0, 0, 0, 1, 2, 3, 2])

expression_data = np.vstack([early_genes, mid_genes, late_genes])
gene_names = [f'Gene_{i:02d}' for i in range(n_genes)]

# å±‚æ¬¡èšç±»
linkage_matrix = linkage(expression_data, method='ward')

# ç»˜åˆ¶æ ‘çŠ¶å›¾
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix, labels=gene_names, orientation='right')
plt.title('åŸºå› è¡¨è¾¾å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾')
plt.xlabel('è·ç¦»')
plt.ylabel('åŸºå› ')
plt.tight_layout()
plt.show()

# åˆ‡å‰²æ ‘è·å¾—èšç±»
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')
clusters = agg_clustering.fit_predict(expression_data)

print("èšç±»ç»“æœ:")
for cluster_id in range(3):
    genes_in_cluster = [gene_names[i] for i, c in enumerate(clusters) if c == cluster_id]
    print(f"  èšç±»{cluster_id}: {', '.join(genes_in_cluster[:5])}...")
```

### 5.3 DBSCAN - åŸºäºå¯†åº¦çš„èšç±»

#### ç”Ÿç‰©å­¦ç±»æ¯”
DBSCANå°±åƒåœ¨æ˜¾å¾®é•œä¸‹è¯†åˆ«ç»†èƒå›¢ï¼šå¯†é›†çš„åŒºåŸŸæ˜¯ä¸€ä¸ªç¾¤ä½“ï¼Œç¨€ç–çš„ç‚¹æ˜¯å¼‚å¸¸å€¼ã€‚

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# åˆ›å»ºæœˆç‰™å½¢æ•°æ®ï¼ˆK-meansä¼šå¤±è´¥ï¼‰
X, y = make_moons(n_samples=200, noise=0.1, random_state=42)

# æ·»åŠ ä¸€äº›å™ªå£°ç‚¹
noise = np.random.uniform(-2, 2, (20, 2))
X_with_noise = np.vstack([X, noise])

# æ¯”è¾ƒK-meanså’ŒDBSCAN
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# åŸå§‹æ•°æ®
axes[0].scatter(X_with_noise[:, 0], X_with_noise[:, 1], alpha=0.6)
axes[0].set_title('åŸå§‹æ•°æ®ï¼ˆå«å™ªå£°ï¼‰')

# K-meansï¼ˆå¤±è´¥ï¼‰
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X_with_noise)
axes[1].scatter(X_with_noise[:, 0], X_with_noise[:, 1], 
               c=kmeans_labels, cmap='viridis', alpha=0.6)
axes[1].set_title('K-meansï¼ˆå½¢çŠ¶è¯†åˆ«å¤±è´¥ï¼‰')

# DBSCANï¼ˆæˆåŠŸï¼‰
dbscan = DBSCAN(eps=0.3, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_with_noise)
axes[2].scatter(X_with_noise[:, 0], X_with_noise[:, 1], 
               c=dbscan_labels, cmap='viridis', alpha=0.6)
axes[2].set_title('DBSCANï¼ˆæ­£ç¡®è¯†åˆ«+å™ªå£°æ£€æµ‹ï¼‰')

plt.tight_layout()
plt.show()

# åˆ†æç»“æœ
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)

print(f"DBSCANå‘ç°çš„èšç±»æ•°: {n_clusters}")
print(f"å™ªå£°ç‚¹æ•°é‡: {n_noise}")
print(f"å™ªå£°ç‚¹æ¯”ä¾‹: {n_noise/len(X_with_noise):.1%}")
```

### 5.4 èšç±»ç®—æ³•æ¯”è¾ƒä¸é€‰æ‹©

```python
# ä¸åŒèšç±»ç®—æ³•çš„ç‰¹ç‚¹æ¯”è¾ƒ
clustering_comparison = {
    "K-means": {
        "å‡è®¾": "çƒå½¢èšç±»ï¼Œå¤§å°ç›¸ä¼¼",
        "ä¼˜ç‚¹": "å¿«é€Ÿï¼Œå¯æ‰©å±•",
        "ç¼ºç‚¹": "éœ€è¦æŒ‡å®šKï¼Œå¯¹å½¢çŠ¶æ•æ„Ÿ",
        "é€‚ç”¨": "çƒå½¢åˆ†å¸ƒï¼Œå¤§æ•°æ®é›†"
    },
    "å±‚æ¬¡èšç±»": {
        "å‡è®¾": "æ— ç‰¹å®šå‡è®¾",
        "ä¼˜ç‚¹": "æ— éœ€æŒ‡å®šèšç±»æ•°ï¼Œå¯è§†åŒ–å¥½",
        "ç¼ºç‚¹": "è®¡ç®—å¤æ‚åº¦é«˜ O(nÂ³)",
        "é€‚ç”¨": "å°æ•°æ®é›†ï¼Œéœ€è¦æ ‘çŠ¶ç»“æ„"
    },
    "DBSCAN": {
        "å‡è®¾": "åŸºäºå¯†åº¦",
        "ä¼˜ç‚¹": "å‘ç°ä»»æ„å½¢çŠ¶ï¼Œæ£€æµ‹å¼‚å¸¸",
        "ç¼ºç‚¹": "å‚æ•°æ•æ„Ÿï¼Œå¯†åº¦ä¸å‡å›°éš¾",
        "é€‚ç”¨": "å¼‚å¸¸æ£€æµ‹ï¼Œéçƒå½¢èšç±»"
    },
    "é«˜æ–¯æ··åˆæ¨¡å‹": {
        "å‡è®¾": "æ•°æ®ç”±é«˜æ–¯åˆ†å¸ƒæ··åˆ",
        "ä¼˜ç‚¹": "è½¯èšç±»ï¼ˆæ¦‚ç‡ï¼‰ï¼Œç†è®ºå®Œå–„",
        "ç¼ºç‚¹": "å¯¹åˆå§‹åŒ–æ•æ„Ÿ",
        "é€‚ç”¨": "éœ€è¦æ¦‚ç‡è¾“å‡º"
    }
}

# åˆ›å»ºç»¼åˆæµ‹è¯•æ•°æ®é›†
def create_clustering_challenges():
    """åˆ›å»ºä¸åŒæŒ‘æˆ˜æ€§çš„èšç±»æ•°æ®"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    datasets = []
    
    # 1. çƒå½¢èšç±»ï¼ˆK-meanså‹å¥½ï¼‰
    X1, y1 = make_blobs(n_samples=200, centers=3, random_state=42)
    datasets.append(('çƒå½¢èšç±»', X1, y1))
    
    # 2. æœˆç‰™å½¢ï¼ˆDBSCANå‹å¥½ï¼‰
    X2, y2 = make_moons(n_samples=200, noise=0.05, random_state=42)
    datasets.append(('æœˆç‰™å½¢', X2, y2))
    
    # 3. ä¸åŒå¯†åº¦
    X3_1, _ = make_blobs(n_samples=100, centers=[[0, 0]], cluster_std=0.2)
    X3_2, _ = make_blobs(n_samples=100, centers=[[3, 3]], cluster_std=1.0)
    X3 = np.vstack([X3_1, X3_2])
    y3 = np.array([0]*100 + [1]*100)
    datasets.append(('ä¸åŒå¯†åº¦', X3, y3))
    
    # 4. ä¸åŒå¤§å°
    X4_1, _ = make_blobs(n_samples=50, centers=[[0, 0]], cluster_std=0.5)
    X4_2, _ = make_blobs(n_samples=200, centers=[[3, 3]], cluster_std=0.5)
    X4 = np.vstack([X4_1, X4_2])
    y4 = np.array([0]*50 + [1]*200)
    datasets.append(('ä¸åŒå¤§å°', X4, y4))
    
    # 5. å™ªå£°æ•°æ®
    X5, y5 = make_blobs(n_samples=150, centers=2, random_state=42)
    noise = np.random.uniform(-4, 4, (50, 2))
    X5 = np.vstack([X5, noise])
    y5 = np.append(y5, [-1]*50)
    datasets.append(('å«å™ªå£°', X5, y5))
    
    # 6. é«˜ç»´æ•°æ®æŠ•å½±
    from sklearn.datasets import make_swiss_roll
    X6, color = make_swiss_roll(n_samples=200, random_state=42)
    X6 = X6[:, [0, 2]]  # ä½¿ç”¨2DæŠ•å½±
    datasets.append(('ç‘å£«å·', X6, color))
    
    # å¯è§†åŒ–æ‰€æœ‰æ•°æ®é›†
    for idx, (name, X, y) in enumerate(datasets):
        ax = axes[idx // 3, idx % 3]
        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.6)
        ax.set_title(name)
        ax.set_xticks([])
        ax.set_yticks([])
    
    plt.suptitle('ä¸åŒç±»å‹çš„èšç±»æŒ‘æˆ˜', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    return datasets

datasets = create_clustering_challenges()
```

---

## ç¬¬6éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ– - ç¡®ä¿è¯Šæ–­å‡†ç¡®

### 6.1 è¯„ä¼°æŒ‡æ ‡è¯¦è§£

#### æ··æ·†çŸ©é˜µ - è¯Šæ–­ç»“æœçš„å…¨è²Œ

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# åˆ›å»ºç™Œç—‡è¯Šæ–­ç¤ºä¾‹
np.random.seed(42)
n_samples = 1000

# æ¨¡æ‹Ÿé¢„æµ‹ç»“æœï¼ˆä¸å®Œç¾çš„æ¨¡å‹ï¼‰
y_true = np.random.randint(0, 2, n_samples)  # çœŸå®ï¼š0=å¥åº·ï¼Œ1=ç™Œç—‡
y_pred = y_true.copy()

# æ·»åŠ ä¸€äº›é”™è¯¯
false_positives = np.random.choice(np.where(y_true == 0)[0], 50)
y_pred[false_positives] = 1  # å‡é˜³æ€§

false_negatives = np.random.choice(np.where(y_true == 1)[0], 30)
y_pred[false_negatives] = 0  # å‡é˜´æ€§

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['é¢„æµ‹:å¥åº·', 'é¢„æµ‹:ç™Œç—‡'],
            yticklabels=['å®é™…:å¥åº·', 'å®é™…:ç™Œç—‡'])
plt.title('ç™Œç—‡è¯Šæ–­æ··æ·†çŸ©é˜µ')
plt.ylabel('å®é™…æƒ…å†µ')
plt.xlabel('é¢„æµ‹ç»“æœ')
plt.show()

# è¯¦ç»†æŒ‡æ ‡
print("åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_true, y_pred, 
                          target_names=['å¥åº·', 'ç™Œç—‡']))

# è§£é‡Šå„æŒ‡æ ‡çš„åŒ»å­¦æ„ä¹‰
medical_metrics = {
    "çµæ•åº¦(å¬å›ç‡)": "æ‚£ç—…è€…ä¸­è¢«æ­£ç¡®è¯Šæ–­çš„æ¯”ä¾‹",
    "ç‰¹å¼‚åº¦": "å¥åº·è€…ä¸­è¢«æ­£ç¡®åˆ¤å®šå¥åº·çš„æ¯”ä¾‹",
    "ç²¾ç¡®åº¦": "è¯Šæ–­ä¸ºç–¾ç—…ä¸­çœŸæ­£æ‚£ç—…çš„æ¯”ä¾‹",
    "é˜´æ€§é¢„æµ‹å€¼": "è¯Šæ–­ä¸ºå¥åº·ä¸­çœŸæ­£å¥åº·çš„æ¯”ä¾‹",
    "å‡†ç¡®ç‡": "æ‰€æœ‰è¯Šæ–­æ­£ç¡®çš„æ¯”ä¾‹"
}

from sklearn.metrics import recall_score, precision_score, accuracy_score

sensitivity = recall_score(y_true, y_pred)  # çµæ•åº¦
precision = precision_score(y_true, y_pred)  # ç²¾ç¡®åº¦
accuracy = accuracy_score(y_true, y_pred)  # å‡†ç¡®ç‡
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # ç‰¹å¼‚åº¦

print("\nåŒ»å­¦è¯Šæ–­æŒ‡æ ‡:")
print(f"çµæ•åº¦ï¼ˆå‘ç°ç–¾ç—…èƒ½åŠ›ï¼‰: {sensitivity:.1%}")
print(f"ç‰¹å¼‚åº¦ï¼ˆæ’é™¤ç–¾ç—…èƒ½åŠ›ï¼‰: {specificity:.1%}")
print(f"ç²¾ç¡®åº¦ï¼ˆè¯Šæ–­å¯ä¿¡åº¦ï¼‰: {precision:.1%}")
print(f"å‡†ç¡®ç‡ï¼ˆæ€»ä½“æ­£ç¡®ç‡ï¼‰: {accuracy:.1%}")
```

#### ROCæ›²çº¿å’ŒAUC - è¯Šæ–­èƒ½åŠ›è¯„ä¼°

```python
from sklearn.metrics import roc_curve, auc, roc_auc_score

# åˆ›å»ºæœ‰æ¦‚ç‡è¾“å‡ºçš„é¢„æµ‹
np.random.seed(42)
n_samples = 500

# ç”ŸæˆçœŸå®æ ‡ç­¾
y_true = np.random.randint(0, 2, n_samples)

# ç”Ÿæˆæ¦‚ç‡é¢„æµ‹ï¼ˆæ¨¡æ‹Ÿä¸åŒè´¨é‡çš„æ¨¡å‹ï¼‰
# å¥½æ¨¡å‹
good_proba = np.zeros(n_samples)
good_proba[y_true == 1] = np.random.beta(8, 2, sum(y_true == 1))
good_proba[y_true == 0] = np.random.beta(2, 8, sum(y_true == 0))

# ä¸­ç­‰æ¨¡å‹
medium_proba = np.zeros(n_samples)
medium_proba[y_true == 1] = np.random.beta(6, 4, sum(y_true == 1))
medium_proba[y_true == 0] = np.random.beta(4, 6, sum(y_true == 0))

# å·®æ¨¡å‹ï¼ˆæ¥è¿‘éšæœºï¼‰
bad_proba = np.random.uniform(0, 1, n_samples)

# è®¡ç®—ROCæ›²çº¿
models = {
    'ä¼˜ç§€æ¨¡å‹': good_proba,
    'ä¸­ç­‰æ¨¡å‹': medium_proba,
    'è¾ƒå·®æ¨¡å‹': bad_proba
}

plt.figure(figsize=(10, 8))

for name, proba in models.items():
    fpr, tpr, thresholds = roc_curve(y_true, proba)
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')

# éšæœºçŒœæµ‹çº¿
plt.plot([0, 1], [0, 1], 'k--', label='éšæœºçŒœæµ‹ (AUC = 0.5)')

plt.xlabel('å‡é˜³æ€§ç‡ (1 - ç‰¹å¼‚åº¦)')
plt.ylabel('çœŸé˜³æ€§ç‡ (çµæ•åº¦)')
plt.title('ROCæ›²çº¿æ¯”è¾ƒ')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.show()

# è§£é‡ŠAUCçš„æ„ä¹‰
print("AUCå€¼çš„è§£é‡Š:")
print("0.90-1.00: ä¼˜ç§€çš„è¯Šæ–­èƒ½åŠ›")
print("0.80-0.90: è‰¯å¥½çš„è¯Šæ–­èƒ½åŠ›")
print("0.70-0.80: ä¸€èˆ¬çš„è¯Šæ–­èƒ½åŠ›")
print("0.60-0.70: è¾ƒå·®çš„è¯Šæ–­èƒ½åŠ›")
print("0.50-0.60: æ— è¯Šæ–­ä»·å€¼")
```

### 6.2 äº¤å‰éªŒè¯ - ç¡®ä¿ç»“æœå¯é 

#### ç”Ÿç‰©å­¦ç±»æ¯”
äº¤å‰éªŒè¯å°±åƒå¤šä¸­å¿ƒä¸´åºŠè¯•éªŒï¼šä¸èƒ½åªåœ¨ä¸€å®¶åŒ»é™¢æµ‹è¯•ï¼Œè¦åœ¨å¤šä¸ªä¸åŒçš„åŒ»é™¢éªŒè¯æ•ˆæœã€‚

```python
from sklearn.model_selection import KFold, StratifiedKFold, cross_validate
from sklearn.datasets import load_breast_cancer

# åŠ è½½ä¹³è…ºç™Œæ•°æ®
data = load_breast_cancer()
X, y = data.data, data.target

# ä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥
cv_strategies = {
    'KæŠ˜äº¤å‰éªŒè¯': KFold(n_splits=5, shuffle=True, random_state=42),
    'åˆ†å±‚KæŠ˜': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
}

# æµ‹è¯•æ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)

print("äº¤å‰éªŒè¯æ¯”è¾ƒ:")
for name, cv in cv_strategies.items():
    # è¯¦ç»†çš„äº¤å‰éªŒè¯
    cv_results = cross_validate(model, X, y, cv=cv,
                               scoring=['accuracy', 'precision', 'recall', 'f1'],
                               return_train_score=True)
    
    print(f"\n{name}:")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {cv_results['train_accuracy'].mean():.3f} Â± {cv_results['train_accuracy'].std():.3f}")
    print(f"  éªŒè¯å‡†ç¡®ç‡: {cv_results['test_accuracy'].mean():.3f} Â± {cv_results['test_accuracy'].std():.3f}")
    print(f"  éªŒè¯ç²¾ç¡®ç‡: {cv_results['test_precision'].mean():.3f} Â± {cv_results['test_precision'].std():.3f}")
    print(f"  éªŒè¯å¬å›ç‡: {cv_results['test_recall'].mean():.3f} Â± {cv_results['test_recall'].std():.3f}")
    print(f"  éªŒè¯F1åˆ†æ•°: {cv_results['test_f1'].mean():.3f} Â± {cv_results['test_f1'].std():.3f}")
    
    # æ£€æŸ¥è¿‡æ‹Ÿåˆ
    overfit = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()
    if overfit > 0.05:
        print(f"  âš ï¸ å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (å·®å¼‚: {overfit:.3f})")

# å¯è§†åŒ–æ¯æŠ˜çš„ç»“æœ
def plot_cv_results(cv_results):
    """å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    folds = range(1, len(cv_results['train_accuracy']) + 1)
    axes[0].plot(folds, cv_results['train_accuracy'], 'o-', label='è®­ç»ƒé›†')
    axes[0].plot(folds, cv_results['test_accuracy'], 's-', label='éªŒè¯é›†')
    axes[0].set_xlabel('æŠ˜æ•°')
    axes[0].set_ylabel('å‡†ç¡®ç‡')
    axes[0].set_title('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å„æŒ‡æ ‡ç®±çº¿å›¾
    metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']
    metric_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
    data_to_plot = [cv_results[m] for m in metrics]
    
    axes[1].boxplot(data_to_plot, labels=metric_names)
    axes[1].set_ylabel('åˆ†æ•°')
    axes[1].set_title('å„æŒ‡æ ‡åˆ†å¸ƒ')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

plot_cv_results(cv_results)
```

### 6.3 è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–

```python
# æ¼”ç¤ºè¿‡æ‹Ÿåˆå’Œæ­£åˆ™åŒ–æ•ˆæœ
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# åˆ›å»ºéçº¿æ€§æ•°æ®
np.random.seed(42)
n_samples = 100
X = np.sort(np.random.rand(n_samples, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, n_samples)

# åˆ†å‰²æ•°æ®
X_train, X_test = X[:80], X[80:]
y_train, y_test = y[:80], y[80:]

# ä¸åŒç¨‹åº¦çš„å¤šé¡¹å¼æ‹Ÿåˆ
degrees = [1, 5, 15]
regularizations = [None, 'Ridge', 'Lasso']

fig, axes = plt.subplots(len(degrees), len(regularizations), 
                        figsize=(15, 12))

for i, degree in enumerate(degrees):
    for j, reg in enumerate(regularizations):
        ax = axes[i, j]
        
        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        poly = PolynomialFeatures(degree=degree)
        X_train_poly = poly.fit_transform(X_train)
        X_test_poly = poly.transform(X_test)
        
        # é€‰æ‹©æ¨¡å‹
        if reg is None:
            model = LinearRegression()
            title = f'Degree {degree}, No Regularization'
        elif reg == 'Ridge':
            model = Ridge(alpha=0.1)
            title = f'Degree {degree}, Ridge (L2)'
        else:
            model = Lasso(alpha=0.01, max_iter=10000)
            title = f'Degree {degree}, Lasso (L1)'
        
        # è®­ç»ƒ
        model.fit(X_train_poly, y_train)
        
        # é¢„æµ‹
        X_plot = np.linspace(0, 10, 300)[:, np.newaxis]
        X_plot_poly = poly.transform(X_plot)
        y_plot = model.predict(X_plot_poly)
        
        # è®¡ç®—è¯¯å·®
        train_score = model.score(X_train_poly, y_train)
        test_score = model.score(X_test_poly, y_test)
        
        # ç»˜å›¾
        ax.scatter(X_train, y_train, alpha=0.5, label='è®­ç»ƒæ•°æ®')
        ax.scatter(X_test, y_test, alpha=0.5, color='red', label='æµ‹è¯•æ•°æ®')
        ax.plot(X_plot, y_plot, color='green', linewidth=2)
        ax.set_title(f'{title}\nTrain: {train_score:.3f}, Test: {test_score:.3f}')
        ax.set_ylim([-2, 2])
        ax.legend(fontsize=8)
        ax.grid(True, alpha=0.3)

plt.suptitle('æ­£åˆ™åŒ–å¯¹è¿‡æ‹Ÿåˆçš„å½±å“', fontsize=16)
plt.tight_layout()
plt.show()
```

### 6.4 è¶…å‚æ•°è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# ä½¿ç”¨ä¹³è…ºç™Œæ•°æ®
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# éšæœºæ£®æ—çš„è¶…å‚æ•°ç©ºé—´
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# è®¡ç®—å‚æ•°ç»„åˆæ•°
n_combinations = np.prod([len(v) for v in param_grid.values()])
print(f"å‚æ•°ç»„åˆæ€»æ•°: {n_combinations}")

# ç½‘æ ¼æœç´¢
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, 
                          scoring='f1', n_jobs=-1, verbose=1)

print("\næ‰§è¡Œç½‘æ ¼æœç´¢...")
grid_search.fit(X_train, y_train)

print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.3f}")

# æµ‹è¯•é›†è¯„ä¼°
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.3f}")

# å¯è§†åŒ–å‚æ•°å½±å“
results = pd.DataFrame(grid_search.cv_results_)

# é€‰æ‹©ä¸¤ä¸ªæœ€é‡è¦çš„å‚æ•°
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# n_estimatorsçš„å½±å“
param = 'param_n_estimators'
param_values = results[param].unique()
mean_scores = []
std_scores = []
for val in param_values:
    mask = results[param] == val
    mean_scores.append(results[mask]['mean_test_score'].mean())
    std_scores.append(results[mask]['std_test_score'].mean())

axes[0].errorbar(param_values, mean_scores, yerr=std_scores, marker='o')
axes[0].set_xlabel('n_estimators')
axes[0].set_ylabel('F1 Score')
axes[0].set_title('æ ‘çš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“')
axes[0].grid(True, alpha=0.3)

# max_depthçš„å½±å“
param = 'param_max_depth'
param_values = [v for v in results[param].unique() if v is not None]
mean_scores = []
for val in param_values:
    mask = results[param] == val
    mean_scores.append(results[mask]['mean_test_score'].mean())

axes[1].plot(param_values, mean_scores, 'o-')
axes[1].set_xlabel('max_depth')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('æ ‘çš„æ·±åº¦å¯¹æ€§èƒ½çš„å½±å“')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## ç¬¬7éƒ¨åˆ†ï¼šç»¼åˆé¡¹ç›® - ç™Œç—‡åˆ†å‹é¢„æµ‹

### 7.1 é¡¹ç›®èƒŒæ™¯

```python
print("""
ğŸ¥ é¡¹ç›®ï¼šåŸºäºåŸºå› è¡¨è¾¾çš„ç™Œç—‡åˆ†å‹é¢„æµ‹

èƒŒæ™¯ï¼š
ç™Œç—‡æ˜¯ä¸€ç§é«˜åº¦å¼‚è´¨æ€§ç–¾ç—…ï¼Œå³ä½¿æ˜¯åŒä¸€ç§ç™Œç—‡ï¼Œä¸åŒæ‚£è€…çš„åˆ†å­ç‰¹å¾ä¹Ÿå¯èƒ½å®Œå…¨ä¸åŒã€‚
ç²¾å‡†åŒ»ç–—è¦æ±‚æˆ‘ä»¬èƒ½å¤Ÿï¼š
1. å‡†ç¡®è¯†åˆ«ç™Œç—‡äºšå‹
2. é¢„æµ‹æ²»ç–—ååº”
3. è¯„ä¼°é¢„åé£é™©

æˆ‘ä»¬çš„ä»»åŠ¡ï¼š
ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ†æåŸºå› è¡¨è¾¾æ•°æ®ï¼Œå®ç°ç™Œç—‡çš„è‡ªåŠ¨åˆ†å‹ã€‚
""")
```

### 7.2 å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹

```python
# åˆ›å»ºæ¨¡æ‹Ÿçš„ç™Œç—‡åŸºå› è¡¨è¾¾æ•°æ®
def create_cancer_dataset():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ç™Œç—‡æ•°æ®é›†"""
    np.random.seed(42)
    
    n_samples = 500
    n_genes = 100
    
    # å››ç§ç™Œç—‡äºšå‹çš„åŸºå› è¡¨è¾¾æ¨¡å¼
    subtype_patterns = {
        'Luminal A': {
            'samples': 150,
            'signature_genes': list(range(0, 20)),  # å‰20ä¸ªåŸºå› é«˜è¡¨è¾¾
            'expression_level': 3.0
        },
        'Luminal B': {
            'samples': 120,
            'signature_genes': list(range(20, 40)),
            'expression_level': 2.5
        },
        'HER2+': {
            'samples': 130,
            'signature_genes': list(range(40, 60)),
            'expression_level': 3.5
        },
        'Triple Negative': {
            'samples': 100,
            'signature_genes': list(range(60, 80)),
            'expression_level': 2.0
        }
    }
    
    expression_data = []
    labels = []
    subtype_names = []
    
    for subtype_id, (subtype, params) in enumerate(subtype_patterns.items()):
        # åŸºç¡€è¡¨è¾¾æ°´å¹³
        base_expression = np.random.lognormal(1, 0.5, (params['samples'], n_genes))
        
        # æ·»åŠ äºšå‹ç‰¹å¼‚æ€§è¡¨è¾¾
        for gene_idx in params['signature_genes']:
            base_expression[:, gene_idx] *= params['expression_level']
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.1, base_expression.shape)
        base_expression += noise
        
        expression_data.append(base_expression)
        labels.extend([subtype_id] * params['samples'])
        subtype_names.extend([subtype] * params['samples'])
    
    # åˆå¹¶æ•°æ®
    X = np.vstack(expression_data)
    y = np.array(labels)
    
    # åˆ›å»ºDataFrame
    gene_names = [f'Gene_{i:03d}' for i in range(n_genes)]
    df = pd.DataFrame(X, columns=gene_names)
    df['subtype'] = subtype_names
    df['subtype_id'] = y
    
    return df, gene_names

# åˆ›å»ºæ•°æ®
cancer_df, gene_names = create_cancer_dataset()
print(f"æ•°æ®é›†å¤§å°: {cancer_df.shape}")
print(f"ç™Œç—‡äºšå‹åˆ†å¸ƒ:\n{cancer_df['subtype'].value_counts()}")
```

### 7.3 æ¢ç´¢æ€§æ•°æ®åˆ†æ

```python
# æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ–
def explore_cancer_data(df, gene_names):
    """æ¢ç´¢ç™Œç—‡æ•°æ®"""
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. äºšå‹åˆ†å¸ƒ
    ax = axes[0, 0]
    df['subtype'].value_counts().plot(kind='bar', ax=ax)
    ax.set_title('ç™Œç—‡äºšå‹åˆ†å¸ƒ')
    ax.set_xlabel('äºšå‹')
    ax.set_ylabel('æ ·æœ¬æ•°')
    
    # 2. åŸºå› è¡¨è¾¾åˆ†å¸ƒ
    ax = axes[0, 1]
    sample_genes = np.random.choice(gene_names, 5)
    for gene in sample_genes:
        ax.hist(df[gene], alpha=0.5, label=gene, bins=30)
    ax.set_title('åŸºå› è¡¨è¾¾åˆ†å¸ƒç¤ºä¾‹')
    ax.set_xlabel('è¡¨è¾¾æ°´å¹³')
    ax.set_ylabel('é¢‘ç‡')
    ax.legend()
    
    # 3. äºšå‹é—´è¡¨è¾¾å·®å¼‚
    ax = axes[0, 2]
    gene_to_plot = gene_names[0]  # ç¬¬ä¸€ä¸ªåŸºå› 
    for subtype in df['subtype'].unique():
        data = df[df['subtype'] == subtype][gene_to_plot]
        ax.hist(data, alpha=0.5, label=subtype, bins=20)
    ax.set_title(f'{gene_to_plot}åœ¨ä¸åŒäºšå‹ä¸­çš„è¡¨è¾¾')
    ax.set_xlabel('è¡¨è¾¾æ°´å¹³')
    ax.set_ylabel('é¢‘ç‡')
    ax.legend()
    
    # 4. ç›¸å…³æ€§çƒ­å›¾ï¼ˆå‰20ä¸ªåŸºå› ï¼‰
    ax = axes[1, 0]
    corr_matrix = df[gene_names[:20]].corr()
    im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)
    ax.set_title('åŸºå› ç›¸å…³æ€§çƒ­å›¾')
    plt.colorbar(im, ax=ax)
    
    # 5. PCAå¯è§†åŒ–
    ax = axes[1, 1]
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(df[gene_names])
    
    for subtype in df['subtype'].unique():
        mask = df['subtype'] == subtype
        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                  label=subtype, alpha=0.6)
    ax.set_title('PCAå¯è§†åŒ–')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    ax.legend()
    
    # 6. t-SNEå¯è§†åŒ–
    ax = axes[1, 2]
    from sklearn.manifold import TSNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(df[gene_names].iloc[:300])  # ä½¿ç”¨éƒ¨åˆ†æ•°æ®åŠ é€Ÿ
    
    for subtype in df['subtype'].unique()[:300]:
        mask = df['subtype'].iloc[:300] == subtype
        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], 
                  label=subtype, alpha=0.6)
    ax.set_title('t-SNEå¯è§†åŒ–')
    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2')
    ax.legend()
    
    plt.tight_layout()
    plt.show()

explore_cancer_data(cancer_df, gene_names)
```

### 7.4 ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©

```python
def feature_engineering_pipeline(df, gene_names):
    """å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹"""
    
    print("=" * 60)
    print("ç‰¹å¾å·¥ç¨‹æµç¨‹")
    print("=" * 60)
    
    # 1. åŸå§‹ç‰¹å¾
    X_raw = df[gene_names].values
    print(f"1. åŸå§‹ç‰¹å¾: {X_raw.shape}")
    
    # 2. å¯¹æ•°è½¬æ¢
    X_log = np.log1p(X_raw)
    print(f"2. å¯¹æ•°è½¬æ¢å: {X_log.shape}")
    
    # 3. æ ‡å‡†åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_log)
    print(f"3. æ ‡å‡†åŒ–å: {X_scaled.shape}")
    
    # 4. ç‰¹å¾é€‰æ‹© - æ–¹å·®é˜ˆå€¼
    from sklearn.feature_selection import VarianceThreshold
    selector_var = VarianceThreshold(threshold=0.1)
    X_var = selector_var.fit_transform(X_scaled)
    print(f"4. æ–¹å·®ç­›é€‰å: {X_var.shape}")
    
    # 5. ç‰¹å¾é€‰æ‹© - å•å˜é‡ç»Ÿè®¡
    from sklearn.feature_selection import SelectKBest, f_classif
    selector_kbest = SelectKBest(score_func=f_classif, k=50)
    X_selected = selector_kbest.fit_transform(X_var, df['subtype_id'])
    print(f"5. ç»Ÿè®¡ç­›é€‰å: {X_selected.shape}")
    
    # 6. PCAé™ç»´ï¼ˆå¯é€‰ï¼‰
    from sklearn.decomposition import PCA
    pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
    X_pca = pca.fit_transform(X_selected)
    print(f"6. PCAé™ç»´å: {X_pca.shape}")
    
    # è¿”å›å¤„ç†åçš„ç‰¹å¾å’Œå¤„ç†å™¨
    return X_selected, {
        'scaler': scaler,
        'var_selector': selector_var,
        'kbest_selector': selector_kbest
    }

X_processed, processors = feature_engineering_pipeline(cancer_df, gene_names)
y = cancer_df['subtype_id'].values
```

### 7.5 æ¨¡å‹è®­ç»ƒå’Œæ¯”è¾ƒ

```python
def train_and_compare_models(X, y, subtype_names):
    """è®­ç»ƒå’Œæ¯”è¾ƒå¤šä¸ªæ¨¡å‹"""
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print("\n" + "=" * 60)
    print("æ¨¡å‹è®­ç»ƒå’Œæ¯”è¾ƒ")
    print("=" * 60)
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(probability=True, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), 
                                       max_iter=1000, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒ
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # è¯„ä¼°
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        from sklearn.model_selection import cross_val_score
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_pred': y_pred,
            'y_proba': y_proba
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  F1åˆ†æ•°: {f1:.3f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
    return results, X_test, y_test

results, X_test, y_test = train_and_compare_models(X_processed, y, 
                                                   cancer_df['subtype'].unique())
```

### 7.6 æœ€ä½³æ¨¡å‹åˆ†æ

```python
def analyze_best_model(results, X_test, y_test, cancer_df):
    """è¯¦ç»†åˆ†ææœ€ä½³æ¨¡å‹"""
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), 
                         key=lambda k: results[k]['f1'])
    best_result = results[best_model_name]
    
    print("\n" + "=" * 60)
    print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
    print("=" * 60)
    
    # 1. æ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, best_result['y_pred'])
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=cancer_df['subtype'].unique(),
                yticklabels=cancer_df['subtype'].unique())
    plt.title(f'{best_model_name} - æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®äºšå‹')
    plt.xlabel('é¢„æµ‹äºšå‹')
    plt.show()
    
    # 2. åˆ†ç±»æŠ¥å‘Š
    from sklearn.metrics import classification_report
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, best_result['y_pred'],
                              target_names=cancer_df['subtype'].unique()))
    
    # 3. ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    if hasattr(best_result['model'], 'feature_importances_'):
        importances = best_result['model'].feature_importances_
        indices = np.argsort(importances)[::-1][:20]
        
        plt.figure(figsize=(10, 6))
        plt.bar(range(20), importances[indices])
        plt.title('Top 20 ç‰¹å¾é‡è¦æ€§')
        plt.xlabel('ç‰¹å¾ç´¢å¼•')
        plt.ylabel('é‡è¦æ€§')
        plt.tight_layout()
        plt.show()
    
    # 4. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
    if best_result['y_proba'] is not None:
        plt.figure(figsize=(12, 4))
        
        for i in range(4):  # 4ä¸ªäºšå‹
            plt.subplot(1, 4, i+1)
            proba = best_result['y_proba'][:, i]
            plt.hist(proba, bins=20, alpha=0.7)
            plt.title(f'{cancer_df["subtype"].unique()[i]}')
            plt.xlabel('é¢„æµ‹æ¦‚ç‡')
            plt.ylabel('é¢‘ç‡')
        
        plt.suptitle('å„äºšå‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
        plt.tight_layout()
        plt.show()
    
    return best_model_name, best_result

best_name, best_model = analyze_best_model(results, X_test, y_test, cancer_df)
```

---

## ç¬¬8éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ å±•æœ› - AIé©å‘½çš„å¼€å§‹

### 8.1 æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©å­¦ä¸­çš„çªç ´

```python
print("""
ğŸ§  æ·±åº¦å­¦ä¹ ï¼šç”Ÿç‰©ä¿¡æ¯å­¦çš„æ–°çºªå…ƒ

æ·±åº¦å­¦ä¹ ä¸ä»…ä»…æ˜¯ä¸€ç§ç®—æ³•ï¼Œå®ƒæ­£åœ¨æ”¹å˜æˆ‘ä»¬ç†è§£ç”Ÿå‘½çš„æ–¹å¼ï¼š

1. AlphaFold2 - ç ´è§£50å¹´éš¾é¢˜
   â€¢ è¾“å…¥ï¼šæ°¨åŸºé…¸åºåˆ—
   â€¢ è¾“å‡ºï¼š3Dè›‹ç™½è´¨ç»“æ„
   â€¢ å‡†ç¡®åº¦ï¼šåŸå­çº§ç²¾åº¦
   â€¢ å½±å“ï¼šåŠ é€Ÿè¯ç‰©ç ”å‘10å€

2. åŸºå› è¡¨è¾¾é¢„æµ‹ - Enformer
   â€¢ è¾“å…¥ï¼šDNAåºåˆ—
   â€¢ è¾“å‡ºï¼šåŸºå› è¡¨è¾¾æ¨¡å¼
   â€¢ èŒƒå›´ï¼š200kbåŸºå› ç»„åŒºåŸŸ
   â€¢ åº”ç”¨ï¼šç†è§£åŸºå› è°ƒæ§

3. è¯ç‰©è®¾è®¡ - ç”Ÿæˆæ¨¡å‹
   â€¢ è¾“å…¥ï¼šé¶ç‚¹ç»“æ„
   â€¢ è¾“å‡ºï¼šå€™é€‰è¯ç‰©åˆ†å­
   â€¢ é€Ÿåº¦ï¼šç§’çº§ç”Ÿæˆ
   â€¢ æˆåŠŸæ¡ˆä¾‹ï¼šCOVID-19è¯ç‰©

4. ç»†èƒå›¾åƒåˆ†æ
   â€¢ è¾“å…¥ï¼šæ˜¾å¾®é•œå›¾åƒ
   â€¢ è¾“å‡ºï¼šç»†èƒç±»å‹ã€çŠ¶æ€
   â€¢ ç²¾åº¦ï¼šè¶…è¶Šäººç±»ä¸“å®¶
   â€¢ åº”ç”¨ï¼šç™Œç—‡æ—©æœŸè¯Šæ–­
""")
```

### 8.2 ç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹

```python
# ä½¿ç”¨sklearnå®ç°ç®€å•çš„ç¥ç»ç½‘ç»œ
from sklearn.neural_network import MLPClassifier, MLPRegressor

def demonstrate_neural_network():
    """æ¼”ç¤ºç¥ç»ç½‘ç»œçš„å¼ºå¤§èƒ½åŠ›"""
    
    print("=" * 60)
    print("ç¥ç»ç½‘ç»œæ¼”ç¤ºï¼šå­¦ä¹ å¤æ‚çš„åŸºå› è°ƒæ§æ¨¡å¼")
    print("=" * 60)
    
    # åˆ›å»ºå¤æ‚çš„éçº¿æ€§æ•°æ®ï¼ˆæ¨¡æ‹ŸåŸºå› è°ƒæ§ç½‘ç»œï¼‰
    np.random.seed(42)
    n_samples = 1000
    n_genes = 10
    
    # è¾“å…¥ï¼šè½¬å½•å› å­è¡¨è¾¾æ°´å¹³
    X = np.random.randn(n_samples, n_genes)
    
    # è¾“å‡ºï¼šç›®æ ‡åŸºå› è¡¨è¾¾ï¼ˆå¤æ‚çš„éçº¿æ€§å…³ç³»ï¼‰
    y = (
        np.sin(X[:, 0] * X[:, 1]) +  # åŸºå› ç›¸äº’ä½œç”¨
        np.exp(-X[:, 2]**2) +         # é¥±å’Œæ•ˆåº”
        X[:, 3] * X[:, 4] * X[:, 5] +  # ä¸‰å› å­ååŒ
        0.1 * np.random.randn(n_samples)  # å™ªå£°
    )
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ¯”è¾ƒçº¿æ€§æ¨¡å‹å’Œç¥ç»ç½‘ç»œ
    from sklearn.linear_model import LinearRegression
    
    # çº¿æ€§æ¨¡å‹
    linear = LinearRegression()
    linear.fit(X_train, y_train)
    linear_score = linear.score(X_test, y_test)
    
    # ç¥ç»ç½‘ç»œ
    nn = MLPRegressor(
        hidden_layer_sizes=(50, 30, 20),  # ä¸‰å±‚éšè—å±‚
        activation='relu',
        max_iter=1000,
        random_state=42
    )
    nn.fit(X_train, y_train)
    nn_score = nn.score(X_test, y_test)
    
    print(f"\næ¨¡å‹æ€§èƒ½æ¯”è¾ƒ (RÂ² åˆ†æ•°):")
    print(f"çº¿æ€§å›å½’: {linear_score:.3f}")
    print(f"ç¥ç»ç½‘ç»œ: {nn_score:.3f}")
    print(f"æ€§èƒ½æå‡: {(nn_score - linear_score) / linear_score * 100:.1f}%")
    
    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # çº¿æ€§æ¨¡å‹é¢„æµ‹
    y_pred_linear = linear.predict(X_test)
    axes[0].scatter(y_test, y_pred_linear, alpha=0.5)
    axes[0].plot([y_test.min(), y_test.max()], 
                 [y_test.min(), y_test.max()], 'r--')
    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title(f'çº¿æ€§å›å½’ (RÂ² = {linear_score:.3f})')
    
    # ç¥ç»ç½‘ç»œé¢„æµ‹
    y_pred_nn = nn.predict(X_test)
    axes[1].scatter(y_test, y_pred_nn, alpha=0.5)
    axes[1].plot([y_test.min(), y_test.max()], 
                 [y_test.min(), y_test.max()], 'r--')
    axes[1].set_xlabel('çœŸå®å€¼')
    axes[1].set_ylabel('é¢„æµ‹å€¼')
    axes[1].set_title(f'ç¥ç»ç½‘ç»œ (RÂ² = {nn_score:.3f})')
    
    plt.tight_layout()
    plt.show()
    
    return nn

nn_model = demonstrate_neural_network()
```

### 8.3 æ·±åº¦å­¦ä¹ èµ„æºå’Œä¸‹ä¸€æ­¥

```python
deep_learning_resources = """
ğŸ“š æ·±åº¦å­¦ä¹ å­¦ä¹ èµ„æº

å…¥é—¨æ•™ç¨‹ï¼š
1. Fast.ai - å®ç”¨çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹
2. DeepLearning.ai - Andrew Ngçš„æ·±åº¦å­¦ä¹ ä¸“é¡¹
3. PyTorchæ•™ç¨‹ - å®˜æ–¹æ–‡æ¡£éå¸¸å‹å¥½

ç”Ÿç‰©ä¿¡æ¯å­¦ä¸“é—¨èµ„æºï¼š
1. DeepChem - è¯ç‰©å‘ç°æ·±åº¦å­¦ä¹ åº“
2. Kipoi - åŸºå› ç»„å­¦æ·±åº¦å­¦ä¹ æ¨¡å‹åº“
3. scVI - å•ç»†èƒæ·±åº¦å­¦ä¹ å·¥å…·

é‡è¦è®ºæ–‡ï¼š
1. AlphaFold2 - Nature 2021
2. Enformer - Nature Methods 2021
3. scBERT - Nature Machine Intelligence 2022

å®è·µé¡¹ç›®ï¼š
1. Kaggleç”Ÿç‰©ä¿¡æ¯å­¦ç«èµ›
2. DREAM Challenges
3. Critical Assessment competitions (CASP, CAGI)

æ¡†æ¶é€‰æ‹©ï¼š
â€¢ PyTorch - ç ”ç©¶é¦–é€‰ï¼Œçµæ´»
â€¢ TensorFlow - å·¥ä¸šéƒ¨ç½²ï¼Œæˆç†Ÿ
â€¢ JAX - é«˜æ€§èƒ½è®¡ç®—ï¼Œå‰æ²¿
"""

print(deep_learning_resources)
```

---

## æ€»ç»“ï¼šæœºå™¨å­¦ä¹ ä¹‹æ—…çš„å¼€å§‹

```python
def course_summary():
    """è¯¾ç¨‹æ€»ç»“"""
    
    print("""
    ========================================================
                    ğŸ“ è¯¾ç¨‹æ€»ç»“
    ========================================================
    
    æ­å–œä½ å®Œæˆäº†æœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ï¼
    
    ä½ å·²ç»æŒæ¡çš„æŠ€èƒ½ï¼š
    âœ… ç†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ
    âœ… åŒºåˆ†ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ 
    âœ… è¿›è¡Œç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†
    âœ… ä½¿ç”¨å¤šç§åˆ†ç±»å’Œèšç±»ç®—æ³•
    âœ… è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œé¿å…è¿‡æ‹Ÿåˆ
    âœ… åº”ç”¨æœºå™¨å­¦ä¹ è§£å†³ç”Ÿç‰©å­¦é—®é¢˜
    
    è®°ä½è¿™äº›æ ¸å¿ƒåŸåˆ™ï¼š
    
    1. æ•°æ®è´¨é‡å†³å®šä¸Šé™
       "Garbage in, garbage out" - å¥½çš„æ•°æ®æ¯”å¤æ‚çš„ç®—æ³•æ›´é‡è¦
    
    2. ç®€å•æ¨¡å‹ä¼˜å…ˆ
       ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œåªåœ¨å¿…è¦æ—¶å¢åŠ å¤æ‚åº¦
    
    3. éªŒè¯ã€éªŒè¯ã€å†éªŒè¯
       æ°¸è¿œä¸è¦ç›¸ä¿¡æ²¡æœ‰ç»è¿‡éªŒè¯çš„æ¨¡å‹
    
    4. ç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯å…³é”®
       æœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦ç†è§£æ˜¯çµé­‚
    
    5. æŒç»­å­¦ä¹ 
       è¿™ä¸ªé¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä¿æŒå­¦ä¹ çš„çƒ­æƒ…
    
    ä¸‹ä¸€æ­¥å»ºè®®ï¼š
    
    ğŸš€ çŸ­æœŸç›®æ ‡ï¼ˆ1-3ä¸ªæœˆï¼‰ï¼š
    â€¢ å®Œæˆä¸€ä¸ªçœŸå®çš„ç”Ÿç‰©æ•°æ®åˆ†æé¡¹ç›®
    â€¢ å­¦ä¹ ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆPyTorchæˆ–TensorFlowï¼‰
    â€¢ å‚åŠ ä¸€ä¸ªKaggleç”Ÿç‰©ä¿¡æ¯å­¦ç«èµ›
    
    ğŸ¯ ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰ï¼š
    â€¢ æŒæ¡ä¸“ä¸šå·¥å…·ï¼ˆScanpy, DESeq2, Seuratï¼‰
    â€¢ ç†è§£å¹¶å®ç°ä¸€ç¯‡æœºå™¨å­¦ä¹ è®ºæ–‡
    â€¢ å»ºç«‹è‡ªå·±çš„ç”Ÿç‰©ä¿¡æ¯å­¦é¡¹ç›®ç»„åˆ
    
    ğŸŒŸ é•¿æœŸç›®æ ‡ï¼ˆ6-12ä¸ªæœˆï¼‰ï¼š
    â€¢ è´¡çŒ®å¼€æºé¡¹ç›®
    â€¢ å‘è¡¨ç ”ç©¶æˆæœ
    â€¢ æˆä¸ºé¢†åŸŸä¸“å®¶
    
    æœ€åçš„è¯ï¼š
    
    æœºå™¨å­¦ä¹ ä¸ºç”Ÿç‰©å­¦ç ”ç©¶æ‰“å¼€äº†æ–°çš„å¤§é—¨ã€‚
    ä½ ç°åœ¨æ‹¥æœ‰äº†é’¥åŒ™ï¼Œå»æ¢ç´¢ç”Ÿå‘½çš„å¥¥ç§˜å§ï¼
    
    è®°ä½ï¼šæ¯ä¸€è¡Œä»£ç éƒ½å¯èƒ½å¸®åŠ©ç†è§£ç”Ÿå‘½ï¼Œ
    æ¯ä¸€ä¸ªæ¨¡å‹éƒ½å¯èƒ½ä¸ºæ²»ç–—ç–¾ç—…å¸¦æ¥å¸Œæœ›ã€‚
    
    ç¥ä½ åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼
    
    - ä½ çš„Pythonå¯¼å¸ˆ ğŸ§¬ğŸ’»ğŸ”¬
    """)

course_summary()
```

---

## é™„å½•ï¼šå¸¸è§é—®é¢˜è§£ç­”

### Q1: æˆ‘åº”è¯¥é€‰æ‹©å“ªç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ

```python
algorithm_decision_tree = """
ç®—æ³•é€‰æ‹©å†³ç­–æ ‘ï¼š

1. ä½ çš„æ•°æ®æœ‰æ ‡ç­¾å—ï¼Ÿ
   â”œâ”€ æ˜¯ â†’ ç›‘ç£å­¦ä¹ 
   â”‚   â”œâ”€ è¾“å‡ºæ˜¯ç±»åˆ«ï¼Ÿâ†’ åˆ†ç±»
   â”‚   â”‚   â”œâ”€ éœ€è¦è§£é‡Šæ€§ï¼Ÿâ†’ å†³ç­–æ ‘/é€»è¾‘å›å½’
   â”‚   â”‚   â”œâ”€ éœ€è¦é«˜ç²¾åº¦ï¼Ÿâ†’ éšæœºæ£®æ—/æ¢¯åº¦æå‡
   â”‚   â”‚   â””â”€ æ•°æ®é‡å¾ˆå¤§ï¼Ÿâ†’ ç¥ç»ç½‘ç»œ
   â”‚   â””â”€ è¾“å‡ºæ˜¯æ•°å€¼ï¼Ÿâ†’ å›å½’
   â”‚       â”œâ”€ çº¿æ€§å…³ç³»ï¼Ÿâ†’ çº¿æ€§å›å½’
   â”‚       â””â”€ éçº¿æ€§å…³ç³»ï¼Ÿâ†’ éšæœºæ£®æ—/ç¥ç»ç½‘ç»œ
   â””â”€ å¦ â†’ æ— ç›‘ç£å­¦ä¹ 
       â”œâ”€ æƒ³è¦åˆ†ç»„ï¼Ÿâ†’ èšç±»
       â”‚   â”œâ”€ çŸ¥é“ç»„æ•°ï¼Ÿâ†’ K-means
       â”‚   â”œâ”€ ä»»æ„å½¢çŠ¶ï¼Ÿâ†’ DBSCAN
       â”‚   â””â”€ éœ€è¦å±‚æ¬¡ï¼Ÿâ†’ å±‚æ¬¡èšç±»
       â””â”€ æƒ³è¦é™ç»´ï¼Ÿâ†’ PCA/t-SNE/UMAP
"""
print(algorithm_decision_tree)
```

### Q2: å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Ÿ

```python
def handle_imbalanced_data():
    """å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„ç­–ç•¥"""
    
    strategies = {
        "1. é‡é‡‡æ ·": {
            "è¿‡é‡‡æ ·": "SMOTE - åˆæˆå°‘æ•°ç±»æ ·æœ¬",
            "æ¬ é‡‡æ ·": "éšæœºæ¬ é‡‡æ ·å¤šæ•°ç±»",
            "ç»„åˆ": "SMOTEENN - ç»“åˆè¿‡é‡‡æ ·å’Œæ¸…æ´—"
        },
        "2. ç®—æ³•å±‚é¢": {
            "ç±»æƒé‡": "class_weight='balanced'",
            "é˜ˆå€¼è°ƒæ•´": "è°ƒæ•´åˆ†ç±»é˜ˆå€¼",
            "é›†æˆæ–¹æ³•": "BalancedRandomForest"
        },
        "3. è¯„ä¼°æŒ‡æ ‡": {
            "ä¸ç”¨å‡†ç¡®ç‡": "å‡†ç¡®ç‡ä¼šè¯¯å¯¼",
            "ä½¿ç”¨": ["ç²¾ç¡®ç‡", "å¬å›ç‡", "F1", "AUC"]
        }
    }
    
    # ç¤ºä¾‹ä»£ç 
    from sklearn.utils.class_weight import compute_class_weight
    
    # è®¡ç®—ç±»æƒé‡
    y = np.array([0] * 900 + [1] * 100)  # ä¸å¹³è¡¡æ•°æ®
    classes = np.unique(y)
    weights = compute_class_weight('balanced', classes=classes, y=y)
    
    print("ç±»åˆ«æƒé‡:")
    for cls, weight in zip(classes, weights):
        count = sum(y == cls)
        print(f"  ç±»åˆ«{cls}: æ ·æœ¬æ•°={count}, æƒé‡={weight:.2f}")
    
    return strategies

imbalance_strategies = handle_imbalanced_data()
```

### Q3: å¦‚ä½•è§£é‡Šæ¨¡å‹é¢„æµ‹ï¼Ÿ

```python
def model_interpretability():
    """æ¨¡å‹å¯è§£é‡Šæ€§æ–¹æ³•"""
    
    methods = {
        "1. å†…ç½®è§£é‡Š": {
            "çº¿æ€§æ¨¡å‹": "æŸ¥çœ‹ç³»æ•°",
            "å†³ç­–æ ‘": "å¯è§†åŒ–è§„åˆ™",
            "éšæœºæ£®æ—": "ç‰¹å¾é‡è¦æ€§"
        },
        "2. æ¨¡å‹æ— å…³æ–¹æ³•": {
            "SHAP": "è§£é‡Šå•ä¸ªé¢„æµ‹",
            "LIME": "å±€éƒ¨è§£é‡Š",
            "Permutation Importance": "ç‰¹å¾é‡è¦æ€§"
        },
        "3. å¯è§†åŒ–": {
            "éƒ¨åˆ†ä¾èµ–å›¾": "ç‰¹å¾å½±å“",
            "å­¦ä¹ æ›²çº¿": "è®­ç»ƒè¿‡ç¨‹",
            "æ··æ·†çŸ©é˜µ": "é”™è¯¯æ¨¡å¼"
        }
    }
    
    print("æ¨¡å‹è§£é‡Šæ–¹æ³•:")
    for category, methods_dict in methods.items():
        print(f"\n{category}:")
        for method, description in methods_dict.items():
            print(f"  â€¢ {method}: {description}")
    
    return methods

interpretability = model_interpretability()
```

### Q4: å¦‚ä½•é¿å…æ•°æ®æ³„éœ²ï¼Ÿ

```python
data_leakage_prevention = """
âš ï¸ æ•°æ®æ³„éœ²é¢„é˜²æ¸…å•ï¼š

1. âœ… åœ¨åˆ†å‰²æ•°æ®ä¹‹å‰ä¸è¦æŸ¥çœ‹æµ‹è¯•é›†
2. âœ… ç‰¹å¾å·¥ç¨‹åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ
3. âœ… æ ‡å‡†åŒ–/å½’ä¸€åŒ–ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°
4. âœ… ç‰¹å¾é€‰æ‹©ä¸åŒ…æ‹¬æµ‹è¯•é›†
5. âœ… äº¤å‰éªŒè¯åœ¨è®­ç»ƒé›†å†…éƒ¨è¿›è¡Œ
6. âŒ ä¸è¦ç”¨å…¨éƒ¨æ•°æ®é€‰æ‹©ç‰¹å¾
7. âŒ ä¸è¦åœ¨åˆ†å‰²å‰è¿›è¡Œé¢„å¤„ç†
8. âŒ ä¸è¦è®©æœªæ¥ä¿¡æ¯æ³„éœ²åˆ°è¿‡å»

ç¤ºä¾‹ä»£ç ï¼š
```python
# é”™è¯¯åšæ³•
X_scaled = scaler.fit_transform(X)  # ç”¨äº†å…¨éƒ¨æ•°æ®
X_train, X_test = train_test_split(X_scaled)

# æ­£ç¡®åšæ³•
X_train, X_test = train_test_split(X)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # åªtransformï¼Œä¸fit
```
"""
print(data_leakage_prevention)
```

---

## ç»“è¯­

æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œä½†è®°ä½ï¼š

> "æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä»·å€¼ä¸åœ¨äºå…¶å¤æ‚åº¦ï¼Œè€Œåœ¨äºå®ƒè§£å†³çš„ç”Ÿç‰©å­¦é—®é¢˜ã€‚"

ç»§ç»­æ¢ç´¢ï¼Œä¿æŒå¥½å¥‡å¿ƒï¼Œç”¨ä½ çš„æ–°æŠ€èƒ½æ¨åŠ¨ç”Ÿå‘½ç§‘å­¦çš„è¿›æ­¥ï¼

**ç¥ä½ æˆä¸ºä¼˜ç§€çš„è®¡ç®—ç”Ÿç‰©å­¦å®¶ï¼** ğŸ§¬ğŸ¤–ğŸ”¬