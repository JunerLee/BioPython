#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chapter 10 ç»ƒä¹ é¢˜: æœºå™¨å­¦ä¹ å…¥é—¨ - æ¨¡å¼è¯†åˆ«çš„è‰ºæœ¯

è¿™äº›ç»ƒä¹ å°†å¸®åŠ©ä½ å·©å›ºæœºå™¨å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µï¼Œ
å¹¶å­¦ä¼šåœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­åº”ç”¨è¿™äº›æŠ€æœ¯ã€‚

ç»ƒä¹ é¡ºåºï¼š
1. æ•°æ®é¢„å¤„ç†å’Œæ¢ç´¢æ€§åˆ†æ
2. ç›‘ç£å­¦ä¹  - åˆ†ç±»ä»»åŠ¡
3. æ— ç›‘ç£å­¦ä¹  - èšç±»åˆ†æ
4. æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
5. ç»¼åˆå®æˆ˜é¡¹ç›®

æ¯ä¸ªç»ƒä¹ éƒ½åŒ…å«ï¼š
- èƒŒæ™¯æè¿°
- ä»»åŠ¡è¦æ±‚
- æœŸå¾…è¾“å‡º
- æç¤ºä¿¡æ¯
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, make_blobs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def exercise_1_data_preprocessing():
    """
    ç»ƒä¹ 1: æ•°æ®é¢„å¤„ç†å’Œæ¢ç´¢æ€§åˆ†æ
    
    èƒŒæ™¯ï¼š
    ä½ è·å¾—äº†ä¸€ä¸ªåŒ…å«100ä¸ªè›‹ç™½è´¨çš„æ•°æ®é›†ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹å¾ï¼š
    - åˆ†å­é‡ (molecular_weight)
    - ç­‰ç”µç‚¹ (isoelectric_point) 
    - æ°¨åŸºé…¸æ•°é‡ (amino_acid_count)
    - ç–æ°´æ€§æŒ‡æ•° (hydrophobicity)
    - ä¸ç¨³å®šæ€§æŒ‡æ•° (instability)
    
    ä»»åŠ¡ï¼š
    1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    2. è¿›è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æ
    3. æ£€æŸ¥ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    4. æ•°æ®æ ‡å‡†åŒ–
    5. ç»˜åˆ¶ç›¸å…³æ€§çƒ­å›¾
    """
    print("=" * 60)
    print("ğŸ§¬ ç»ƒä¹ 1: è›‹ç™½è´¨æ•°æ®é¢„å¤„ç†")
    print("=" * 60)
    
    # TODO: åˆ›å»ºæ¨¡æ‹Ÿè›‹ç™½è´¨æ•°æ®
    # æç¤º: ä½¿ç”¨np.randomè®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    np.random.seed(42)
    
    # TODO: åˆ›å»º100ä¸ªè›‹ç™½è´¨çš„ç‰¹å¾æ•°æ®
    # ç”Ÿç‰©å­¦èƒŒæ™¯è®¾å®šï¼š
    # åˆ†å­é‡: 5-150 kDa ï¼ˆå¤§å¤šæ•°è›‹ç™½è´¨åœ¨æ­¤èŒƒå›´ï¼‰ï¼Œå¯¹æ•°æ­£æ€åˆ†å¸ƒ
    # ç­‰ç”µç‚¹: 4.0-11.0 ï¼ˆè›‹ç™½è´¨pIèŒƒå›´ï¼‰ï¼Œæ­£æ€åˆ†å¸ƒåé…¸æ€§  
    # æ°¨åŸºé…¸æ•°é‡: 50-2000 ï¼ˆç»“æ„åŸŸåˆ°å¤§å‹è›‹ç™½è´¨ï¼‰ï¼Œä¸åˆ†å­é‡ç›¸å…³
    # ç–æ°´æ€§æŒ‡æ•°: -2.5åˆ°2.5 ï¼ˆGRAVYå€¼èŒƒå›´ï¼‰ï¼Œæ­£æ€åˆ†å¸ƒ
    # ä¸ç¨³å®šæ€§æŒ‡æ•°: 0-100 ï¼ˆInstability Indexï¼‰ï¼ŒæŒ‡æ•°åˆ†å¸ƒ
    
    protein_data = {
        'molecular_weight': None,  # è¯·å®Œæˆ
        'isoelectric_point': None,  # è¯·å®Œæˆ
        'amino_acid_count': None,   # è¯·å®Œæˆ
        'hydrophobicity': None,     # è¯·å®Œæˆ
        'instability': None         # è¯·å®Œæˆ
    }
    
    # TODO: è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(protein_data)
    
    # TODO: æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼(éšæœºé€‰æ‹©5ä¸ªä½ç½®)
    
    # TODO: æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼(å°†å‡ ä¸ªæ•°å€¼è®¾ç½®ä¸ºæç«¯å€¼)
    
    print("æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    # TODO: æ˜¾ç¤ºå‰5è¡Œæ•°æ®
    
    print("\næè¿°æ€§ç»Ÿè®¡:")
    # TODO: æ˜¾ç¤ºdescribe()ç»“æœ
    
    print("\nç¼ºå¤±å€¼æ£€æŸ¥:")
    # TODO: æ£€æŸ¥å¹¶æ˜¾ç¤ºç¼ºå¤±å€¼æ•°é‡
    
    # TODO: å¤„ç†ç¼ºå¤±å€¼(ç”¨å‡å€¼å¡«å……)
    
    # TODO: æ£€æµ‹å¼‚å¸¸å€¼(ä½¿ç”¨IQRæ–¹æ³•æˆ–Z-scoreæ–¹æ³•)
    
    # TODO: æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    # df_scaled = ...
    
    # TODO: ç»˜åˆ¶ç›¸å…³æ€§çƒ­å›¾
    plt.figure(figsize=(10, 8))
    # correlation_matrix = ...
    # sns.heatmap(correlation_matrix, ...)
    plt.title('è›‹ç™½è´¨ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
    plt.show()
    
    print("âœ… ç»ƒä¹ 1å®Œæˆï¼")
    return df  # è¿”å›å¤„ç†åçš„æ•°æ®


def exercise_2_gene_function_classifier():
    """
    ç»ƒä¹ 2: åŸºå› åŠŸèƒ½åˆ†ç±»å™¨
    
    ğŸ§¬ ç”Ÿç‰©å­¦èƒŒæ™¯ï¼š
    åŸºäºå¤šç»´ç‰¹å¾é¢„æµ‹åŸºå› åŠŸèƒ½ç±»åˆ«ï¼š
    - ç»“æ„è›‹ç™½åŸºå› ï¼šç¼–ç èƒ¶åŸè›‹ç™½ã€é…¸è›‹ç™½ç­‰ç»“æ„æˆåˆ†
    - é…¶åŸºå› ï¼šç¼–ç å‚¬åŒ–ç”ŸåŒ–ååº”çš„é…¶ç±»
    - è°ƒèŠ‚è›‹ç™½åŸºå› ï¼šç¼–ç è½¬å½•å› å­ã€ä¿¡å·åˆ†å­ç­‰
    
    ğŸ” ç‰¹å¾å·¥ç¨‹ï¼š
    - GCå«é‡ï¼šä¸åŸºå› è¡¨è¾¾æ°´å¹³ç›¸å…³
    - åŸºå› é•¿åº¦ï¼šä¸åŠŸèƒ½å¤æ‚æ€§ç›¸å…³
    - å¤–æ˜¾å­æ•°é‡ï¼šåæ˜ è°ƒæ§å¤æ‚æ€§
    - ä¿å­˜æ€§è¯„åˆ†ï¼šé‡è¦åŠŸèƒ½åŸºå› æ›´ä¿å®ˆ
    - è¡¨è¾¾æ°´å¹³ï¼šä¸åŒåŠŸèƒ½ç±»åˆ«è¡¨è¾¾æ¨¡å¼ä¸åŒ
    - ç”²åŸºåŒ–æ°´å¹³ï¼šè¡¨è§‚é—ä¼ ä¿®é¥°å½±å“åŸºå› è¡¨è¾¾
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç»ƒä¹ 2: åŸºå› åŠŸèƒ½åˆ†ç±»å™¨")
    print("=" * 60)
    
    # TODO: åˆ›å»ºåŸºå› åˆ†ç±»æ•°æ®
    # ä½¿ç”¨make_classificationåˆ›å»º3åˆ†ç±»é—®é¢˜
    # n_samples=300, n_features=8, n_classes=3, n_informative=6
    np.random.seed(42)
    
    # X, y = make_classification(...)
    X, y = None, None  # è¯·å®Œæˆ
    
    # æ·»åŠ ç”Ÿç‰©å­¦æ„ä¹‰æ˜ç¡®çš„ç‰¹å¾åç§°
    feature_names = [
        'gc_content',           # GCå«é‡ï¼šä¸åŸºå› è¡¨è¾¾çº§åˆ«å’ŒæŸ“è‰²è´¨ç»“æ„ç›¸å…³
        'gene_length',          # åŸºå› é•¿åº¦ï¼šå¤æ‚åŠŸèƒ½åŸºå› é€šå¸¸æ›´é•¿
        'exon_count',           # å¤–æ˜¾å­æ•°ï¼šåæ˜ æ›¿ä»£å‰–æ¥å¤æ‚æ€§
        'intron_length',        # å†…å«å­é•¿åº¦ï¼šè°ƒèŠ‚å…ƒä»¶åˆ†å¸ƒåŒºåŸŸ
        'promoter_strength',    # å¯åŠ¨å­å¼ºåº¦ï¼šåŸºç¡€è½¬å½•æ´»æ€§
        'conservation_score',   # è¿›åŒ–ä¿å®ˆæ€§ï¼šåŠŸèƒ½é‡è¦æ€§æŒ‡ç¤º
        'expression_level',     # è¡¨è¾¾æ°´å¹³ï¼šç»„ç»‡ç‰¹å¼‚æ€§å’Œæ—¶é—´åŠ¨æ€
        'methylation_level'     # ç”²åŸºåŒ–æ°´å¹³ï¼šè¡¨è§‚é—ä¼ è°ƒæ§çŠ¶æ€
    ]
    
    # TODO: åˆ›å»ºDataFrame
    df = pd.DataFrame(X, columns=feature_names)
    df['function_class'] = y
    
    print("åŸºå› åˆ†ç±»æ•°æ®ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{df['function_class'].value_counts()}")
    
    # TODO: å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
    plt.figure(figsize=(12, 4))
    
    # å­å›¾1: ç±»åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾
    plt.subplot(1, 3, 1)
    # df['function_class'].value_counts().plot(kind='bar')
    plt.title('åŸºå› åŠŸèƒ½ç±»åˆ«åˆ†å¸ƒ')
    
    # å­å›¾2: ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾(é€‰æ‹©å‰4ä¸ªç‰¹å¾)
    plt.subplot(1, 3, 2)
    # df[feature_names[:4]].boxplot()
    plt.title('ç‰¹å¾åˆ†å¸ƒ')
    
    # å­å›¾3: ä¸åŒç±»åˆ«ä¸‹æŸä¸ªç‰¹å¾çš„åˆ†å¸ƒ
    plt.subplot(1, 3, 3)
    # for class_id in range(3):
    #     data = df[df['function_class'] == class_id]['gc_content']
    #     plt.hist(data, alpha=0.5, label=f'Class {class_id}', bins=15)
    plt.title('GCå«é‡åœ¨ä¸åŒç±»åˆ«ä¸­çš„åˆ†å¸ƒ')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # TODO: æ•°æ®åˆ†å‰²
    # X_train, X_test, y_train, y_test = train_test_split(...)
    
    # TODO: æ•°æ®æ ‡å‡†åŒ–
    # scaler = StandardScaler()
    # X_train_scaled = ...
    # X_test_scaled = ...
    
    # TODO: è®­ç»ƒå¤šä¸ªåˆ†ç±»æ¨¡å‹
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42),
        'å†³ç­–æ ‘': DecisionTreeClassifier(random_state=42),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42),
        'SVM': SVC(random_state=42),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    results = {}
    
    # TODO: è®­ç»ƒæ¯ä¸ªæ¨¡å‹å¹¶è¯„ä¼°
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        # model.fit(...)
        
        # é¢„æµ‹
        # y_pred = model.predict(...)
        
        # è®¡ç®—å‡†ç¡®ç‡
        # accuracy = accuracy_score(...)
        
        # å­˜å‚¨ç»“æœ
        # results[name] = {'accuracy': accuracy, 'y_pred': y_pred}
        
        print(f"  å‡†ç¡®ç‡: {None}")  # è¯·å®Œæˆ
    
    # TODO: å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ
    plt.figure(figsize=(10, 6))
    # model_names = list(results.keys())
    # accuracies = [results[name]['accuracy'] for name in model_names]
    # plt.bar(model_names, accuracies)
    plt.title('æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.xticks(rotation=45)
    plt.show()
    
    # TODO: é€‰æ‹©æœ€ä½³æ¨¡å‹å¹¶æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
    # best_model = max(results.keys(), key=lambda k: results[k]['accuracy'])
    # best_pred = results[best_model]['y_pred']
    
    print(f"\næœ€ä½³æ¨¡å‹: {None}")  # è¯·å®Œæˆ
    print("åˆ†ç±»æŠ¥å‘Š:")
    # print(classification_report(y_test, best_pred, 
    #                           target_names=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½']))
    
    # TODO: ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    # cm = confusion_matrix(y_test, best_pred)
    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
    #             xticklabels=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½'],
    #             yticklabels=['ç»“æ„è›‹ç™½', 'é…¶', 'è°ƒèŠ‚è›‹ç™½'])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.show()
    
    print("âœ… ç»ƒä¹ 2å®Œæˆï¼")
    return results


def exercise_3_cell_clustering():
    """
    ç»ƒä¹ 3: å•ç»†èƒèšç±»åˆ†æ
    
    ğŸ§¬ ç§‘å­¦èƒŒæ™¯ï¼š
    å•ç»†èƒRNAæµ‹åº(scRNA-seq)æ˜¯ç°ä»£ç»†èƒç”Ÿç‰©å­¦çš„é©å‘½æ€§æŠ€æœ¯ã€‚
    é€šè¿‡åˆ†æå•ä¸ªç»†èƒçš„åŸºå› è¡¨è¾¾è°±ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
    â€¢ å‘ç°æ–°çš„ç»†èƒäºšå‹å’ŒçŠ¶æ€
    â€¢ æ­ç¤ºç»†èƒåˆ†åŒ–è½¨è¿¹å’Œè°±ç³»å…³ç³»
    â€¢ ç†è§£ç–¾ç—…ä¸­çš„ç»†èƒå¼‚è´¨æ€§
    â€¢ è¯†åˆ«ç¨€æœ‰ç»†èƒç±»å‹å’ŒçŠ¶æ€
    
    ğŸ” æ•°æ®ç‰¹ç‚¹ï¼š
    - é«˜ç»´æ€§ï¼šæ¯ä¸ªç»†èƒ~20,000ä¸ªåŸºå› 
    - ç¨€ç–æ€§ï¼šå¤§éƒ¨åˆ†åŸºå› ä¸è¡¨è¾¾(dropout)
    - å™ªå£°ï¼šæŠ€æœ¯å™ªå£°å’Œç”Ÿç‰©å™ªå£°
    - å¼‚è´¨æ€§ï¼šç»†èƒå‘¨æœŸã€çŠ¶æ€å·®å¼‚
    
    ğŸ“Š åˆ†ææµç¨‹ï¼š
    1. è´¨é‡æ§åˆ¶ï¼šè¿‡æ»¤ä½è´¨é‡ç»†èƒå’ŒåŸºå› 
    2. å½’ä¸€åŒ–ï¼šæ¶ˆé™¤æŠ€æœ¯å˜å¼‚å’Œæ‰¹æ¬¡æ•ˆåº”
    3. ç‰¹å¾é€‰æ‹©ï¼šæ‰¾åˆ°é«˜å˜å¼‚åŸºå› (HVGs)
    4. é™ç»´ï¼šPCAæ¶ˆé™¤å™ªå£°ï¼Œt-SNE/UMAPå¯è§†åŒ–
    5. èšç±»ï¼šè¯†åˆ«ç»†èƒç¾¤ä½“
    6. æ³¨é‡Šï¼šè¯†åˆ«ç»†èƒç±»å‹å’ŒmarkeråŸºå› 
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç»ƒä¹ 3: å•ç»†èƒèšç±»åˆ†æ")
    print("=" * 60)
    
    # TODO: ç”Ÿæˆæ¨¡æ‹Ÿå•ç»†èƒæ•°æ®
    # ä½¿ç”¨make_blobsåˆ›å»º3ä¸ªç»†èƒç¾¤
    # n_samples=100, n_features=50, centers=3
    np.random.seed(42)
    
    # X, true_labels = make_blobs(...)
    X, true_labels = None, None  # è¯·å®Œæˆ
    
    # TODO: æ¨¡æ‹ŸåŸºå› è¡¨è¾¾æ•°æ®ç‰¹ç‚¹(å¯¹æ•°æ­£æ€åˆ†å¸ƒ)
    # X = np.exp(X)  # è½¬æ¢ä¸ºå¯¹æ•°æ­£æ€åˆ†å¸ƒ
    
    # åˆ›å»ºåŸºå› åç§°
    gene_names = [f'Gene_{i:02d}' for i in range(1, 51)]
    cell_names = [f'Cell_{i:03d}' for i in range(1, 101)]
    
    # TODO: åˆ›å»ºDataFrame
    df = pd.DataFrame(X, columns=gene_names, index=cell_names)
    
    print("å•ç»†èƒæ•°æ®ä¿¡æ¯:")
    print(f"ç»†èƒæ•°é‡: {df.shape[0]}")
    print(f"åŸºå› æ•°é‡: {df.shape[1]}")
    print(f"è¡¨è¾¾å€¼èŒƒå›´: [{df.values.min():.2f}, {df.values.max():.2f}]")
    
    # TODO: æ•°æ®é¢„å¤„ç†
    # 1. å¯¹æ•°è½¬æ¢ (log1p)
    # df_log = np.log1p(df)
    
    # 2. æ ‡å‡†åŒ–
    # scaler = StandardScaler()
    # df_scaled = scaler.fit_transform(df_log)
    
    # TODO: PCAé™ç»´åˆ†æ
    from sklearn.decomposition import PCA
    
    # pca = PCA(n_components=2)
    # df_pca = pca.fit_transform(df_scaled)
    
    print(f"\nPCAè§£é‡Šæ–¹å·®æ¯”: {None}")  # è¯·å®Œæˆ
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {None}")  # è¯·å®Œæˆ
    
    # TODO: å°è¯•ä¸åŒçš„èšç±»ç®—æ³•
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.mixture import GaussianMixture
    from sklearn.metrics import silhouette_score, adjusted_rand_score
    
    clustering_methods = {
        'K-Means': KMeans(n_clusters=3, random_state=42),
        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
        'å±‚æ¬¡èšç±»': AgglomerativeClustering(n_clusters=3),
        'é«˜æ–¯æ··åˆ': GaussianMixture(n_components=3, random_state=42)
    }
    
    clustering_results = {}
    
    # TODO: æ‰§è¡Œèšç±»å¹¶è¯„ä¼°
    for name, clusterer in clustering_methods.items():
        print(f"\næ‰§è¡Œ {name} èšç±»...")
        
        # èšç±»
        # if name == 'é«˜æ–¯æ··åˆ':
        #     labels = clusterer.fit_predict(df_scaled)
        # else:
        #     labels = clusterer.fit_predict(df_scaled)
        
        labels = None  # è¯·å®Œæˆ
        
        # è¯„ä¼°æŒ‡æ ‡
        if labels is not None and len(set(labels)) > 1:
            # silhouette = silhouette_score(df_scaled, labels)
            # ari = adjusted_rand_score(true_labels, labels)
            silhouette = None  # è¯·å®Œæˆ
            ari = None  # è¯·å®Œæˆ
        else:
            silhouette = -1
            ari = -1
        
        clustering_results[name] = {
            'labels': labels,
            'silhouette': silhouette,
            'ari': ari
        }
        
        print(f"  è½®å»“ç³»æ•°: {silhouette}")
        print(f"  è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari}")
    
    # TODO: å¯è§†åŒ–èšç±»ç»“æœ
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (name, result) in enumerate(clustering_results.items()):
        ax = axes[idx]
        labels = result['labels']
        
        if labels is not None:
            # scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
            #                     c=labels, cmap='viridis', alpha=0.6)
            ax.set_title(f'{name}\nSilhouette: {result["silhouette"]:.3f}')
            # plt.colorbar(scatter, ax=ax)
    
    # çœŸå®æ ‡ç­¾
    ax = axes[4]
    # scatter = ax.scatter(df_pca[:, 0], df_pca[:, 1], 
    #                     c=true_labels, cmap='Set1', alpha=0.6)
    ax.set_title('çœŸå®æ ‡ç­¾')
    # plt.colorbar(scatter, ax=ax)
    
    # ç©ºç™½
    axes[5].axis('off')
    
    for ax in axes[:5]:
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
    
    plt.tight_layout()
    plt.show()
    
    # TODO: åˆ›å»ºæœ€ä¼˜èšç±»çš„çƒ­å›¾
    # é€‰æ‹©è½®å»“ç³»æ•°æœ€é«˜çš„èšç±»æ–¹æ³•
    # best_method = max(clustering_results.keys(), 
    #                   key=lambda k: clustering_results[k]['silhouette'])
    # best_labels = clustering_results[best_method]['labels']
    
    print(f"\næœ€ä½³èšç±»æ–¹æ³•: {None}")  # è¯·å®Œæˆ
    
    # TODO: ç»˜åˆ¶èšç±»çƒ­å›¾
    # é€‰æ‹©è¡¨è¾¾å˜å¼‚æœ€å¤§çš„20ä¸ªåŸºå› 
    # gene_std = df.std(axis=0)
    # top_genes = gene_std.nlargest(20).index
    
    plt.figure(figsize=(12, 8))
    # TODO: åˆ›å»ºèšç±»çƒ­å›¾
    # æç¤º: æŒ‰èšç±»æ ‡ç­¾æ’åºç»†èƒï¼Œç„¶åç»˜åˆ¶çƒ­å›¾
    plt.title('ç»†èƒèšç±»çƒ­å›¾')
    plt.show()
    
    print("âœ… ç»ƒä¹ 3å®Œæˆï¼")
    return clustering_results


def exercise_4_model_optimization():
    """
    ç»ƒä¹ 4: æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
    
    èƒŒæ™¯ï¼š
    ä½¿ç”¨ä¹³è…ºç™Œæ•°æ®é›†ç»ƒä¹ æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯ï¼š
    - äº¤å‰éªŒè¯
    - è¶…å‚æ•°è°ƒä¼˜
    - è¿‡æ‹Ÿåˆæ£€æµ‹
    - ç‰¹å¾é€‰æ‹©
    
    ä»»åŠ¡ï¼š
    1. åŠ è½½æ•°æ®
    2. å®ç°äº¤å‰éªŒè¯
    3. ç½‘æ ¼æœç´¢ä¼˜åŒ–è¶…å‚æ•°
    4. åˆ†æè¿‡æ‹Ÿåˆç°è±¡
    5. ç‰¹å¾é€‰æ‹©
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç»ƒä¹ 4: æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–")
    print("=" * 60)
    
    # TODO: åˆ›å»ºåˆ†ç±»æ•°æ®
    # ä½¿ç”¨make_classificationåˆ›å»ºäºŒåˆ†ç±»é—®é¢˜ï¼Œæœ‰ä¸€äº›å™ªå£°ç‰¹å¾
    np.random.seed(42)
    
    # X, y = make_classification(
    #     n_samples=500, n_features=20, n_informative=10, 
    #     n_redundant=5, n_classes=2, flip_y=0.05, random_state=42
    # )
    X, y = None, None  # è¯·å®Œæˆ
    
    print("æ•°æ®ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°: {X.shape[0] if X is not None else 'None'}")
    print(f"ç‰¹å¾æ•°: {X.shape[1] if X is not None else 'None'}")
    print(f"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {np.bincount(y) if y is not None else 'None'}")
    
    # TODO: æ•°æ®åˆ†å‰²
    # X_train, X_test, y_train, y_test = train_test_split(...)
    
    # TODO: æ•°æ®æ ‡å‡†åŒ–
    # scaler = StandardScaler()
    # X_train_scaled = ...
    # X_test_scaled = ...
    
    # ä»»åŠ¡4.1: äº¤å‰éªŒè¯
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.1: äº¤å‰éªŒè¯")
    print("-" * 40)
    
    from sklearn.model_selection import cross_val_score, StratifiedKFold
    from sklearn.ensemble import RandomForestClassifier
    
    # TODO: ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°éšæœºæ£®æ—
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # cv_scores = cross_val_score(rf, X_train_scaled, y_train, 
    #                            cv=StratifiedKFold(5), scoring='accuracy')
    cv_scores = None  # è¯·å®Œæˆ
    
    print(f"äº¤å‰éªŒè¯åˆ†æ•°: {cv_scores if cv_scores is not None else 'None'}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {cv_scores.mean() if cv_scores is not None else 'None':.3f} Â± {cv_scores.std() if cv_scores is not None else 0:.3f}")
    
    # ä»»åŠ¡4.2: è¶…å‚æ•°è°ƒä¼˜
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.2: è¶…å‚æ•°è°ƒä¼˜")
    print("-" * 40)
    
    from sklearn.model_selection import GridSearchCV
    
    # TODO: å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {np.prod([len(v) for v in param_grid.values()])}")
    
    # TODO: ç½‘æ ¼æœç´¢
    # grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='f1', n_jobs=-1)
    # grid_search.fit(X_train_scaled, y_train)
    
    grid_search = None  # è¯·å®Œæˆ
    
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_ if grid_search else 'None'}")
    print(f"æœ€ä½³CVåˆ†æ•°: {grid_search.best_score_ if grid_search else 'None':.3f}")
    
    # ä»»åŠ¡4.3: è¿‡æ‹Ÿåˆåˆ†æ
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.3: è¿‡æ‹Ÿåˆåˆ†æ")
    print("-" * 40)
    
    # TODO: æ¯”è¾ƒä¸åŒå¤æ‚åº¦æ¨¡å‹çš„è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®
    from sklearn.model_selection import validation_curve
    
    # ä»¥æ ‘çš„æ·±åº¦ä¸ºä¾‹åˆ†æè¿‡æ‹Ÿåˆ
    # depths = [1, 3, 5, 10, 15, 20, None]
    # train_scores, val_scores = validation_curve(
    #     DecisionTreeClassifier(random_state=42), 
    #     X_train_scaled, y_train,
    #     param_name='max_depth', param_range=depths,
    #     cv=5, scoring='accuracy'
    # )
    
    depths = None  # è¯·å®Œæˆ
    train_scores = None  # è¯·å®Œæˆ
    val_scores = None  # è¯·å®Œæˆ
    
    # TODO: ç»˜åˆ¶éªŒè¯æ›²çº¿
    if train_scores is not None and val_scores is not None:
        plt.figure(figsize=(10, 6))
        
        # train_mean = train_scores.mean(axis=1)
        # train_std = train_scores.std(axis=1)
        # val_mean = val_scores.mean(axis=1)
        # val_std = val_scores.std(axis=1)
        
        # plt.plot(depths, train_mean, 'o-', label='è®­ç»ƒè¯¯å·®', color='blue')
        # plt.fill_between(depths, train_mean-train_std, train_mean+train_std, alpha=0.1, color='blue')
        # plt.plot(depths, val_mean, 'o-', label='éªŒè¯è¯¯å·®', color='red')
        # plt.fill_between(depths, val_mean-val_std, val_mean+val_std, alpha=0.1, color='red')
        
        plt.xlabel('æ ‘çš„æ·±åº¦')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.title('éªŒè¯æ›²çº¿ - è¿‡æ‹Ÿåˆåˆ†æ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    
    # ä»»åŠ¡4.4: ç‰¹å¾é€‰æ‹©
    print("\n" + "-" * 40)
    print("ä»»åŠ¡4.4: ç‰¹å¾é€‰æ‹©")
    print("-" * 40)
    
    from sklearn.feature_selection import SelectKBest, f_classif
    from sklearn.feature_selection import RFE
    
    # TODO: ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•é€‰æ‹©ç‰¹å¾
    # selector = SelectKBest(score_func=f_classif, k=10)
    # X_train_selected = selector.fit_transform(X_train_scaled, y_train)
    # X_test_selected = selector.transform(X_test_scaled)
    
    # selected_features = selector.get_support()
    # print(f"é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {np.where(selected_features)[0]}")
    
    # TODO: ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤
    from sklearn.linear_model import LogisticRegression
    
    # rfe = RFE(LogisticRegression(random_state=42), n_features_to_select=10)
    # rfe.fit(X_train_scaled, y_train)
    
    # print(f"RFEé€‰ä¸­çš„ç‰¹å¾: {np.where(rfe.support_)[0]}")
    # print(f"ç‰¹å¾æ’å: {rfe.ranking_}")
    
    # TODO: æ¯”è¾ƒç‰¹å¾é€‰æ‹©å‰åçš„æ€§èƒ½
    # ä½¿ç”¨é€‰æ‹©åçš„ç‰¹å¾è®­ç»ƒæ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½
    
    print("âœ… ç»ƒä¹ 4å®Œæˆï¼")
    return grid_search


def exercise_5_cancer_subtype_project():
    """
    ç»ƒä¹ 5: ç»¼åˆé¡¹ç›® - ç™Œç—‡äºšå‹åˆ†ç±»
    
    èƒŒæ™¯ï¼š
    è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§é¡¹ç›®ï¼Œæ¨¡æ‹ŸçœŸå®çš„ç™Œç—‡äºšå‹åˆ†ç±»ä»»åŠ¡ã€‚
    ä½ éœ€è¦æ•´åˆå‰é¢å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯ï¼š
    - æ•°æ®é¢„å¤„ç†
    - ç‰¹å¾å·¥ç¨‹  
    - æ¨¡å‹è®­ç»ƒå’Œé€‰æ‹©
    - ç»“æœè¯„ä¼°å’Œå¯è§†åŒ–
    
    ä»»åŠ¡ï¼š
    1. ç”Ÿæˆæ¨¡æ‹Ÿçš„ç™Œç—‡åŸºå› è¡¨è¾¾æ•°æ®
    2. å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹
    3. ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©
    4. è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶è°ƒä¼˜
    5. æ¨¡å‹è§£é‡Šå’Œç»“æœå¯è§†åŒ–
    6. æ’°å†™åˆ†ææŠ¥å‘Š
    """
    print("\n" + "=" * 60)
    print("ğŸ§¬ ç»¼åˆé¡¹ç›®: ç™Œç—‡äºšå‹åˆ†ç±»")
    print("=" * 60)
    
    print("""
é¡¹ç›®èƒŒæ™¯:
ä½ æ˜¯ä¸€åç”Ÿç‰©ä¿¡æ¯å­¦ç ”ç©¶å‘˜ï¼Œéœ€è¦åŸºäºåŸºå› è¡¨è¾¾æ•°æ®
è‡ªåŠ¨è¯†åˆ«ä¹³è…ºç™Œçš„åˆ†å­äºšå‹ï¼š

1. Luminal A - æ¿€ç´ å—ä½“é˜³æ€§ï¼Œé¢„åè¾ƒå¥½
2. Luminal B - æ¿€ç´ å—ä½“é˜³æ€§ï¼Œå¢æ®–æ´»è·ƒ  
3. HER2+ - HER2æ‰©å¢ï¼Œé¶å‘æ²»ç–—æ•æ„Ÿ
4. Basal-like - ä¸‰é˜´æ€§ï¼Œé¢„åè¾ƒå·®

ä½ çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ªå‡†ç¡®çš„åˆ†ç±»å™¨ï¼Œ
å¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œç²¾å‡†è¯Šæ–­ã€‚
    """)
    
    # TODO: æ­¥éª¤1 - æ•°æ®ç”Ÿæˆå’Œæ¢ç´¢
    print("\n" + "-" * 50)
    print("æ­¥éª¤1: æ•°æ®ç”Ÿæˆå’Œæ¢ç´¢")
    print("-" * 50)
    
    # åˆ›å»º4ç±»ç™Œç—‡äºšå‹æ•°æ®
    np.random.seed(42)
    
    # æ¨¡æ‹Ÿ4ç§äºšå‹çš„åŸºå› è¡¨è¾¾æ¨¡å¼
    n_samples_per_class = [120, 100, 80, 100]  # ä¸å¹³è¡¡æ•°æ®
    n_genes = 100
    
    # TODO: ä¸ºæ¯ä¸ªäºšå‹åˆ›å»ºç‰¹å¼‚æ€§è¡¨è¾¾æ¨¡å¼
    # æç¤º: æ¯ä¸ªäºšå‹åœ¨ä¸åŒåŸºå› ä¸Šæœ‰é«˜è¡¨è¾¾
    
    all_data = []
    all_labels = []
    
    for subtype_id, n_samples in enumerate(n_samples_per_class):
        # TODO: åˆ›å»ºæ¯ä¸ªäºšå‹çš„è¡¨è¾¾æ•°æ®
        # åŸºç¡€è¡¨è¾¾æ°´å¹³ + äºšå‹ç‰¹å¼‚æ€§è¡¨è¾¾ + å™ªå£°
        
        # base_expression = np.random.lognormal(3, 1, (n_samples, n_genes))
        # # æ·»åŠ äºšå‹ç‰¹å¼‚æ€§è¡¨è¾¾æ¨¡å¼
        # ...
        
        # all_data.append(subtype_expression)
        # all_labels.extend([subtype_id] * n_samples)
        pass
    
    # X = np.vstack(all_data)
    # y = np.array(all_labels)
    X, y = None, None  # è¯·å®Œæˆ
    
    # åˆ›å»ºç‰¹å¾åç§°ï¼ˆåŸºå› åï¼‰
    gene_names = [f'Gene_{i:04d}' for i in range(1, n_genes+1)]
    subtype_names = ['Luminal A', 'Luminal B', 'HER2+', 'Basal-like']
    
    if X is not None and y is not None:
        print(f"æ•°æ®é›†å¤§å°: {X.shape}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # TODO: æ­¥éª¤2 - æ•°æ®é¢„å¤„ç†
    print("\n" + "-" * 50)
    print("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†")
    print("-" * 50)
    
    # 2.1 å¤„ç†ç¼ºå¤±å€¼(æ¨¡æ‹Ÿä¸€äº›ç¼ºå¤±)
    # 2.2 å¯¹æ•°è½¬æ¢å¤„ç†åæ€åˆ†å¸ƒ
    # 2.3 æ ‡å‡†åŒ–
    # 2.4 å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    
    # TODO: æ­¥éª¤3 - æ¢ç´¢æ€§æ•°æ®åˆ†æ
    print("\n" + "-" * 50)
    print("æ­¥éª¤3: æ¢ç´¢æ€§æ•°æ®åˆ†æ") 
    print("-" * 50)
    
    # TODO: ç»˜åˆ¶å¤šä¸ªå¯è§†åŒ–å›¾è¡¨
    # 3.1 ç±»åˆ«åˆ†å¸ƒ
    # 3.2 ä¸»æˆåˆ†åˆ†æå¯è§†åŒ–
    # 3.3 å·®å¼‚è¡¨è¾¾åŸºå› çƒ­å›¾
    # 3.4 ç‰¹å¾ç›¸å…³æ€§åˆ†æ
    
    # TODO: æ­¥éª¤4 - ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©
    print("\n" + "-" * 50)
    print("æ­¥éª¤4: ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©")
    print("-" * 50)
    
    # 4.1 å•å˜é‡ç‰¹å¾é€‰æ‹©
    # 4.2 é€’å½’ç‰¹å¾æ¶ˆé™¤
    # 4.3 åŸºäºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
    # 4.4 åˆ›å»ºç»„åˆç‰¹å¾
    
    # TODO: æ­¥éª¤5 - æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
    print("\n" + "-" * 50)
    print("æ­¥éª¤5: æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–")
    print("-" * 50)
    
    # 5.1 è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹
    # 5.2 è¶…å‚æ•°è°ƒä¼˜
    # 5.3 é›†æˆå­¦ä¹ æ–¹æ³•
    # 5.4 å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    
    models_to_try = [
        'LogisticRegression',
        'RandomForest', 
        'SVM',
        'XGBoost',  # å¦‚æœå¯ç”¨
        'NeuralNetwork'
    ]
    
    # TODO: æ­¥éª¤6 - æ¨¡å‹è¯„ä¼°
    print("\n" + "-" * 50)
    print("æ­¥éª¤6: æ¨¡å‹è¯„ä¼°")
    print("-" * 50)
    
    # 6.1 äº¤å‰éªŒè¯
    # 6.2 å¤šç§è¯„ä¼°æŒ‡æ ‡
    # 6.3 æ··æ·†çŸ©é˜µ
    # 6.4 ROCæ›²çº¿å’ŒAUC
    # 6.5 ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
    
    # TODO: æ­¥éª¤7 - æ¨¡å‹è§£é‡Š
    print("\n" + "-" * 50)
    print("æ­¥éª¤7: æ¨¡å‹è§£é‡Š")
    print("-" * 50)
    
    # 7.1 ç‰¹å¾é‡è¦æ€§åˆ†æ
    # 7.2 SHAPå€¼(å¦‚æœå¯ç”¨)
    # 7.3 é”™è¯¯æ¡ˆä¾‹åˆ†æ
    # 7.4 ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š
    
    # TODO: æ­¥éª¤8 - ç»“æœæ€»ç»“å’ŒæŠ¥å‘Š
    print("\n" + "-" * 50)
    print("æ­¥éª¤8: ç»“æœæ€»ç»“")
    print("-" * 50)
    
    report = """
    é¡¹ç›®æ€»ç»“æŠ¥å‘Š:
    ==============
    
    1. æ•°æ®æ¦‚å†µ:
       - æ ·æœ¬æ•°: {samples}
       - åŸºå› æ•°: {genes}  
       - äºšå‹æ•°: {subtypes}
       - ç±»åˆ«åˆ†å¸ƒ: {distribution}
    
    2. æœ€ä½³æ¨¡å‹:
       - ç®—æ³•: {best_algorithm}
       - å‡†ç¡®ç‡: {accuracy:.3f}
       - F1åˆ†æ•°: {f1_score:.3f}
       - AUC: {auc:.3f}
    
    3. å…³é”®å‘ç°:
       - æœ€é‡è¦çš„é¢„æµ‹åŸºå› : {top_genes}
       - æœ€éš¾åŒºåˆ†çš„äºšå‹ç»„åˆ: {difficult_pairs}
       - æ¨¡å‹çš„ä¸»è¦å±€é™æ€§: {limitations}
    
    4. ä¸´åºŠæ„ä¹‰:
       - è¯¥æ¨¡å‹å¯ä»¥è¾…åŠ©{clinical_application}
       - å»ºè®®ç»“åˆ{additional_info}è¿›ä¸€æ­¥éªŒè¯
       - ä¸‹ä¸€æ­¥ç ”ç©¶æ–¹å‘: {future_work}
    
    5. æŠ€æœ¯ç»†èŠ‚:
       - ç‰¹å¾é€‰æ‹©æ–¹æ³•: {feature_selection}
       - è¶…å‚æ•°ä¼˜åŒ–: {hyperparameter_tuning}
       - äº¤å‰éªŒè¯ç­–ç•¥: {cross_validation}
    """.format(
        samples="å¾…å®Œæˆ",
        genes="å¾…å®Œæˆ", 
        subtypes="å¾…å®Œæˆ",
        distribution="å¾…å®Œæˆ",
        best_algorithm="å¾…å®Œæˆ",
        accuracy=0.0,
        f1_score=0.0,
        auc=0.0,
        top_genes="å¾…å®Œæˆ",
        difficult_pairs="å¾…å®Œæˆ", 
        limitations="å¾…å®Œæˆ",
        clinical_application="å¾…å®Œæˆ",
        additional_info="å¾…å®Œæˆ",
        future_work="å¾…å®Œæˆ",
        feature_selection="å¾…å®Œæˆ",
        hyperparameter_tuning="å¾…å®Œæˆ",
        cross_validation="å¾…å®Œæˆ"
    )
    
    print(report)
    
    print("\nâœ… ç»¼åˆé¡¹ç›®å®Œæˆï¼")
    print("\n" + "=" * 60)
    print("ğŸ‰ æ­å–œä½ å®Œæˆäº†æœºå™¨å­¦ä¹ å…¥é—¨çš„æ‰€æœ‰ç»ƒä¹ ï¼")
    print("=" * 60)
    print("""
ä½ å·²ç»æŒæ¡äº†ï¼š
âœ… æ•°æ®é¢„å¤„ç†å’Œæ¢ç´¢æ€§åˆ†æ
âœ… ç›‘ç£å­¦ä¹ åˆ†ç±»ç®—æ³•
âœ… æ— ç›‘ç£å­¦ä¹ èšç±»åˆ†æ  
âœ… æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–æŠ€æœ¯
âœ… ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©æ–¹æ³•
âœ… ç»¼åˆé¡¹ç›®å®æˆ˜ç»éªŒ

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
ğŸš€ å°è¯•çœŸå®çš„ç”Ÿç‰©æ•°æ®é›†
ğŸš€ å­¦ä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶
ğŸš€ æ¢ç´¢ä¸“ä¸šç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·
ğŸš€ å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®

è®°ä½ï¼šæœºå™¨å­¦ä¹ æ˜¯å·¥å…·ï¼Œç”Ÿç‰©å­¦çŸ¥è¯†æ˜¯çµé­‚ï¼
    """)
    
    return report


def main():
    """è¿è¡Œæ‰€æœ‰ç»ƒä¹ """
    print("="*80)
    print("ğŸ§¬ Chapter 10: æœºå™¨å­¦ä¹ å…¥é—¨ç»ƒä¹  - å¾ªåºæ¸è¿›æŒæ¡AIæŠ€èƒ½")
    print("="*80)
    print("""
ğŸ“š å­¦ä¹ è·¯å¾„è®¾è®¡ï¼š

ğŸ”° ç»ƒä¹ 1: æ•°æ®é¢„å¤„ç† (åŸºç¡€ â­)
   â€¢ æŒæ¡æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†çš„åŸºæœ¬æŠ€èƒ½
   â€¢ ç†è§£ç”Ÿç‰©æ•°æ®çš„ç‰¹ç‚¹å’Œå¤„ç†æ–¹æ³•
   â€¢ ä¸ºåç»­å»ºæ¨¡æ‰“ä¸‹åšå®åŸºç¡€

ğŸ¯ ç»ƒä¹ 2: åŸºå› åŠŸèƒ½åˆ†ç±» (è¿›é˜¶ â­â­)
   â€¢ å­¦ä¹ ç›‘ç£å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µ
   â€¢ æ¯”è¾ƒä¸åŒåˆ†ç±»ç®—æ³•çš„æ€§èƒ½
   â€¢ ç†è§£ç”Ÿç‰©ç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§

ğŸ”¬ ç»ƒä¹ 3: å•ç»†èƒèšç±»åˆ†æ (è¿›é˜¶ â­â­)
   â€¢ æ¢ç´¢æ— ç›‘ç£å­¦ä¹ çš„å¥¥ç§˜
   â€¢ å‘ç°éšè—çš„ç»†èƒäºšå‹æ¨¡å¼
   â€¢ æŒæ¡é™ç»´å’Œèšç±»æŠ€æœ¯

âš–ï¸ ç»ƒä¹ 4: æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ– (é«˜çº§ â­â­â­)
   â€¢ æ·±å…¥ç†è§£æ¨¡å‹è¯„ä¼°ä½“ç³»
   â€¢ å­¦ä¼šé˜²æ­¢è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ
   â€¢ æŒæ¡è¶…å‚æ•°è°ƒä¼˜æŠ€å·§

ğŸ† ç»ƒä¹ 5: ç»¼åˆå®æˆ˜é¡¹ç›® (ä¸“å®¶ â­â­â­â­)
   â€¢ æ•´åˆæ‰€æœ‰æŠ€èƒ½å®ŒæˆçœŸå®é¡¹ç›®
   â€¢ ä½“éªŒå®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹
   â€¢ è¾¾åˆ°ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›

å»ºè®®ï¼šæŒ‰é¡ºåºå®Œæˆï¼Œæ¯ä¸ªç»ƒä¹ éƒ½ä¸ºä¸‹ä¸€ä¸ªå¥ å®šåŸºç¡€ï¼
    """)
    
    try:
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹ä½ çš„æœºå™¨å­¦ä¹ ä¹‹æ—…...")
        print("="*60)
        
        # ç»ƒä¹ 1: æ•°æ®é¢„å¤„ç† (åŸºç¡€çº§)
        print("\nğŸ”° å‡†å¤‡è¿›å…¥ç»ƒä¹ 1: æ•°æ®é¢„å¤„ç†åŸºç¡€...")
        input("æŒ‰å›è½¦é”®ç»§ç»­ â†’ ")
        exercise_1_data_preprocessing()
        
        # ç»ƒä¹ 2: åŸºå› åŠŸèƒ½åˆ†ç±» (è¿›é˜¶çº§)
        print("\nğŸ¯ å‡†å¤‡è¿›å…¥ç»ƒä¹ 2: ç›‘ç£å­¦ä¹ è¿›é˜¶...")
        input("å®Œæˆç»ƒä¹ 1åï¼ŒæŒ‰å›è½¦ç»§ç»­ â†’ ")
        exercise_2_gene_function_classifier()
        
        # ç»ƒä¹ 3: å•ç»†èƒèšç±» (è¿›é˜¶çº§)
        print("\nğŸ”¬ å‡†å¤‡è¿›å…¥ç»ƒä¹ 3: æ— ç›‘ç£å­¦ä¹ æ¢ç´¢...")
        input("å®Œæˆç»ƒä¹ 2åï¼ŒæŒ‰å›è½¦ç»§ç»­ â†’ ")
        exercise_3_cell_clustering()
        
        # ç»ƒä¹ 4: æ¨¡å‹ä¼˜åŒ– (é«˜çº§)
        print("\nâš–ï¸ å‡†å¤‡è¿›å…¥ç»ƒä¹ 4: é«˜çº§æ¨¡å‹ä¼˜åŒ–...")
        input("å®Œæˆç»ƒä¹ 3åï¼ŒæŒ‰å›è½¦ç»§ç»­ â†’ ")
        exercise_4_model_optimization()
        
        # ç»ƒä¹ 5: ç»¼åˆé¡¹ç›® (ä¸“å®¶çº§)
        print("\nğŸ† å‡†å¤‡è¿›å…¥ç»ƒä¹ 5: ä¸“å®¶çº§ç»¼åˆå®æˆ˜...")
        input("å®Œæˆç»ƒä¹ 4åï¼ŒæŒ‰å›è½¦æŒ‘æˆ˜æœ€ç»ˆé¡¹ç›® â†’ ")
        exercise_5_cancer_subtype_project()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ­å–œï¼ä½ å·²å®Œæˆæ‰€æœ‰æœºå™¨å­¦ä¹ ç»ƒä¹ ï¼")
        print("="*80)
        print("""
ğŸ… æˆå°±è§£é”ï¼š
âœ… æ•°æ®é¢„å¤„ç†ä¸“å®¶
âœ… ç›‘ç£å­¦ä¹ å®è·µè€… 
âœ… æ— ç›‘ç£å­¦ä¹ æ¢ç´¢è€…
âœ… æ¨¡å‹ä¼˜åŒ–å¤§å¸ˆ
âœ… ç»¼åˆé¡¹ç›®å®Œæˆè€…

ä½ ç°åœ¨å·²ç»å…·å¤‡äº†ï¼š
ğŸ§  æœºå™¨å­¦ä¹ çš„ç†è®ºåŸºç¡€
ğŸ› ï¸ å®é™…é¡¹ç›®çš„å¼€å‘èƒ½åŠ›
ğŸ”¬ ç”Ÿç‰©æ•°æ®çš„åˆ†ææŠ€èƒ½
ğŸ¯ è§£å†³å®é™…é—®é¢˜çš„ç»éªŒ

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
ğŸš€ å°è¯•çœŸå®çš„ç”Ÿç‰©æ•°æ®é›†
ğŸ“š æ·±å…¥å­¦ä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶
ğŸŒŸ å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®
        """)
        
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸ ç»ƒä¹ å·²æš‚åœã€‚ä½ å¯ä»¥éšæ—¶é‡æ–°å¼€å§‹ï¼")
    except Exception as e:
        print(f"\nâŒ ç»ƒä¹ è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ä»£ç å¹¶å®ŒæˆTODOéƒ¨åˆ†")
        print("ğŸ”§ æ¯ä¸ªç»ƒä¹ éƒ½æœ‰è¯¦ç»†çš„æç¤ºå’Œæ³¨é‡Šå¸®åŠ©ä½ å®Œæˆ")


if __name__ == "__main__":
    main()