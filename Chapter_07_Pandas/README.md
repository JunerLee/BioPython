# Chapter 07: Pandasè¿›é˜¶ - è½¬å½•ç»„æ·±åº¦è§£æä¸é«˜çº§æ•°æ®åˆ†æ

## ğŸ”¬ å†™åœ¨æœ€å‰é¢ï¼šç»™è½¬å½•ç»„ç ”ç©¶è€…çš„ä¸€å°ä¿¡

äº²çˆ±çš„ç ”ç©¶è€…ï¼š

å¦‚æœä½ å·²ç»æ¥åˆ°ç¬¬7ç« ï¼Œè¯´æ˜ä½ å·²ç»æŒæ¡äº†Pandasçš„åŸºç¡€æ“ä½œï¼Œèƒ½å¤Ÿåƒä½¿ç”¨ç§»æ¶²å™¨ä¸€æ ·ç†Ÿç»ƒåœ°æ“ä½œDataFrameäº†ã€‚ç°åœ¨ï¼Œæ˜¯æ—¶å€™è¿›å…¥çœŸæ­£çš„"æ·±æ°´åŒº"äº†â€”â€”å¤„ç†çœŸå®çš„è½¬å½•ç»„æ•°æ®ï¼Œè¿›è¡Œä¸“ä¸šçº§åˆ«çš„ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æã€‚

è¿˜è®°å¾—ç¬¬6ç« ä¸­ï¼Œæˆ‘ä»¬ç”¨Pandasåˆ†æäº†ç®€å•çš„åŸºå› è¡¨è¾¾æ•°æ®å—ï¼Ÿé‚£åªæ˜¯å†°å±±ä¸€è§’ã€‚åœ¨çœŸå®çš„ç ”ç©¶ä¸­ï¼Œä½ ä¼šé¢å¯¹ï¼š
- **æµ·é‡æ•°æ®**ï¼šåŠ¨è¾„ä¸Šä¸‡ä¸ªåŸºå› ã€å‡ åä¸ªæ ·æœ¬çš„è¡¨è¾¾çŸ©é˜µ
- **å¤æ‚å®éªŒè®¾è®¡**ï¼šæ—¶é—´åºåˆ—ã€å¤šæ¡ä»¶æ¯”è¾ƒã€æ‰¹æ¬¡æ•ˆåº”
- **ä¸¥æ ¼çš„ç»Ÿè®¡è¦æ±‚**ï¼šå¤šé‡æ£€éªŒæ ¡æ­£ã€æ•ˆåº”å¤§å°è¯„ä¼°
- **ç”Ÿç‰©å­¦è§£é‡Š**ï¼šä»æ•°å­—åˆ°ç”Ÿç‰©å­¦æ„ä¹‰çš„è½¬åŒ–

è¿™ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åƒä¸€ä¸ªçœŸæ­£çš„ç”Ÿç‰©ä¿¡æ¯å­¦å®¶é‚£æ ·ï¼Œç”¨Pandaså®Œæˆä¸€ä¸ªå®Œæ•´çš„RNA-seqæ•°æ®åˆ†ææµç¨‹ã€‚ä»åŸå§‹çš„countsçŸ©é˜µï¼Œåˆ°æœ€ç»ˆçš„å·®å¼‚è¡¨è¾¾åŸºå› åˆ—è¡¨å’ŒåŠŸèƒ½å¯Œé›†ç»“æœã€‚

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹è¿™æ®µæ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼

## ğŸ“– æœ¬ç« å¯¼èˆªï¼šä»æ•°æ®åˆ°å‘ç°çš„å®Œæ•´è·¯å¾„

### ğŸ¯ ä½ å°†æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½

#### 1. **æ•°æ®å·¥ç¨‹èƒ½åŠ›** ğŸ—ï¸
- å¤šæ¥æºæ•°æ®æ•´åˆï¼ˆä¸åŒå®éªŒå®¤ã€ä¸åŒå¹³å°ï¼‰
- æ‰¹æ¬¡æ•ˆåº”è¯†åˆ«ä¸æ ¡æ­£
- æ•°æ®è´¨é‡æ§åˆ¶ä¸å¼‚å¸¸å€¼å¤„ç†
- å¤§è§„æ¨¡æ•°æ®çš„æ€§èƒ½ä¼˜åŒ–

#### 2. **ç»Ÿè®¡åˆ†æèƒ½åŠ›** ğŸ“Š
- å·®å¼‚è¡¨è¾¾åˆ†æçš„å®Œæ•´æµç¨‹
- å¤šé‡æ£€éªŒæ ¡æ­£ï¼ˆFDRã€Bonferroniï¼‰
- ç›¸å…³æ€§åˆ†æä¸èšç±»
- æ—¶é—´åºåˆ—è¡¨è¾¾æ¨¡å¼è¯†åˆ«

#### 3. **ç”Ÿç‰©å­¦æ´å¯ŸåŠ›** ğŸ§¬
- åŠŸèƒ½å¯Œé›†åˆ†æ
- é€šè·¯åˆ†æä¸ç½‘ç»œæ„å»º
- è¡¨è¾¾æ¨¡å¼çš„ç”Ÿç‰©å­¦è§£é‡Š
- ä»æ•°æ®åˆ°å‡è®¾çš„æ¨ç†

### ğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾

```
èµ·ç‚¹ï¼šåŸºç¡€Pandasæ“ä½œ
    â†“
ç¬¬ä¸€ç«™ï¼šæ•°æ®æ•´åˆä¸é‡å¡‘ â† ä½ åœ¨è¿™é‡Œ
    â€¢ mergeã€joinã€concatçš„é«˜çº§ç”¨æ³•
    â€¢ pivotã€meltã€stackçš„æ•°æ®å˜å½¢
    â€¢ å¤šçº§ç´¢å¼•çš„æ„å»ºä¸æ“ä½œ
    â†“
ç¬¬äºŒç«™ï¼šåˆ†ç»„åˆ†æä¸èšåˆ
    â€¢ groupbyçš„é«˜çº§æŠ€å·§
    â€¢ è‡ªå®šä¹‰èšåˆå‡½æ•°
    â€¢ transformä¸applyçš„åŒºåˆ«
    â†“
ç¬¬ä¸‰ç«™ï¼šç»Ÿè®¡æ£€éªŒä¸æ ¡æ­£
    â€¢ t-testã€ANOVAã€ç›¸å…³æ€§æ£€éªŒ
    â€¢ å¤šé‡æ£€éªŒæ ¡æ­£æ–¹æ³•
    â€¢ æ•ˆåº”å¤§å°çš„è¯„ä¼°
    â†“
ç¬¬å››ç«™ï¼šæ—¶é—´åºåˆ—åˆ†æ
    â€¢ è¡¨è¾¾åŠ¨æ€è¿½è¸ª
    â€¢ æ¨¡å¼è¯†åˆ«ä¸èšç±»
    â€¢ å…³é”®æ—¶é—´ç‚¹è¯†åˆ«
    â†“
ç»ˆç‚¹ï¼šå®Œæ•´çš„RNA-seqåˆ†æç®¡é“
```

## ğŸ§¬ ç”Ÿç‰©å­¦åœºæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦é«˜çº§æ•°æ®åˆ†æï¼Ÿ

### çœŸå®æ¡ˆä¾‹ï¼šç™Œç—‡è¯ç‰©å“åº”ç ”ç©¶

æƒ³è±¡ä½ æ˜¯ä¸€åç™Œç—‡ç ”ç©¶å‘˜ï¼Œæ­£åœ¨ç ”ç©¶ä¸€ç§æ–°è¯ç‰©å¯¹è‚¿ç˜¤ç»†èƒçš„å½±å“ï¼š

**å®éªŒè®¾è®¡**ï¼š
- 3ç§ç»†èƒç³»ï¼ˆæ•æ„Ÿå‹ã€è€è¯å‹ã€ä¸­é—´å‹ï¼‰
- 5ä¸ªæ—¶é—´ç‚¹ï¼ˆ0hã€2hã€6hã€12hã€24hï¼‰
- 3ä¸ªç”Ÿç‰©å­¦é‡å¤
- 20,000ä¸ªåŸºå› çš„è¡¨è¾¾æ£€æµ‹

**æ•°æ®æŒ‘æˆ˜**ï¼š
- æ•°æ®é‡ï¼š3Ã—5Ã—3Ã—20,000 = 900,000ä¸ªæ•°æ®ç‚¹
- æ‰¹æ¬¡æ•ˆåº”ï¼šä¸åŒæ—¶é—´åšçš„å®éªŒ
- æŠ€æœ¯å™ªéŸ³ï¼šæµ‹åºæ·±åº¦ä¸ä¸€è‡´
- ç”Ÿç‰©å­¦å˜å¼‚ï¼šç»†èƒç³»é—´çš„å·®å¼‚

**åˆ†æç›®æ ‡**ï¼š
1. æ‰¾å‡ºè¯ç‰©å“åº”åŸºå› 
2. è¯†åˆ«æ—©æœŸvsæ™šæœŸå“åº”
3. å‘ç°è€è¯ç›¸å…³é€šè·¯
4. é¢„æµ‹è¯ç‰©ä½œç”¨æœºåˆ¶

è¿™å°±æ˜¯æˆ‘ä»¬è¿™ä¸€ç« è¦è§£å†³çš„é—®é¢˜ç±»å‹ï¼

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ1ï¼šæ•°æ®åˆå¹¶ä¸é‡å¡‘ - åƒæ‹¼å›¾ä¸€æ ·ç»„è£…æ•°æ®

### ğŸ§© ä¸ºä»€ä¹ˆæ•°æ®æ•´åˆå¦‚æ­¤é‡è¦ï¼Ÿ

åœ¨çœŸå®ç ”ç©¶ä¸­ï¼Œä½ çš„æ•°æ®å¾€å¾€æ¥è‡ªå¤šä¸ªæ¥æºï¼š
- ä¸åŒæ‰¹æ¬¡çš„æµ‹åº
- ä¸åŒå®éªŒå®¤çš„æ•°æ®
- ä¸åŒæŠ€æœ¯å¹³å°ï¼ˆRNA-seqã€èŠ¯ç‰‡ï¼‰
- ä¸´åºŠä¿¡æ¯ä¸ç»„å­¦æ•°æ®

å°±åƒç»„è£…ä¸€ä¸ªå¤§å‹è›‹ç™½è´¨å¤åˆç‰©ï¼Œä½ éœ€è¦å°†è¿™äº›"äºšåŸº"æ­£ç¡®åœ°ç»„è£…åœ¨ä¸€èµ·ã€‚

### ğŸ“š mergeï¼šç²¾ç¡®çš„æ•°æ®è¿æ¥

```python
import pandas as pd
import numpy as np

# åœºæ™¯ï¼šæ•´åˆåŸºå› è¡¨è¾¾æ•°æ®å’ŒåŸºå› æ³¨é‡Šä¿¡æ¯
print("ğŸ”¬ æ•°æ®æ•´åˆå®æˆ˜ï¼šæ„å»ºå®Œæ•´çš„åŸºå› ä¿¡æ¯æ•°æ®åº“")
print("=" * 60)

# åˆ›å»ºåŸºå› è¡¨è¾¾æ•°æ®ï¼ˆæ¥è‡ªRNA-seqï¼‰
expression_data = pd.DataFrame({
    'gene_id': ['ENSG001', 'ENSG002', 'ENSG003', 'ENSG004', 'ENSG005'],
    'sample_A': [120.5, 45.2, 890.3, 12.1, 567.8],
    'sample_B': [115.3, 48.9, 920.1, 10.5, 580.2],
    'sample_C': [125.8, 41.7, 875.6, 13.8, 560.1]
})

# åˆ›å»ºåŸºå› æ³¨é‡Šæ•°æ®ï¼ˆæ¥è‡ªæ•°æ®åº“ï¼‰
annotation_data = pd.DataFrame({
    'gene_id': ['ENSG001', 'ENSG002', 'ENSG003', 'ENSG004', 'ENSG006'],  # æ³¨æ„ENSG006
    'gene_name': ['TP53', 'BRCA1', 'MYC', 'KRAS', 'EGFR'],
    'chromosome': ['chr17', 'chr17', 'chr8', 'chr12', 'chr7'],
    'gene_type': ['tumor_suppressor', 'tumor_suppressor', 'oncogene', 'oncogene', 'oncogene']
})

# åˆ›å»ºåŠŸèƒ½æ³¨é‡Šæ•°æ®ï¼ˆæ¥è‡ªGOæ•°æ®åº“ï¼‰
function_data = pd.DataFrame({
    'gene_id': ['ENSG001', 'ENSG002', 'ENSG003', 'ENSG005'],
    'go_term': ['apoptosis', 'DNA_repair', 'cell_cycle', 'metabolism'],
    'pathway': ['p53_signaling', 'BRCA_pathway', 'MYC_pathway', 'glucose_metabolism']
})

print("åŸå§‹æ•°æ®æ¦‚è§ˆï¼š")
print(f"è¡¨è¾¾æ•°æ®: {expression_data.shape[0]} ä¸ªåŸºå› ")
print(f"æ³¨é‡Šæ•°æ®: {annotation_data.shape[0]} ä¸ªåŸºå› ")
print(f"åŠŸèƒ½æ•°æ®: {function_data.shape[0]} ä¸ªåŸºå› ")

# ä¸åŒç±»å‹çš„åˆå¹¶ç­–ç•¥
print("\n1ï¸âƒ£ Inner Join - åªä¿ç•™æ‰€æœ‰è¡¨ä¸­éƒ½æœ‰çš„åŸºå› ")
inner_merged = expression_data.merge(annotation_data, on='gene_id', how='inner')
inner_merged = inner_merged.merge(function_data, on='gene_id', how='inner')
print(f"Inner Joinç»“æœ: {inner_merged.shape[0]} ä¸ªåŸºå› ")
print(inner_merged[['gene_id', 'gene_name', 'sample_A', 'chromosome', 'go_term']].head())

print("\n2ï¸âƒ£ Left Join - ä»¥è¡¨è¾¾æ•°æ®ä¸ºä¸»ï¼Œè¡¥å……æ³¨é‡Š")
left_merged = expression_data.merge(annotation_data, on='gene_id', how='left')
left_merged = left_merged.merge(function_data, on='gene_id', how='left')
print(f"Left Joinç»“æœ: {left_merged.shape[0]} ä¸ªåŸºå› ")
print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
print(left_merged.isnull().sum())

print("\n3ï¸âƒ£ Outer Join - ä¿ç•™æ‰€æœ‰åŸºå› ä¿¡æ¯")
outer_merged = expression_data.merge(annotation_data, on='gene_id', how='outer')
outer_merged = outer_merged.merge(function_data, on='gene_id', how='outer')
print(f"Outer Joinç»“æœ: {outer_merged.shape[0]} ä¸ªåŸºå› ")
print("å®Œæ•´æ€§æ£€æŸ¥:")
print(f"æœ‰è¡¨è¾¾æ•°æ®çš„åŸºå› : {outer_merged['sample_A'].notna().sum()}")
print(f"æœ‰æ³¨é‡Šä¿¡æ¯çš„åŸºå› : {outer_merged['gene_name'].notna().sum()}")
print(f"æœ‰åŠŸèƒ½ä¿¡æ¯çš„åŸºå› : {outer_merged['go_term'].notna().sum()}")
```

**è¾“å‡ºç»“æœ**ï¼š
```
ğŸ”¬ æ•°æ®æ•´åˆå®æˆ˜ï¼šæ„å»ºå®Œæ•´çš„åŸºå› ä¿¡æ¯æ•°æ®åº“
============================================================
åŸå§‹æ•°æ®æ¦‚è§ˆï¼š
è¡¨è¾¾æ•°æ®: 5 ä¸ªåŸºå› 
æ³¨é‡Šæ•°æ®: 5 ä¸ªåŸºå› 
åŠŸèƒ½æ•°æ®: 4 ä¸ªåŸºå› 

1ï¸âƒ£ Inner Join - åªä¿ç•™æ‰€æœ‰è¡¨ä¸­éƒ½æœ‰çš„åŸºå› 
Inner Joinç»“æœ: 3 ä¸ªåŸºå› 
   gene_id gene_name  sample_A chromosome     go_term
0  ENSG001      TP53     120.5      chr17   apoptosis
1  ENSG002     BRCA1      45.2      chr17  DNA_repair
2  ENSG003       MYC     890.3       chr8  cell_cycle

2ï¸âƒ£ Left Join - ä»¥è¡¨è¾¾æ•°æ®ä¸ºä¸»ï¼Œè¡¥å……æ³¨é‡Š
Left Joinç»“æœ: 5 ä¸ªåŸºå› 
ç¼ºå¤±å€¼ç»Ÿè®¡:
gene_id        0
sample_A       0
sample_B       0
sample_C       0
gene_name      1
chromosome     1
gene_type      1
go_term        2
pathway        2

3ï¸âƒ£ Outer Join - ä¿ç•™æ‰€æœ‰åŸºå› ä¿¡æ¯
Outer Joinç»“æœ: 6 ä¸ªåŸºå› 
å®Œæ•´æ€§æ£€æŸ¥:
æœ‰è¡¨è¾¾æ•°æ®çš„åŸºå› : 5
æœ‰æ³¨é‡Šä¿¡æ¯çš„åŸºå› : 5
æœ‰åŠŸèƒ½ä¿¡æ¯çš„åŸºå› : 4
```

### ğŸ”„ æ•°æ®é‡å¡‘ï¼šåœ¨å®½æ ¼å¼å’Œé•¿æ ¼å¼é—´è½¬æ¢

```python
print("\n" + "=" * 60)
print("ğŸ”„ æ•°æ®é‡å¡‘ï¼šé€‚åº”ä¸åŒåˆ†æéœ€æ±‚çš„æ•°æ®æ ¼å¼")
print("=" * 60)

# åˆ›å»ºå®½æ ¼å¼æ•°æ®ï¼ˆå…¸å‹çš„è¡¨è¾¾çŸ©é˜µï¼‰
wide_data = pd.DataFrame({
    'gene_id': ['GENE_001', 'GENE_002', 'GENE_003'],
    'control_1': [100, 200, 150],
    'control_2': [105, 195, 155],
    'control_3': [95, 205, 145],
    'treatment_1': [150, 180, 200],
    'treatment_2': [155, 175, 210],
    'treatment_3': [145, 185, 190]
})

print("ğŸ“Š å®½æ ¼å¼æ•°æ®ï¼ˆWide Formatï¼‰- é€‚åˆæŸ¥çœ‹:")
print(wide_data)
print(f"å½¢çŠ¶: {wide_data.shape}")

# meltï¼šä»å®½åˆ°é•¿
print("\nğŸ“ˆ ä½¿ç”¨meltè½¬æ¢ä¸ºé•¿æ ¼å¼ï¼ˆLong Formatï¼‰- é€‚åˆç»Ÿè®¡åˆ†æ:")
long_data = pd.melt(
    wide_data,
    id_vars=['gene_id'],
    var_name='sample',
    value_name='expression'
)

# æ·»åŠ å®éªŒæ¡ä»¶ä¿¡æ¯
long_data['condition'] = long_data['sample'].str.split('_').str[0]
long_data['replicate'] = long_data['sample'].str.split('_').str[1]

print(long_data.head(10))
print(f"å½¢çŠ¶: {long_data.shape}")

# ä½¿ç”¨é•¿æ ¼å¼è¿›è¡Œåˆ†ç»„åˆ†æ
print("\nğŸ“Š é•¿æ ¼å¼çš„ä¼˜åŠ¿ï¼šè½»æ¾è¿›è¡Œåˆ†ç»„ç»Ÿè®¡")
stats = long_data.groupby(['gene_id', 'condition'])['expression'].agg([
    'mean', 'std', 'min', 'max'
]).round(2)
print(stats)

# pivotï¼šä»é•¿åˆ°å®½
print("\nğŸ”„ ä½¿ç”¨pivot_tableè½¬å›å®½æ ¼å¼:")
wide_again = pd.pivot_table(
    long_data,
    index='gene_id',
    columns='sample',
    values='expression'
)
print(wide_again)

# åˆ›å»ºæ±‡æ€»é€è§†è¡¨
print("\nğŸ“Š é«˜çº§é€è§†è¡¨ï¼šå¤šç»´åº¦æ±‡æ€»")
summary_pivot = pd.pivot_table(
    long_data,
    index='gene_id',
    columns='condition',
    values='expression',
    aggfunc=['mean', 'std', 'count']
)
print(summary_pivot)
```

### ğŸ—ï¸ å¤šçº§ç´¢å¼•ï¼šå¤„ç†å¤æ‚å®éªŒè®¾è®¡

```python
print("\n" + "=" * 60)
print("ğŸ—ï¸ å¤šçº§ç´¢å¼•ï¼šä¼˜é›…åœ°å¤„ç†å¤æ‚å®éªŒè®¾è®¡")
print("=" * 60)

# åˆ›å»ºå¤šå› ç´ å®éªŒæ•°æ®
# åœºæ™¯ï¼šä¸åŒç»†èƒç³»ã€ä¸åŒå¤„ç†ã€ä¸åŒæ—¶é—´ç‚¹
np.random.seed(42)

# å®éªŒå› ç´ 
cell_lines = ['HeLa', 'MCF7', 'A549']
treatments = ['control', 'drug_A', 'drug_B']
time_points = ['0h', '6h', '24h']
genes = [f'GENE_{i:03d}' for i in range(1, 11)]

# åˆ›å»ºå¤šçº§ç´¢å¼•
index = pd.MultiIndex.from_product(
    [cell_lines, treatments, time_points],
    names=['cell_line', 'treatment', 'time']
)

# ç”Ÿæˆè¡¨è¾¾æ•°æ®
data = np.random.lognormal(8, 1.5, (27, 10))
multi_df = pd.DataFrame(data, index=index, columns=genes)

print("å¤šçº§ç´¢å¼•DataFrameç»“æ„:")
print(multi_df.head(10))
print(f"\næ•°æ®ç»´åº¦: {multi_df.shape}")
print(f"ç´¢å¼•å±‚çº§: {multi_df.index.names}")

# å¤šçº§ç´¢å¼•çš„å¼ºå¤§æŸ¥è¯¢èƒ½åŠ›
print("\nğŸ” çµæ´»çš„æ•°æ®æŸ¥è¯¢:")

print("\n1. æŸ¥è¯¢ç‰¹å®šç»†èƒç³»çš„æ‰€æœ‰æ•°æ®:")
hela_data = multi_df.loc['HeLa']
print(f"HeLaç»†èƒç³»æ•°æ®å½¢çŠ¶: {hela_data.shape}")
print(hela_data.head(3))

print("\n2. æŸ¥è¯¢ç‰¹å®šå¤„ç†åœ¨æ‰€æœ‰ç»†èƒç³»ä¸­çš„æ•ˆæœ:")
drug_a_data = multi_df.xs('drug_A', level='treatment')
print(f"Drug Aå¤„ç†æ•°æ®å½¢çŠ¶: {drug_a_data.shape}")
print(drug_a_data.head(3))

print("\n3. äº¤å‰æŸ¥è¯¢ - HeLaç»†èƒç³»drug_Aå¤„ç†24hçš„æ•°æ®:")
specific_data = multi_df.loc[('HeLa', 'drug_A', '24h')]
print(specific_data.head())

# å¤šçº§åˆ†ç»„ç»Ÿè®¡
print("\nğŸ“Š å¤šç»´åº¦ç»Ÿè®¡åˆ†æ:")
print("\næŒ‰ç»†èƒç³»å’Œå¤„ç†åˆ†ç»„ï¼Œè®¡ç®—å¹³å‡è¡¨è¾¾:")
grouped_mean = multi_df.groupby(level=['cell_line', 'treatment']).mean()
print(grouped_mean[genes[:3]].round(2))

print("\nè®¡ç®—è¯ç‰©å“åº”ï¼ˆdrug vs controlï¼‰:")
# é‡å¡‘æ•°æ®ä»¥ä¾¿è®¡ç®—fold change
control_mean = multi_df.xs('control', level='treatment').groupby(level='cell_line').mean()
drug_a_mean = multi_df.xs('drug_A', level='treatment').groupby(level='cell_line').mean()

fold_change = drug_a_mean / control_mean
print("Fold Change (Drug A / Control):")
print(fold_change[genes[:5]].round(2))
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ2ï¼šåˆ†ç»„åˆ†æGroupBy - ç”Ÿç‰©å­¦åˆ†ç»„çš„è‰ºæœ¯

### ğŸ”¬ ä¸ºä»€ä¹ˆGroupByæ˜¯è½¬å½•ç»„åˆ†æçš„æ ¸å¿ƒï¼Ÿ

åœ¨ç”Ÿç‰©å­¦ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯åœ¨æ¯”è¾ƒï¼š
- ç–¾ç—… vs å¥åº·
- å¤„ç† vs å¯¹ç…§
- ä¸åŒæ—¶é—´ç‚¹
- ä¸åŒç»„ç»‡ç±»å‹
- ä¸åŒåŸºå› å®¶æ—

GroupByå°±æ˜¯å®ç°è¿™äº›æ¯”è¾ƒçš„æ ¸å¿ƒå·¥å…·ï¼

### ğŸ“Š GroupByå®Œæ•´å·¥ä½œæµç¨‹

```python
print("\n" + "=" * 60)
print("ğŸ”¬ GroupByæ·±åº¦è§£æï¼šä»åˆ†ç»„åˆ°æ´å¯Ÿ")
print("=" * 60)

# åˆ›å»ºç»¼åˆå®éªŒæ•°æ®
np.random.seed(42)

# æ¨¡æ‹Ÿ50ä¸ªåŸºå› çš„è¡¨è¾¾æ•°æ®
n_genes = 50
gene_data = pd.DataFrame({
    'gene_id': [f'GENE_{i:03d}' for i in range(1, n_genes + 1)],
    'gene_family': np.random.choice(['kinase', 'phosphatase', 'transcription_factor', 
                                   'receptor', 'metabolic'], n_genes),
    'chromosome': np.random.choice([f'chr{i}' for i in range(1, 23)], n_genes),
    'normal_1': np.random.lognormal(7, 1, n_genes),
    'normal_2': np.random.lognormal(7, 1, n_genes),
    'normal_3': np.random.lognormal(7, 1, n_genes),
    'tumor_1': np.random.lognormal(8, 1.5, n_genes),
    'tumor_2': np.random.lognormal(8, 1.5, n_genes),
    'tumor_3': np.random.lognormal(8, 1.5, n_genes),
})

# æ·»åŠ ä¸€äº›åŸºå› åœ¨è‚¿ç˜¤ä¸­ç‰¹å¼‚æ€§ä¸Šè°ƒ
up_genes = np.random.choice(n_genes, 15, replace=False)
for col in ['tumor_1', 'tumor_2', 'tumor_3']:
    gene_data.loc[up_genes, col] *= np.random.uniform(2, 5, len(up_genes))

print("å®éªŒæ•°æ®æ¦‚è§ˆ:")
print(gene_data.head())
print(f"\næ•°æ®åŒ…å«: {n_genes} ä¸ªåŸºå› , {len(gene_data['gene_family'].unique())} ä¸ªåŸºå› å®¶æ—")

# 1. åŸºç¡€åˆ†ç»„ï¼šæŒ‰åŸºå› å®¶æ—
print("\nğŸ“Š 1. æŒ‰åŸºå› å®¶æ—åˆ†ç»„åˆ†æ:")
family_groups = gene_data.groupby('gene_family')

print("å„åŸºå› å®¶æ—çš„åŸºå› æ•°é‡:")
print(family_groups.size().sort_values(ascending=False))

print("\nå„åŸºå› å®¶æ—çš„å¹³å‡è¡¨è¾¾æ°´å¹³:")
expression_cols = ['normal_1', 'normal_2', 'normal_3', 'tumor_1', 'tumor_2', 'tumor_3']
family_expression = family_groups[expression_cols].mean()
print(family_expression.round(2))

# 2. å¤šé‡èšåˆï¼šåŒæ—¶è®¡ç®—å¤šä¸ªç»Ÿè®¡é‡
print("\nğŸ“Š 2. å¤šé‡èšåˆåˆ†æ:")
agg_funcs = {
    'normal_1': ['mean', 'std', 'min', 'max'],
    'tumor_1': ['mean', 'std', 'min', 'max']
}
family_stats = family_groups.agg(agg_funcs).round(2)
print(family_stats)

# 3. è‡ªå®šä¹‰èšåˆå‡½æ•°
print("\nğŸ“Š 3. è‡ªå®šä¹‰èšåˆå‡½æ•° - è®¡ç®—å˜å¼‚ç³»æ•°(CV):")
def calculate_cv(x):
    """è®¡ç®—å˜å¼‚ç³»æ•°ï¼šæ ‡å‡†å·®/å‡å€¼"""
    return (x.std() / x.mean()) * 100

cv_by_family = family_groups[expression_cols].agg(calculate_cv)
print("å„åŸºå› å®¶æ—çš„è¡¨è¾¾å˜å¼‚ç³»æ•°(%):")
print(cv_by_family.round(2))

# 4. Transform vs Apply
print("\nğŸ“Š 4. Transform vs Apply çš„åŒºåˆ«:")

# Transform: è¿”å›ä¸åŸæ•°æ®ç›¸åŒå¤§å°çš„ç»“æœ
print("ä½¿ç”¨transformè¿›è¡Œç»„å†…æ ‡å‡†åŒ–:")
# è®¡ç®—æ¯ä¸ªåŸºå› å®¶æ—å†…çš„z-score
zscore_normalized = gene_data.copy()
zscore_normalized[expression_cols] = family_groups[expression_cols].transform(
    lambda x: (x - x.mean()) / x.std()
)
print("æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆå‰5è¡Œï¼‰:")
print(zscore_normalized[['gene_id', 'gene_family', 'normal_1', 'tumor_1']].head().round(2))

# Apply: å¯ä»¥è¿”å›ä»»æ„ç»“æœ
print("\nä½¿ç”¨applyè¿›è¡Œå¤æ‚åˆ†æ:")
def analyze_family(group):
    """å¯¹æ¯ä¸ªåŸºå› å®¶æ—è¿›è¡Œç»¼åˆåˆ†æ"""
    result = {
        'n_genes': len(group),
        'mean_normal': group[['normal_1', 'normal_2', 'normal_3']].mean().mean(),
        'mean_tumor': group[['tumor_1', 'tumor_2', 'tumor_3']].mean().mean(),
        'fold_change': group[['tumor_1', 'tumor_2', 'tumor_3']].mean().mean() / 
                      group[['normal_1', 'normal_2', 'normal_3']].mean().mean(),
        'high_expr_genes': group[group['tumor_1'] > group['tumor_1'].quantile(0.75)]['gene_id'].tolist()[:3]
    }
    return pd.Series(result)

family_analysis = family_groups.apply(analyze_family)
print(family_analysis)

# 5. åˆ†ç»„åçš„ç­›é€‰
print("\nğŸ“Š 5. åŸºäºåˆ†ç»„ç»“æœçš„ç­›é€‰:")
print("æ‰¾å‡ºå¹³å‡è¡¨è¾¾é‡æœ€é«˜çš„åŸºå› å®¶æ—ä¸­çš„åŸºå› :")
# è®¡ç®—æ¯ä¸ªå®¶æ—çš„å¹³å‡è¡¨è¾¾
family_mean_expr = family_groups[expression_cols].mean().mean(axis=1)
top_family = family_mean_expr.idxmax()
print(f"è¡¨è¾¾é‡æœ€é«˜çš„åŸºå› å®¶æ—: {top_family}")

top_family_genes = gene_data[gene_data['gene_family'] == top_family]
print(f"è¯¥å®¶æ—åŒ…å« {len(top_family_genes)} ä¸ªåŸºå› :")
print(top_family_genes[['gene_id', 'gene_family']].head())
```

### ğŸ¯ é«˜çº§åˆ†ç»„æŠ€å·§ï¼šæ»‘åŠ¨çª—å£ä¸æ‰©å±•çª—å£

```python
print("\n" + "=" * 60)
print("ğŸ¯ é«˜çº§åˆ†ç»„æŠ€å·§ï¼šçª—å£å‡½æ•°åœ¨åŸºå› ç»„åˆ†æä¸­çš„åº”ç”¨")
print("=" * 60)

# åˆ›å»ºåŸºå› ç»„ä½ç½®æ•°æ®
# æ¨¡æ‹Ÿä¸€æ¡æŸ“è‰²ä½“ä¸Šçš„åŸºå› è¡¨è¾¾
np.random.seed(42)
n_genes = 100

genomic_data = pd.DataFrame({
    'gene_id': [f'GENE_{i:03d}' for i in range(1, n_genes + 1)],
    'position': np.sort(np.random.randint(1000000, 50000000, n_genes)),  # åŸºå› ç»„ä½ç½®
    'expression': np.random.lognormal(7, 1.5, n_genes),
    'gc_content': np.random.uniform(0.3, 0.7, n_genes),
    'gene_length': np.random.randint(1000, 10000, n_genes)
})

# åœ¨æŸäº›åŒºåŸŸåˆ›å»ºè¡¨è¾¾çƒ­ç‚¹
hotspot_start = 20000000
hotspot_end = 25000000
hotspot_genes = (genomic_data['position'] >= hotspot_start) & (genomic_data['position'] <= hotspot_end)
genomic_data.loc[hotspot_genes, 'expression'] *= np.random.uniform(2, 4, hotspot_genes.sum())

print("åŸºå› ç»„æ•°æ®æ¦‚è§ˆ:")
print(genomic_data.head(10))

# 1. æ»‘åŠ¨çª—å£ï¼šè¯†åˆ«è¡¨è¾¾çƒ­ç‚¹
print("\nğŸ“Š 1. æ»‘åŠ¨çª—å£åˆ†æ - è¯†åˆ«é«˜è¡¨è¾¾åŒºåŸŸ:")

# æŒ‰ä½ç½®æ’åº
genomic_data = genomic_data.sort_values('position')

# è®¡ç®—æ»‘åŠ¨çª—å£å¹³å‡ï¼ˆçª—å£å¤§å°=10ä¸ªåŸºå› ï¼‰
window_size = 10
genomic_data['rolling_mean'] = genomic_data['expression'].rolling(
    window=window_size, center=True
).mean()

genomic_data['rolling_std'] = genomic_data['expression'].rolling(
    window=window_size, center=True
).std()

# è¯†åˆ«é«˜è¡¨è¾¾åŒºåŸŸï¼ˆè¶…è¿‡å¹³å‡å€¼2ä¸ªæ ‡å‡†å·®ï¼‰
mean_expr = genomic_data['expression'].mean()
std_expr = genomic_data['expression'].std()
genomic_data['is_hotspot'] = genomic_data['rolling_mean'] > (mean_expr + 2 * std_expr)

print(f"è¯†åˆ«åˆ°çš„çƒ­ç‚¹åŒºåŸŸåŸºå› æ•°: {genomic_data['is_hotspot'].sum()}")
hotspot_regions = genomic_data[genomic_data['is_hotspot']]
if len(hotspot_regions) > 0:
    print(f"çƒ­ç‚¹åŒºåŸŸä½ç½®èŒƒå›´: {hotspot_regions['position'].min():.0f} - {hotspot_regions['position'].max():.0f}")
    print("çƒ­ç‚¹åŒºåŸŸåŸºå› ç¤ºä¾‹:")
    print(hotspot_regions[['gene_id', 'position', 'expression', 'rolling_mean']].head().round(2))

# 2. æ‰©å±•çª—å£ï¼šç´¯ç§¯åˆ†æ
print("\nğŸ“Š 2. æ‰©å±•çª—å£åˆ†æ - ç´¯ç§¯è¡¨è¾¾æ¨¡å¼:")

genomic_data['cumulative_mean'] = genomic_data['expression'].expanding().mean()
genomic_data['cumulative_genes'] = range(1, len(genomic_data) + 1)

print("ç´¯ç§¯å¹³å‡è¡¨è¾¾çš„å˜åŒ–:")
checkpoints = [10, 25, 50, 75, 100]
for n in checkpoints:
    if n <= len(genomic_data):
        cum_mean = genomic_data.iloc[:n]['expression'].mean()
        print(f"å‰{n}ä¸ªåŸºå› çš„å¹³å‡è¡¨è¾¾: {cum_mean:.2f}")

# 3. åŸºäºä½ç½®çš„åˆ†ç»„ï¼ˆå°†æŸ“è‰²ä½“åˆ†æˆbinsï¼‰
print("\nğŸ“Š 3. åŸºå› ç»„åŒºåŸŸåˆ†ç»„åˆ†æ:")

# å°†æŸ“è‰²ä½“åˆ†æˆ10ä¸ªåŒºåŸŸ
n_bins = 10
genomic_data['region'] = pd.cut(
    genomic_data['position'], 
    bins=n_bins, 
    labels=[f'Region_{i+1}' for i in range(n_bins)]
)

region_stats = genomic_data.groupby('region').agg({
    'expression': ['mean', 'std', 'count'],
    'gc_content': 'mean',
    'gene_length': 'mean'
}).round(2)

print("å„æŸ“è‰²ä½“åŒºåŸŸçš„ç»Ÿè®¡:")
print(region_stats)

# æ‰¾å‡ºè¡¨è¾¾é‡æœ€é«˜çš„åŒºåŸŸ
region_mean_expr = genomic_data.groupby('region')['expression'].mean()
top_region = region_mean_expr.idxmax()
print(f"\nè¡¨è¾¾é‡æœ€é«˜çš„åŒºåŸŸ: {top_region}")
print(f"è¯¥åŒºåŸŸå¹³å‡è¡¨è¾¾é‡: {region_mean_expr[top_region]:.2f}")
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ3ï¼šæ—¶é—´åºåˆ—åˆ†æ - è¿½è¸ªåŸºå› è¡¨è¾¾çš„åŠ¨æ€å˜åŒ–

### â° ä¸ºä»€ä¹ˆæ—¶é—´åºåˆ—åˆ†æåœ¨ç”Ÿç‰©å­¦ä¸­è‡³å…³é‡è¦ï¼Ÿ

ç”Ÿå‘½æ˜¯åŠ¨æ€çš„ï¼š
- **å‘è‚²è¿‡ç¨‹**ï¼šä»å—ç²¾åµåˆ°æˆä½“çš„åŸºå› è¡¨è¾¾å˜åŒ–
- **æ˜¼å¤œèŠ‚å¾‹**ï¼š24å°æ—¶å‘¨æœŸçš„åŸºå› è¡¨è¾¾æŒ¯è¡
- **è¯ç‰©å“åº”**ï¼šç»™è¯åä¸åŒæ—¶é—´ç‚¹çš„è¡¨è¾¾å˜åŒ–
- **ç–¾ç—…è¿›ç¨‹**ï¼šä»å¥åº·åˆ°ç–¾ç—…çš„æ¸è¿›å˜åŒ–

### ğŸ“ˆ æ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹æ®Šå¤„ç†

```python
print("\n" + "=" * 60)
print("â° æ—¶é—´åºåˆ—åˆ†æï¼šæ•æ‰åŸºå› è¡¨è¾¾çš„åŠ¨æ€å˜åŒ–")
print("=" * 60)

# åˆ›å»ºè¯ç‰©å¤„ç†çš„æ—¶é—´åºåˆ—æ•°æ®
np.random.seed(42)

# å®éªŒè®¾è®¡ï¼šè¯ç‰©å¤„ç†åçš„å¤šä¸ªæ—¶é—´ç‚¹
time_points = [0, 1, 2, 4, 6, 8, 12, 24, 48]  # å°æ—¶
n_genes = 100

# åˆ›å»ºä¸åŒå“åº”æ¨¡å¼çš„åŸºå› 
def generate_expression_pattern(pattern_type, time_points):
    """ç”Ÿæˆä¸åŒçš„è¡¨è¾¾æ¨¡å¼"""
    t = np.array(time_points)
    
    if pattern_type == 'immediate_up':
        # ç«‹å³ä¸Šè°ƒï¼Œç„¶åç»´æŒ
        expr = 100 + 50 * (1 - np.exp(-t/2))
    elif pattern_type == 'immediate_down':
        # ç«‹å³ä¸‹è°ƒ
        expr = 100 * np.exp(-t/5)
    elif pattern_type == 'delayed_up':
        # å»¶è¿Ÿä¸Šè°ƒ
        expr = 100 + 50 * (1 - np.exp(-(t-4)/3)) * (t > 4)
    elif pattern_type == 'transient':
        # ç¬æ—¶å“åº”
        expr = 100 + 80 * np.exp(-(t-4)**2/8) 
    elif pattern_type == 'oscillating':
        # æŒ¯è¡æ¨¡å¼
        expr = 100 + 30 * np.sin(t * np.pi / 12)
    else:  # stable
        # ç¨³å®šè¡¨è¾¾
        expr = 100 + np.random.normal(0, 5, len(t))
    
    # æ·»åŠ å™ªå£°
    expr = expr * np.random.uniform(0.9, 1.1, len(t))
    return expr

# ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
patterns = ['immediate_up', 'immediate_down', 'delayed_up', 
           'transient', 'oscillating', 'stable']
pattern_distribution = np.random.choice(patterns, n_genes, 
                                      p=[0.2, 0.2, 0.15, 0.15, 0.1, 0.2])

time_series_data = []
for i in range(n_genes):
    gene_expr = generate_expression_pattern(pattern_distribution[i], time_points)
    time_series_data.append(gene_expr)

# åˆ›å»ºDataFrame
columns = [f'T{t}h' for t in time_points]
ts_df = pd.DataFrame(time_series_data, columns=columns)
ts_df['gene_id'] = [f'GENE_{i:03d}' for i in range(1, n_genes + 1)]
ts_df['pattern'] = pattern_distribution

# é‡æ–°æ’åˆ—åˆ—
ts_df = ts_df[['gene_id', 'pattern'] + columns]

print("æ—¶é—´åºåˆ—æ•°æ®ç»“æ„:")
print(ts_df.head(10).round(2))

# 1. è¯†åˆ«ä¸åŒçš„è¡¨è¾¾æ¨¡å¼
print("\nğŸ“Š 1. è¡¨è¾¾æ¨¡å¼åˆ†ç±»:")
pattern_counts = ts_df['pattern'].value_counts()
print(pattern_counts)

# 2. è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾
print("\nğŸ“Š 2. æ—¶é—´åºåˆ—ç‰¹å¾æå–:")

# æœ€å¤§å˜åŒ–æ—¶é—´ç‚¹
def find_max_change_time(row):
    """æ‰¾å‡ºæœ€å¤§å˜åŒ–çš„æ—¶é—´ç‚¹"""
    values = row[columns].values
    changes = np.abs(np.diff(values))
    if len(changes) > 0:
        max_change_idx = np.argmax(changes)
        return time_points[max_change_idx + 1]
    return 0

ts_df['max_change_time'] = ts_df.apply(find_max_change_time, axis=1)

# æ€»å˜åŒ–å¹…åº¦
ts_df['total_change'] = ts_df[columns].max(axis=1) - ts_df[columns].min(axis=1)

# å˜åŒ–æ–¹å‘
ts_df['change_direction'] = (ts_df['T48h'] - ts_df['T0h']).apply(
    lambda x: 'up' if x > 10 else ('down' if x < -10 else 'stable')
)

print("æ—¶é—´åºåˆ—ç‰¹å¾ç»Ÿè®¡:")
print(ts_df.groupby('pattern').agg({
    'max_change_time': 'mean',
    'total_change': 'mean',
    'change_direction': lambda x: x.value_counts().to_dict()
}).round(2))

# 3. ç›¸å…³æ€§åˆ†æ - æ‰¾å‡ºç›¸ä¼¼è¡¨è¾¾æ¨¡å¼çš„åŸºå› 
print("\nğŸ“Š 3. è¡¨è¾¾æ¨¡å¼ç›¸å…³æ€§åˆ†æ:")

# è®¡ç®—æ‰€æœ‰åŸºå› é—´çš„ç›¸å…³æ€§
correlation_matrix = ts_df[columns].T.corr()

# æ‰¾å‡ºé«˜åº¦ç›¸å…³çš„åŸºå› å¯¹
high_corr_pairs = []
for i in range(len(correlation_matrix)):
    for j in range(i+1, len(correlation_matrix)):
        corr = correlation_matrix.iloc[i, j]
        if abs(corr) > 0.9:
            high_corr_pairs.append({
                'gene1': ts_df.iloc[i]['gene_id'],
                'gene2': ts_df.iloc[j]['gene_id'],
                'correlation': corr,
                'pattern1': ts_df.iloc[i]['pattern'],
                'pattern2': ts_df.iloc[j]['pattern']
            })

if high_corr_pairs:
    corr_df = pd.DataFrame(high_corr_pairs).head(10)
    print("é«˜åº¦ç›¸å…³çš„åŸºå› å¯¹ï¼ˆ|r| > 0.9ï¼‰:")
    print(corr_df.round(3))
    
    # ç»Ÿè®¡ç›¸åŒæ¨¡å¼çš„ç›¸å…³æ€§
    same_pattern = corr_df[corr_df['pattern1'] == corr_df['pattern2']]
    if len(same_pattern) > 0:
        print(f"\nç›¸åŒæ¨¡å¼å†…çš„å¹³å‡ç›¸å…³æ€§: {same_pattern['correlation'].mean():.3f}")

# 4. æ—¶é—´ç‚¹ä¹‹é—´çš„è¡¨è¾¾å˜åŒ–
print("\nğŸ“Š 4. å…³é”®æ—¶é—´ç‚¹åˆ†æ:")

# è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹ç›¸å¯¹äºT0çš„fold change
for t in time_points[1:]:
    col_name = f'T{t}h'
    fc_name = f'FC_{t}h'
    ts_df[fc_name] = ts_df[col_name] / (ts_df['T0h'] + 1)  # åŠ 1é¿å…é™¤é›¶

# æ‰¾å‡ºæ—©æœŸå“åº”åŸºå› ï¼ˆ1-2å°æ—¶å†…å˜åŒ–>1.5å€ï¼‰
early_response = ts_df[(ts_df['FC_1h'] > 1.5) | (ts_df['FC_1h'] < 0.67) |
                       (ts_df['FC_2h'] > 1.5) | (ts_df['FC_2h'] < 0.67)]
print(f"æ—©æœŸå“åº”åŸºå› æ•°: {len(early_response)}")

# æ‰¾å‡ºæ™šæœŸå“åº”åŸºå› ï¼ˆ24å°æ—¶åæ‰æœ‰æ˜¾è‘—å˜åŒ–ï¼‰
late_response = ts_df[(ts_df['FC_24h'] > 1.5) | (ts_df['FC_24h'] < 0.67)]
late_response = late_response[(late_response['FC_4h'] < 1.2) & 
                             (late_response['FC_4h'] > 0.83)]
print(f"æ™šæœŸå“åº”åŸºå› æ•°: {len(late_response)}")

# 5. èšç±»åˆ†æ - åŸºäºè¡¨è¾¾æ¨¡å¼åˆ†ç»„
print("\nğŸ“Š 5. è¡¨è¾¾æ¨¡å¼èšç±»:")

from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

# è®¡ç®—è·ç¦»çŸ©é˜µ
expression_matrix = ts_df[columns].values
distances = pdist(expression_matrix, metric='correlation')

# è¿›è¡Œå±‚æ¬¡èšç±»
linkage_matrix = linkage(distances, method='average')

# æ ¹æ®èšç±»ç»“æœåˆ†ç»„ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
from scipy.cluster.hierarchy import fcluster
clusters = fcluster(linkage_matrix, t=0.5, criterion='distance')
ts_df['cluster'] = clusters

print("èšç±»ç»“æœç»Ÿè®¡:")
cluster_stats = ts_df.groupby('cluster').agg({
    'gene_id': 'count',
    'pattern': lambda x: x.value_counts().to_dict()
})
cluster_stats.columns = ['n_genes', 'pattern_distribution']
print(cluster_stats)
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ4ï¼šå·®å¼‚è¡¨è¾¾åˆ†æ - ä»æ•°æ®åˆ°ç”Ÿç‰©å­¦å‘ç°

### ğŸ”¬ å®Œæ•´çš„å·®å¼‚è¡¨è¾¾åˆ†ææµç¨‹

å·®å¼‚è¡¨è¾¾åˆ†ææ˜¯RNA-seqæ•°æ®åˆ†æçš„æ ¸å¿ƒï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£æ¯ä¸€æ­¥ï¼š

```python
print("\n" + "=" * 60)
print("ğŸ§¬ å·®å¼‚è¡¨è¾¾åˆ†æï¼šå®Œæ•´çš„ä¸“ä¸šæµç¨‹")
print("=" * 60)

# åˆ›å»ºæ¨¡æ‹Ÿçš„RNA-seq countæ•°æ®
np.random.seed(42)

# å®éªŒè®¾è®¡ï¼š6ä¸ªæ ·æœ¬ï¼ˆ3ä¸ªå¯¹ç…§ï¼Œ3ä¸ªå¤„ç†ï¼‰
n_genes = 1000
samples = ['Control_1', 'Control_2', 'Control_3', 
          'Treatment_1', 'Treatment_2', 'Treatment_3']

# ç”ŸæˆåŸºç¡€è¡¨è¾¾æ°´å¹³ï¼ˆæ¨¡æ‹ŸçœŸå®çš„countåˆ†å¸ƒï¼‰
base_expression = np.random.lognormal(8, 2, n_genes)

# åˆ›å»ºcountçŸ©é˜µ
count_matrix = pd.DataFrame(index=[f'GENE_{i:04d}' for i in range(1, n_genes + 1)],
                           columns=samples)

# ç”Ÿæˆå¯¹ç…§ç»„æ•°æ®
for sample in ['Control_1', 'Control_2', 'Control_3']:
    # æ·»åŠ ç”Ÿç‰©å­¦å˜å¼‚
    bio_variation = np.random.normal(1, 0.1, n_genes)
    count_matrix[sample] = np.random.poisson(base_expression * bio_variation)

# ç”Ÿæˆå¤„ç†ç»„æ•°æ®ï¼ŒåŒ…å«å·®å¼‚è¡¨è¾¾åŸºå› 
treatment_expression = base_expression.copy()

# 30%åŸºå› ä¸Šè°ƒï¼ˆæ¨¡æ‹Ÿæ¿€æ´»çš„é€šè·¯ï¼‰
up_genes = np.random.choice(n_genes, int(n_genes * 0.15), replace=False)
treatment_expression[up_genes] *= np.random.uniform(2, 10, len(up_genes))

# 30%åŸºå› ä¸‹è°ƒï¼ˆæ¨¡æ‹ŸæŠ‘åˆ¶çš„é€šè·¯ï¼‰
down_genes = np.random.choice(
    [i for i in range(n_genes) if i not in up_genes],
    int(n_genes * 0.15),
    replace=False
)
treatment_expression[down_genes] *= np.random.uniform(0.1, 0.5, len(down_genes))

# ç”Ÿæˆå¤„ç†ç»„æ•°æ®
for sample in ['Treatment_1', 'Treatment_2', 'Treatment_3']:
    bio_variation = np.random.normal(1, 0.1, n_genes)
    count_matrix[sample] = np.random.poisson(treatment_expression * bio_variation)

print("åŸå§‹CountçŸ©é˜µæ¦‚è§ˆ:")
print(count_matrix.head())
print(f"çŸ©é˜µå¤§å°: {count_matrix.shape}")

# Step 1: æ•°æ®è´¨é‡æ§åˆ¶
print("\n" + "=" * 40)
print("Step 1: æ•°æ®è´¨é‡æ§åˆ¶")
print("=" * 40)

# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„åŸºæœ¬ç»Ÿè®¡
sample_stats = pd.DataFrame({
    'total_reads': count_matrix.sum(),
    'detected_genes': (count_matrix > 0).sum(),
    'mean_expression': count_matrix.mean(),
    'median_expression': count_matrix.median()
})

print("æ ·æœ¬è´¨é‡ç»Ÿè®¡:")
print(sample_stats.round(2))

# è¿‡æ»¤ä½è¡¨è¾¾åŸºå› ï¼ˆè‡³å°‘åœ¨3ä¸ªæ ·æœ¬ä¸­è¡¨è¾¾é‡>10ï¼‰
min_count = 10
min_samples = 3
expressed_genes = (count_matrix > min_count).sum(axis=1) >= min_samples
filtered_counts = count_matrix[expressed_genes]

print(f"\nè¿‡æ»¤å‰åŸºå› æ•°: {len(count_matrix)}")
print(f"è¿‡æ»¤ååŸºå› æ•°: {len(filtered_counts)}")
print(f"è¿‡æ»¤æ‰çš„åŸºå› æ•°: {len(count_matrix) - len(filtered_counts)}")

# Step 2: æ•°æ®æ ‡å‡†åŒ–
print("\n" + "=" * 40)
print("Step 2: æ•°æ®æ ‡å‡†åŒ–ï¼ˆTPMæ ‡å‡†åŒ–ï¼‰")
print("=" * 40)

# ç®€åŒ–çš„TPMæ ‡å‡†åŒ–ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦åŸºå› é•¿åº¦ä¿¡æ¯ï¼‰
def calculate_tpm(counts):
    """è®¡ç®—TPM (Transcripts Per Million)"""
    # è¿™é‡Œå‡è®¾æ‰€æœ‰åŸºå› é•¿åº¦ç›¸åŒï¼Œå®é™…åº”ç”¨éœ€è¦çœŸå®çš„åŸºå› é•¿åº¦
    rpk = counts / 1  # å‡è®¾åŸºå› é•¿åº¦ä¸º1kb
    scaling_factor = rpk.sum() / 1e6
    tpm = rpk / scaling_factor
    return tpm

tpm_matrix = filtered_counts.apply(calculate_tpm, axis=0)

print("TPMæ ‡å‡†åŒ–åçš„æ•°æ®:")
print(tpm_matrix.head().round(2))

# Step 3: ç»Ÿè®¡æ£€éªŒ
print("\n" + "=" * 40)
print("Step 3: å·®å¼‚è¡¨è¾¾ç»Ÿè®¡æ£€éªŒ")
print("=" * 40)

from scipy import stats

results = []
for gene in filtered_counts.index:
    control_values = filtered_counts.loc[gene, ['Control_1', 'Control_2', 'Control_3']].values
    treatment_values = filtered_counts.loc[gene, ['Treatment_1', 'Treatment_2', 'Treatment_3']].values
    
    # è®¡ç®—meanå’Œfold change
    control_mean = np.mean(control_values)
    treatment_mean = np.mean(treatment_values)
    fold_change = treatment_mean / (control_mean + 1)  # åŠ 1é¿å…é™¤é›¶
    log2_fc = np.log2(fold_change + 0.001)  # åŠ å°å€¼é¿å…log(0)
    
    # tæ£€éªŒ
    t_stat, p_value = stats.ttest_ind(control_values, treatment_values)
    
    # Mann-Whitney Uæ£€éªŒï¼ˆéå‚æ•°æ£€éªŒï¼Œæ›´ç¨³å¥ï¼‰
    u_stat, u_pvalue = stats.mannwhitneyu(control_values, treatment_values)
    
    results.append({
        'gene': gene,
        'control_mean': control_mean,
        'treatment_mean': treatment_mean,
        'fold_change': fold_change,
        'log2_fc': log2_fc,
        't_pvalue': p_value,
        'u_pvalue': u_pvalue
    })

de_results = pd.DataFrame(results)

# Step 4: å¤šé‡æ£€éªŒæ ¡æ­£
print("\n" + "=" * 40)
print("Step 4: å¤šé‡æ£€éªŒæ ¡æ­£")
print("=" * 40)

from statsmodels.stats.multitest import multipletests

# FDRæ ¡æ­£ï¼ˆBenjamini-Hochbergæ–¹æ³•ï¼‰
_, fdr_bh, _, _ = multipletests(de_results['t_pvalue'].values, method='fdr_bh')
de_results['fdr_bh'] = fdr_bh

# Bonferroniæ ¡æ­£ï¼ˆæ›´ä¸¥æ ¼ï¼‰
_, bonferroni, _, _ = multipletests(de_results['t_pvalue'].values, method='bonferroni')
de_results['bonferroni'] = bonferroni

print("å¤šé‡æ£€éªŒæ ¡æ­£ç»“æœå¯¹æ¯”:")
print(f"åŸå§‹på€¼ < 0.05çš„åŸºå› æ•°: {(de_results['t_pvalue'] < 0.05).sum()}")
print(f"FDR < 0.05çš„åŸºå› æ•°: {(de_results['fdr_bh'] < 0.05).sum()}")
print(f"Bonferroni < 0.05çš„åŸºå› æ•°: {(de_results['bonferroni'] < 0.05).sum()}")

# Step 5: ç­›é€‰æ˜¾è‘—å·®å¼‚è¡¨è¾¾åŸºå› 
print("\n" + "=" * 40)
print("Step 5: ç­›é€‰æ˜¾è‘—å·®å¼‚è¡¨è¾¾åŸºå› ")
print("=" * 40)

# è®¾ç½®ç­›é€‰é˜ˆå€¼
fdr_threshold = 0.05
fc_threshold = 2  # fold changeé˜ˆå€¼

# ç­›é€‰æ˜¾è‘—åŸºå› 
sig_genes = de_results[
    (de_results['fdr_bh'] < fdr_threshold) & 
    (np.abs(de_results['log2_fc']) > np.log2(fc_threshold))
].copy()

# åˆ†ç±»
sig_genes['direction'] = sig_genes['log2_fc'].apply(
    lambda x: 'UP' if x > 0 else 'DOWN'
)

print(f"æ˜¾è‘—å·®å¼‚è¡¨è¾¾åŸºå› ç»Ÿè®¡:")
print(f"æ€»è®¡: {len(sig_genes)} ä¸ªåŸºå› ")
print(f"ä¸Šè°ƒ: {(sig_genes['direction'] == 'UP').sum()} ä¸ªåŸºå› ")
print(f"ä¸‹è°ƒ: {(sig_genes['direction'] == 'DOWN').sum()} ä¸ªåŸºå› ")

# å±•ç¤ºtopåŸºå› 
print("\nğŸ”¥ Top 10 ä¸Šè°ƒåŸºå› :")
top_up = sig_genes[sig_genes['direction'] == 'UP'].nlargest(10, 'log2_fc')
print(top_up[['gene', 'fold_change', 'log2_fc', 'fdr_bh']].round(4))

print("\nâ„ï¸ Top 10 ä¸‹è°ƒåŸºå› :")
top_down = sig_genes[sig_genes['direction'] == 'DOWN'].nsmallest(10, 'log2_fc')
print(top_down[['gene', 'fold_change', 'log2_fc', 'fdr_bh']].round(4))

# Step 6: ç«å±±å›¾æ•°æ®å‡†å¤‡
print("\n" + "=" * 40)
print("Step 6: å¯è§†åŒ–å‡†å¤‡ï¼ˆç«å±±å›¾æ•°æ®ï¼‰")
print("=" * 40)

# æ·»åŠ æ˜¾è‘—æ€§æ ‡ç­¾
de_results['significance'] = 'Not Significant'
de_results.loc[
    (de_results['fdr_bh'] < fdr_threshold) & 
    (de_results['log2_fc'] > np.log2(fc_threshold)), 
    'significance'
] = 'Up-regulated'
de_results.loc[
    (de_results['fdr_bh'] < fdr_threshold) & 
    (de_results['log2_fc'] < -np.log2(fc_threshold)), 
    'significance'
] = 'Down-regulated'

# æ·»åŠ -log10(p-value)ç”¨äºç«å±±å›¾
de_results['neg_log10_pvalue'] = -np.log10(de_results['t_pvalue'] + 1e-300)

print("ç«å±±å›¾æ•°æ®å‡†å¤‡å®Œæˆ:")
print(de_results.groupby('significance').size())
print("\næ•°æ®ç¤ºä¾‹ï¼ˆç”¨äºç«å±±å›¾ï¼‰:")
print(de_results[['gene', 'log2_fc', 'neg_log10_pvalue', 'significance']].head())
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ5ï¼šç»Ÿè®¡æ£€éªŒæ–¹æ³•è¯¦è§£

### ğŸ“Š é€‰æ‹©æ­£ç¡®çš„ç»Ÿè®¡æ£€éªŒ

```python
print("\n" + "=" * 60)
print("ğŸ“Š ç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼šé€‰æ‹©æ­£ç¡®çš„å·¥å…·")
print("=" * 60)

# åˆ›å»ºä¸åŒåˆ†å¸ƒçš„æ•°æ®ç”¨äºæ¼”ç¤º
np.random.seed(42)
n_samples = 100

# 1. æ­£æ€åˆ†å¸ƒæ•°æ®ï¼ˆé€‚åˆtæ£€éªŒï¼‰
normal_control = np.random.normal(100, 15, n_samples)
normal_treatment = np.random.normal(110, 15, n_samples)

# 2. åæ€åˆ†å¸ƒæ•°æ®ï¼ˆé€‚åˆéå‚æ•°æ£€éªŒï¼‰
skewed_control = np.random.lognormal(4, 0.5, n_samples)
skewed_treatment = np.random.lognormal(4.2, 0.5, n_samples)

# 3. é…å¯¹æ•°æ®ï¼ˆåŒä¸€æ ·æœ¬çš„å‰åå¯¹æ¯”ï¼‰
paired_before = np.random.normal(100, 10, n_samples)
paired_after = paired_before + np.random.normal(5, 5, n_samples)  # ç›¸å…³çš„å˜åŒ–

print("ğŸ”¬ åœºæ™¯1ï¼šä¸¤ç»„ç‹¬ç«‹æ ·æœ¬çš„æ¯”è¾ƒ")
print("-" * 40)

# tæ£€éªŒï¼ˆå‚æ•°æ£€éªŒï¼‰
t_stat, t_pval = stats.ttest_ind(normal_control, normal_treatment)
print(f"Student's t-test:")
print(f"  tç»Ÿè®¡é‡: {t_stat:.4f}")
print(f"  på€¼: {t_pval:.4f}")
print(f"  ç»“è®º: {'æ˜¾è‘—å·®å¼‚' if t_pval < 0.05 else 'æ— æ˜¾è‘—å·®å¼‚'}")

# Welch's tæ£€éªŒï¼ˆæ–¹å·®ä¸ç­‰ï¼‰
welch_stat, welch_pval = stats.ttest_ind(normal_control, normal_treatment, equal_var=False)
print(f"\nWelch's t-testï¼ˆæ–¹å·®ä¸ç­‰ï¼‰:")
print(f"  tç»Ÿè®¡é‡: {welch_stat:.4f}")
print(f"  på€¼: {welch_pval:.4f}")

# Mann-Whitney Uæ£€éªŒï¼ˆéå‚æ•°ï¼‰
u_stat, u_pval = stats.mannwhitneyu(skewed_control, skewed_treatment)
print(f"\nMann-Whitney U testï¼ˆéå‚æ•°ï¼‰:")
print(f"  Uç»Ÿè®¡é‡: {u_stat:.4f}")
print(f"  på€¼: {u_pval:.4f}")
print(f"  é€‚ç”¨äº: éæ­£æ€åˆ†å¸ƒæ•°æ®")

print("\nğŸ”¬ åœºæ™¯2ï¼šé…å¯¹æ ·æœ¬çš„æ¯”è¾ƒ")
print("-" * 40)

# é…å¯¹tæ£€éªŒ
paired_t_stat, paired_t_pval = stats.ttest_rel(paired_before, paired_after)
print(f"é…å¯¹tæ£€éªŒ:")
print(f"  tç»Ÿè®¡é‡: {paired_t_stat:.4f}")
print(f"  på€¼: {paired_t_pval:.4f}")
print(f"  å¹³å‡å·®å¼‚: {np.mean(paired_after - paired_before):.2f}")

# Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆéå‚æ•°é…å¯¹æ£€éªŒï¼‰
wilcox_stat, wilcox_pval = stats.wilcoxon(paired_before, paired_after)
print(f"\nWilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆéå‚æ•°ï¼‰:")
print(f"  ç»Ÿè®¡é‡: {wilcox_stat:.4f}")
print(f"  på€¼: {wilcox_pval:.4f}")

print("\nğŸ”¬ åœºæ™¯3ï¼šå¤šç»„æ¯”è¾ƒï¼ˆANOVAï¼‰")
print("-" * 40)

# åˆ›å»ºä¸‰ç»„æ•°æ®
group1 = np.random.normal(100, 10, 50)
group2 = np.random.normal(105, 10, 50)
group3 = np.random.normal(110, 10, 50)

# å•å› ç´ æ–¹å·®åˆ†æ
f_stat, anova_pval = stats.f_oneway(group1, group2, group3)
print(f"å•å› ç´ ANOVA:")
print(f"  Fç»Ÿè®¡é‡: {f_stat:.4f}")
print(f"  på€¼: {anova_pval:.4f}")
print(f"  ç»“è®º: {'ç»„é—´æœ‰æ˜¾è‘—å·®å¼‚' if anova_pval < 0.05 else 'ç»„é—´æ— æ˜¾è‘—å·®å¼‚'}")

# Kruskal-Wallisæ£€éªŒï¼ˆéå‚æ•°ANOVAï¼‰
kw_stat, kw_pval = stats.kruskal(group1, group2, group3)
print(f"\nKruskal-Wallisæ£€éªŒï¼ˆéå‚æ•°ï¼‰:")
print(f"  Hç»Ÿè®¡é‡: {kw_stat:.4f}")
print(f"  på€¼: {kw_pval:.4f}")

print("\nğŸ”¬ åœºæ™¯4ï¼šç›¸å…³æ€§åˆ†æ")
print("-" * 40)

# åˆ›å»ºç›¸å…³çš„æ•°æ®
x = np.random.normal(0, 1, 100)
y_linear = 2 * x + np.random.normal(0, 0.5, 100)  # çº¿æ€§ç›¸å…³
y_nonlinear = x**2 + np.random.normal(0, 0.5, 100)  # éçº¿æ€§ç›¸å…³

# Pearsonç›¸å…³ï¼ˆçº¿æ€§ç›¸å…³ï¼‰
pearson_r, pearson_pval = stats.pearsonr(x, y_linear)
print(f"Pearsonç›¸å…³ç³»æ•°:")
print(f"  r = {pearson_r:.4f}")
print(f"  på€¼ = {pearson_pval:.4f}")
print(f"  è§£é‡Š: {'å¼º' if abs(pearson_r) > 0.7 else 'ä¸­ç­‰' if abs(pearson_r) > 0.3 else 'å¼±'}ç›¸å…³")

# Spearmanç›¸å…³ï¼ˆç§©ç›¸å…³ï¼Œå¯¹éçº¿æ€§æ•æ„Ÿï¼‰
spearman_r, spearman_pval = stats.spearmanr(x, y_nonlinear)
print(f"\nSpearmanç§©ç›¸å…³ç³»æ•°:")
print(f"  Ï = {spearman_r:.4f}")
print(f"  på€¼ = {spearman_pval:.4f}")

# å†³ç­–æ ‘ï¼šå¦‚ä½•é€‰æ‹©ç»Ÿè®¡æ£€éªŒ
print("\n" + "=" * 60)
print("ğŸ“‹ ç»Ÿè®¡æ£€éªŒé€‰æ‹©æŒ‡å—")
print("=" * 60)

decision_guide = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ç»Ÿè®¡æ£€éªŒé€‰æ‹©å†³ç­–æ ‘                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘ 1. æ¯”è¾ƒä¸¤ç»„ï¼Ÿ                                                â•‘
â•‘    â”œâ”€ ç‹¬ç«‹æ ·æœ¬ï¼Ÿ                                             â•‘
â•‘    â”‚   â”œâ”€ æ­£æ€åˆ†å¸ƒï¼Ÿ â†’ tæ£€éªŒ                                 â•‘
â•‘    â”‚   â””â”€ éæ­£æ€ï¼Ÿ â†’ Mann-Whitney Uæ£€éªŒ                      â•‘
â•‘    â””â”€ é…å¯¹æ ·æœ¬ï¼Ÿ                                             â•‘
â•‘        â”œâ”€ æ­£æ€åˆ†å¸ƒï¼Ÿ â†’ é…å¯¹tæ£€éªŒ                             â•‘
â•‘        â””â”€ éæ­£æ€ï¼Ÿ â†’ Wilcoxonç¬¦å·ç§©æ£€éªŒ                      â•‘
â•‘                                                              â•‘
â•‘ 2. æ¯”è¾ƒå¤šç»„ï¼Ÿ                                                â•‘
â•‘    â”œâ”€ æ­£æ€åˆ†å¸ƒï¼Ÿ â†’ ANOVA                                     â•‘
â•‘    â””â”€ éæ­£æ€ï¼Ÿ â†’ Kruskal-Wallisæ£€éªŒ                          â•‘
â•‘                                                              â•‘
â•‘ 3. ç›¸å…³æ€§åˆ†æï¼Ÿ                                              â•‘
â•‘    â”œâ”€ çº¿æ€§å…³ç³»ï¼Ÿ â†’ Pearsonç›¸å…³                               â•‘
â•‘    â””â”€ éçº¿æ€§æˆ–åºæ•°ï¼Ÿ â†’ Spearmanç›¸å…³                          â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
print(decision_guide)
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ6ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§

### âš¡ å¤„ç†å¤§è§„æ¨¡è½¬å½•ç»„æ•°æ®çš„ä¼˜åŒ–ç­–ç•¥

```python
print("\n" + "=" * 60)
print("âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šå¤„ç†ç™¾ä¸‡çº§æ•°æ®ç‚¹")
print("=" * 60)

# åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†
print("åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®...")
n_genes = 10000
n_samples = 100

# ä½æ•ˆæ–¹æ³• vs é«˜æ•ˆæ–¹æ³•å¯¹æ¯”
import time

# åˆ›å»ºå¤§å‹DataFrame
big_df = pd.DataFrame(
    np.random.randn(n_genes, n_samples),
    index=[f'GENE_{i:05d}' for i in range(n_genes)],
    columns=[f'Sample_{i:03d}' for i in range(n_samples)]
)

print(f"æ•°æ®è§„æ¨¡: {big_df.shape[0]:,} åŸºå›  Ã— {big_df.shape[1]} æ ·æœ¬")
print(f"æ€»æ•°æ®ç‚¹: {big_df.shape[0] * big_df.shape[1]:,}")

# ä¼˜åŒ–æŠ€å·§1ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œä»£æ›¿å¾ªç¯
print("\nâš¡ ä¼˜åŒ–æŠ€å·§1ï¼šå‘é‡åŒ–æ“ä½œ")
print("-" * 40)

# ä½æ•ˆï¼šä½¿ç”¨å¾ªç¯è®¡ç®—z-score
def slow_zscore(df):
    result = df.copy()
    for col in df.columns:
        for idx in df.index:
            mean = df[col].mean()
            std = df[col].std()
            result.loc[idx, col] = (df.loc[idx, col] - mean) / std
    return result

# é«˜æ•ˆï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œ
def fast_zscore(df):
    return (df - df.mean()) / df.std()

# æµ‹è¯•å°æ•°æ®é›†çš„æ€§èƒ½
small_df = big_df.iloc[:100, :10]

start = time.time()
result_fast = fast_zscore(small_df)
fast_time = time.time() - start

print(f"å‘é‡åŒ–æ“ä½œæ—¶é—´: {fast_time:.4f} ç§’")
print(f"æ€§èƒ½æå‡: 100å€ä»¥ä¸Šï¼ˆå¾ªç¯æ–¹æ³•å¤ªæ…¢ï¼Œè·³è¿‡æµ‹è¯•ï¼‰")

# ä¼˜åŒ–æŠ€å·§2ï¼šä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹
print("\nâš¡ ä¼˜åŒ–æŠ€å·§2ï¼šä¼˜åŒ–æ•°æ®ç±»å‹")
print("-" * 40)

# åˆ›å»ºç¤ºä¾‹æ•°æ®
memory_df = pd.DataFrame({
    'gene_id': [f'GENE_{i:05d}' for i in range(1000)],
    'count': np.random.randint(0, 65535, 1000),  # 16ä½æ•´æ•°èŒƒå›´
    'expression': np.random.randn(1000),
    'chromosome': np.random.choice(['chr1', 'chr2', 'chr3'], 1000)
})

print("ä¼˜åŒ–å‰å†…å­˜ä½¿ç”¨:")
print(memory_df.dtypes)
print(f"æ€»å†…å­˜: {memory_df.memory_usage(deep=True).sum() / 1024:.2f} KB")

# ä¼˜åŒ–æ•°æ®ç±»å‹
optimized_df = memory_df.copy()
optimized_df['count'] = optimized_df['count'].astype('uint16')  # ä½¿ç”¨æ— ç¬¦å·16ä½æ•´æ•°
optimized_df['expression'] = optimized_df['expression'].astype('float32')  # ä½¿ç”¨32ä½æµ®ç‚¹
optimized_df['chromosome'] = optimized_df['chromosome'].astype('category')  # ç±»åˆ«ç±»å‹

print("\nä¼˜åŒ–åå†…å­˜ä½¿ç”¨:")
print(optimized_df.dtypes)
print(f"æ€»å†…å­˜: {optimized_df.memory_usage(deep=True).sum() / 1024:.2f} KB")
print(f"å†…å­˜èŠ‚çœ: {(1 - optimized_df.memory_usage(deep=True).sum() / memory_df.memory_usage(deep=True).sum()) * 100:.1f}%")

# ä¼˜åŒ–æŠ€å·§3ï¼šä½¿ç”¨queryå’Œeval
print("\nâš¡ ä¼˜åŒ–æŠ€å·§3ï¼šä½¿ç”¨queryè¿›è¡Œé«˜æ•ˆç­›é€‰")
print("-" * 40)

# æ·»åŠ ä¸€äº›å±æ€§åˆ—
big_df['mean_expr'] = big_df.mean(axis=1)
big_df['std_expr'] = big_df.std(axis=1)
big_df['cv'] = big_df['std_expr'] / big_df['mean_expr']

# ä¼ ç»Ÿç­›é€‰æ–¹æ³•
start = time.time()
traditional_filter = big_df[(big_df['mean_expr'] > 0) & 
                           (big_df['cv'] > 0.5) & 
                           (big_df['std_expr'] < 2)]
traditional_time = time.time() - start

# ä½¿ç”¨queryæ–¹æ³•
start = time.time()
query_filter = big_df.query('mean_expr > 0 & cv > 0.5 & std_expr < 2')
query_time = time.time() - start

print(f"ä¼ ç»Ÿç­›é€‰æ—¶é—´: {traditional_time:.4f} ç§’")
print(f"Queryç­›é€‰æ—¶é—´: {query_time:.4f} ç§’")
print(f"ç­›é€‰ç»“æœæ•°é‡: {len(query_filter)} ä¸ªåŸºå› ")

# ä¼˜åŒ–æŠ€å·§4ï¼šåˆ†å—å¤„ç†å¤§æ–‡ä»¶
print("\nâš¡ ä¼˜åŒ–æŠ€å·§4ï¼šåˆ†å—è¯»å–å¤§æ–‡ä»¶")
print("-" * 40)

# æ¨¡æ‹Ÿå¤§æ–‡ä»¶è¯»å–
print("æ¨¡æ‹Ÿåˆ†å—å¤„ç†å¤§å‹CSVæ–‡ä»¶:")

def process_chunk(chunk):
    """å¤„ç†å•ä¸ªæ•°æ®å—"""
    # è¿‡æ»¤ä½è¡¨è¾¾åŸºå› 
    high_expr = chunk[chunk.mean(axis=1) > chunk.mean().mean()]
    return high_expr

# æ¨¡æ‹Ÿåˆ†å—å¤„ç†
chunk_size = 1000
n_chunks = n_genes // chunk_size
processed_genes = 0

for i in range(n_chunks):
    chunk = big_df.iloc[i*chunk_size:(i+1)*chunk_size]
    processed_chunk = process_chunk(chunk)
    processed_genes += len(processed_chunk)
    
    if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
        print(f"  å— {i+1}: å¤„ç† {len(chunk)} ä¸ªåŸºå› ï¼Œä¿ç•™ {len(processed_chunk)} ä¸ª")

print(f"æ€»è®¡å¤„ç†: {n_genes} ä¸ªåŸºå› ")
print(f"æ€»è®¡ä¿ç•™: {processed_genes} ä¸ªåŸºå› ")

# ä¼˜åŒ–æŠ€å·§5ï¼šå¹¶è¡Œå¤„ç†
print("\nâš¡ ä¼˜åŒ–æŠ€å·§5ï¼šä½¿ç”¨å¹¶è¡Œå¤„ç†")
print("-" * 40)

def calculate_gene_stats(gene_data):
    """è®¡ç®—å•ä¸ªåŸºå› çš„ç»Ÿè®¡é‡"""
    return {
        'mean': gene_data.mean(),
        'std': gene_data.std(),
        'cv': gene_data.std() / gene_data.mean() if gene_data.mean() != 0 else 0,
        'max': gene_data.max(),
        'min': gene_data.min()
    }

# ä¸²è¡Œå¤„ç†ï¼ˆç¤ºä¾‹ï¼‰
start = time.time()
serial_results = []
for idx in big_df.index[:100]:  # åªå¤„ç†å‰100ä¸ªåŸºå› ä½œä¸ºç¤ºä¾‹
    stats = calculate_gene_stats(big_df.loc[idx])
    serial_results.append(stats)
serial_time = time.time() - start

print(f"ä¸²è¡Œå¤„ç†100ä¸ªåŸºå› æ—¶é—´: {serial_time:.4f} ç§’")
print(f"é¢„è®¡å¤„ç†å…¨éƒ¨{n_genes}ä¸ªåŸºå› : {serial_time * n_genes / 100:.2f} ç§’")

# ä½¿ç”¨applyï¼ˆPandaså†…éƒ¨ä¼˜åŒ–ï¼‰
start = time.time()
apply_results = big_df.iloc[:100].apply(calculate_gene_stats, axis=1)
apply_time = time.time() - start

print(f"ä½¿ç”¨applyå¤„ç†100ä¸ªåŸºå› æ—¶é—´: {apply_time:.4f} ç§’")
print(f"æ€§èƒ½æå‡: {serial_time / apply_time:.2f}x")

# æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µæ€»ç»“
print("\n" + "=" * 60)
print("ğŸ“‹ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µæ€»ç»“")
print("=" * 60)

best_practices = """
1. ğŸ“Š æ•°æ®ç±»å‹ä¼˜åŒ–
   â€¢ æ•´æ•°ï¼šä½¿ç”¨æœ€å°çš„åˆé€‚ç±»å‹ï¼ˆint8, int16, int32ï¼‰
   â€¢ æµ®ç‚¹æ•°ï¼šfloat32 vs float64
   â€¢ å­—ç¬¦ä¸²ï¼šè€ƒè™‘ä½¿ç”¨categoryç±»å‹
   
2. âš¡ å‘é‡åŒ–æ“ä½œ
   â€¢ é¿å…Pythonå¾ªç¯ï¼Œä½¿ç”¨NumPy/Pandaså†…ç½®å‡½æ•°
   â€¢ ä½¿ç”¨.apply()è€Œä¸æ˜¯iterrows()
   â€¢ æ‰¹é‡æ“ä½œä¼˜äºé€ä¸ªæ“ä½œ
   
3. ğŸ” é«˜æ•ˆç­›é€‰
   â€¢ ä½¿ç”¨.query()è¿›è¡Œå¤æ‚æ¡ä»¶ç­›é€‰
   â€¢ ä½¿ç”¨.loc[]å’Œ.iloc[]è€Œä¸æ˜¯é“¾å¼ç´¢å¼•
   â€¢ å¸ƒå°”ç´¢å¼•ä¼˜äºå¾ªç¯ç­›é€‰
   
4. ğŸ’¾ å†…å­˜ç®¡ç†
   â€¢ åˆ†å—è¯»å–å¤§æ–‡ä»¶ï¼ˆchunksizeå‚æ•°ï¼‰
   â€¢ åŠæ—¶åˆ é™¤ä¸éœ€è¦çš„å˜é‡ï¼ˆdel, gc.collect()ï¼‰
   â€¢ ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†åºåˆ—æ•°æ®
   
5. ğŸš€ å¹¶è¡Œå¤„ç†
   â€¢ ä½¿ç”¨multiprocessingå¤„ç†ç‹¬ç«‹ä»»åŠ¡
   â€¢ è€ƒè™‘ä½¿ç”¨Daskå¤„ç†è¶…å¤§æ•°æ®é›†
   â€¢ NumPyçš„å¹¶è¡Œæ“ä½œï¼ˆè®¾ç½®MKLçº¿ç¨‹æ•°ï¼‰
"""
print(best_practices)
```

## ğŸ¯ ç»¼åˆé¡¹ç›®ï¼šå®Œæ•´çš„RNA-seqåˆ†æç®¡é“

### ğŸ§¬ ä»åŸå§‹æ•°æ®åˆ°ç”Ÿç‰©å­¦æ´å¯Ÿ

```python
print("\n" + "=" * 70)
print("ğŸ§¬ ç»¼åˆé¡¹ç›®ï¼šç™Œç—‡è¯ç‰©å“åº”çš„å®Œæ•´è½¬å½•ç»„åˆ†æ")
print("=" * 70)

class RNASeqAnalysisPipeline:
    """å®Œæ•´çš„RNA-seqåˆ†æç®¡é“"""
    
    def __init__(self, name="Cancer Drug Response Study"):
        self.name = name
        self.raw_counts = None
        self.normalized_data = None
        self.de_results = None
        self.enrichment_results = None
        
    def load_data(self):
        """æ­¥éª¤1ï¼šåŠ è½½å’Œæ•´åˆæ•°æ®"""
        print("\nğŸ“ æ­¥éª¤1ï¼šæ•°æ®åŠ è½½å’Œæ•´åˆ")
        print("-" * 50)
        
        np.random.seed(42)
        
        # æ¨¡æ‹Ÿ3ä¸ªç»†èƒç³»ï¼Œæ¯ä¸ªæœ‰å¯¹ç…§å’Œå¤„ç†ï¼Œå„3ä¸ªé‡å¤
        cell_lines = ['MCF7', 'MDA-MB-231', 'T47D']
        conditions = ['Control', 'Treated']
        replicates = ['Rep1', 'Rep2', 'Rep3']
        
        # ç”Ÿæˆæ ·æœ¬åç§°
        samples = []
        sample_info = []
        for cell_line in cell_lines:
            for condition in conditions:
                for replicate in replicates:
                    sample_name = f"{cell_line}_{condition}_{replicate}"
                    samples.append(sample_name)
                    sample_info.append({
                        'sample': sample_name,
                        'cell_line': cell_line,
                        'condition': condition,
                        'replicate': replicate
                    })
        
        # åˆ›å»ºæ ·æœ¬ä¿¡æ¯è¡¨
        self.sample_metadata = pd.DataFrame(sample_info)
        
        # ç”ŸæˆcountçŸ©é˜µï¼ˆ2000ä¸ªåŸºå› ï¼‰
        n_genes = 2000
        gene_ids = [f'ENSG{i:05d}' for i in range(1, n_genes + 1)]
        
        # ç”ŸæˆåŸºå› æ³¨é‡Š
        gene_names = [f'GENE{i}' for i in range(1, n_genes + 1)]
        gene_types = np.random.choice(
            ['protein_coding', 'lncRNA', 'miRNA', 'pseudogene'],
            n_genes,
            p=[0.7, 0.15, 0.1, 0.05]
        )
        chromosomes = np.random.choice([f'chr{i}' for i in range(1, 23)] + ['chrX', 'chrY'], n_genes)
        
        self.gene_annotation = pd.DataFrame({
            'gene_id': gene_ids,
            'gene_name': gene_names,
            'gene_type': gene_types,
            'chromosome': chromosomes
        })
        
        # ç”Ÿæˆè¡¨è¾¾æ•°æ®
        base_expression = np.random.lognormal(8, 2, n_genes)
        count_matrix = pd.DataFrame(index=gene_ids, columns=samples)
        
        for sample in samples:
            sample_meta = self.sample_metadata[self.sample_metadata['sample'] == sample].iloc[0]
            
            # æ·»åŠ ç»†èƒç³»ç‰¹å¼‚æ€§
            cell_line_effect = {'MCF7': 1.0, 'MDA-MB-231': 1.2, 'T47D': 0.8}
            
            # æ·»åŠ å¤„ç†æ•ˆåº”
            if sample_meta['condition'] == 'Treated':
                expression = base_expression.copy()
                # 30%åŸºå› å—å½±å“
                affected_genes = np.random.choice(n_genes, int(n_genes * 0.3), replace=False)
                expression[affected_genes] *= np.random.uniform(0.3, 3, len(affected_genes))
            else:
                expression = base_expression
            
            # æ·»åŠ å™ªå£°å’Œç»†èƒç³»æ•ˆåº”
            expression = expression * cell_line_effect[sample_meta['cell_line']]
            expression = expression * np.random.normal(1, 0.1, n_genes)
            
            count_matrix[sample] = np.random.poisson(expression)
        
        self.raw_counts = count_matrix
        
        print(f"âœ“ åŠ è½½æ•°æ®å®Œæˆ:")
        print(f"  â€¢ åŸºå› æ•°: {len(gene_ids)}")
        print(f"  â€¢ æ ·æœ¬æ•°: {len(samples)}")
        print(f"  â€¢ ç»†èƒç³»: {', '.join(cell_lines)}")
        print(f"  â€¢ å®éªŒè®¾è®¡: {len(conditions)} ä¸ªæ¡ä»¶ Ã— {len(replicates)} ä¸ªé‡å¤")
        
        return self
    
    def quality_control(self):
        """æ­¥éª¤2ï¼šè´¨é‡æ§åˆ¶"""
        print("\nğŸ” æ­¥éª¤2ï¼šè´¨é‡æ§åˆ¶")
        print("-" * 50)
        
        # è®¡ç®—QCæŒ‡æ ‡
        qc_metrics = pd.DataFrame({
            'total_counts': self.raw_counts.sum(),
            'detected_genes': (self.raw_counts > 0).sum(),
            'mean_count': self.raw_counts.mean(),
            'median_count': self.raw_counts.median()
        })
        
        # æ·»åŠ æ ·æœ¬ä¿¡æ¯
        qc_metrics = qc_metrics.merge(self.sample_metadata, left_index=True, right_on='sample')
        
        print("æ ·æœ¬è´¨é‡æŒ‡æ ‡æ±‡æ€»:")
        print(qc_metrics.groupby(['cell_line', 'condition'])[['total_counts', 'detected_genes']].mean().round(0))
        
        # è¯†åˆ«ç¦»ç¾¤æ ·æœ¬
        z_scores = np.abs((qc_metrics['total_counts'] - qc_metrics['total_counts'].mean()) / 
                         qc_metrics['total_counts'].std())
        outliers = qc_metrics[z_scores > 3]
        
        if len(outliers) > 0:
            print(f"\nâš ï¸ å‘ç° {len(outliers)} ä¸ªç¦»ç¾¤æ ·æœ¬")
        else:
            print("\nâœ“ æ‰€æœ‰æ ·æœ¬é€šè¿‡è´¨é‡æ§åˆ¶")
        
        # è¿‡æ»¤ä½è¡¨è¾¾åŸºå› 
        min_count = 10
        min_samples = 3
        keep_genes = (self.raw_counts >= min_count).sum(axis=1) >= min_samples
        self.filtered_counts = self.raw_counts[keep_genes]
        
        print(f"\nåŸºå› è¿‡æ»¤:")
        print(f"  â€¢ è¿‡æ»¤å‰: {len(self.raw_counts)} ä¸ªåŸºå› ")
        print(f"  â€¢ è¿‡æ»¤å: {len(self.filtered_counts)} ä¸ªåŸºå› ")
        print(f"  â€¢ è¿‡æ»¤æ‰: {len(self.raw_counts) - len(self.filtered_counts)} ä¸ªä½è¡¨è¾¾åŸºå› ")
        
        return self
    
    def normalize_data(self):
        """æ­¥éª¤3ï¼šæ•°æ®æ ‡å‡†åŒ–"""
        print("\nğŸ“Š æ­¥éª¤3ï¼šæ•°æ®æ ‡å‡†åŒ–")
        print("-" * 50)
        
        # CPMæ ‡å‡†åŒ–ï¼ˆCounts Per Millionï¼‰
        cpm = self.filtered_counts.div(self.filtered_counts.sum()) * 1e6
        
        # log2è½¬æ¢
        self.normalized_data = np.log2(cpm + 1)
        
        print("âœ“ CPMæ ‡å‡†åŒ–å®Œæˆ")
        print(f"  â€¢ æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´: [{self.normalized_data.min().min():.2f}, {self.normalized_data.max().max():.2f}]")
        
        # æ£€æŸ¥æ‰¹æ¬¡æ•ˆåº”
        print("\næ‰¹æ¬¡æ•ˆåº”æ£€æŸ¥:")
        # è®¡ç®—æ ·æœ¬é—´ç›¸å…³æ€§
        sample_corr = self.normalized_data.corr()
        
        # æŒ‰ç»†èƒç³»åˆ†ç»„æŸ¥çœ‹ç›¸å…³æ€§
        for cell_line in self.sample_metadata['cell_line'].unique():
            cell_line_samples = self.sample_metadata[
                self.sample_metadata['cell_line'] == cell_line
            ]['sample'].values
            
            within_corr = sample_corr.loc[cell_line_samples, cell_line_samples].values
            within_corr = within_corr[np.triu_indices_from(within_corr, k=1)]
            
            print(f"  â€¢ {cell_line} å†…éƒ¨ç›¸å…³æ€§: {within_corr.mean():.3f} Â± {within_corr.std():.3f}")
        
        return self
    
    def differential_expression(self):
        """æ­¥éª¤4ï¼šå·®å¼‚è¡¨è¾¾åˆ†æ"""
        print("\nğŸ§¬ æ­¥éª¤4ï¼šå·®å¼‚è¡¨è¾¾åˆ†æ")
        print("-" * 50)
        
        results_list = []
        
        for cell_line in self.sample_metadata['cell_line'].unique():
            print(f"\nåˆ†æ {cell_line} ç»†èƒç³»:")
            
            # è·å–è¯¥ç»†èƒç³»çš„æ ·æœ¬
            control_samples = self.sample_metadata[
                (self.sample_metadata['cell_line'] == cell_line) & 
                (self.sample_metadata['condition'] == 'Control')
            ]['sample'].values
            
            treated_samples = self.sample_metadata[
                (self.sample_metadata['cell_line'] == cell_line) & 
                (self.sample_metadata['condition'] == 'Treated')
            ]['sample'].values
            
            # å¯¹æ¯ä¸ªåŸºå› è¿›è¡Œå·®å¼‚åˆ†æ
            for gene in self.filtered_counts.index:
                control_values = self.filtered_counts.loc[gene, control_samples].values
                treated_values = self.filtered_counts.loc[gene, treated_samples].values
                
                # è®¡ç®—ç»Ÿè®¡é‡
                control_mean = control_values.mean()
                treated_mean = treated_values.mean()
                fold_change = treated_mean / (control_mean + 1)
                log2_fc = np.log2(fold_change + 0.001)
                
                # tæ£€éªŒ
                _, p_value = stats.ttest_ind(control_values, treated_values)
                
                results_list.append({
                    'cell_line': cell_line,
                    'gene': gene,
                    'control_mean': control_mean,
                    'treated_mean': treated_mean,
                    'fold_change': fold_change,
                    'log2_fc': log2_fc,
                    'p_value': p_value
                })
        
        self.de_results = pd.DataFrame(results_list)
        
        # FDRæ ¡æ­£
        for cell_line in self.de_results['cell_line'].unique():
            mask = self.de_results['cell_line'] == cell_line
            p_values = self.de_results.loc[mask, 'p_value'].values
            _, fdr, _, _ = multipletests(p_values, method='fdr_bh')
            self.de_results.loc[mask, 'fdr'] = fdr
        
        # ç»Ÿè®¡æ˜¾è‘—åŸºå› 
        sig_threshold = 0.05
        fc_threshold = 1.5
        
        for cell_line in self.de_results['cell_line'].unique():
            cell_line_de = self.de_results[self.de_results['cell_line'] == cell_line]
            sig_genes = cell_line_de[
                (cell_line_de['fdr'] < sig_threshold) & 
                (np.abs(cell_line_de['log2_fc']) > np.log2(fc_threshold))
            ]
            
            n_up = (sig_genes['log2_fc'] > 0).sum()
            n_down = (sig_genes['log2_fc'] < 0).sum()
            
            print(f"  â€¢ æ˜¾è‘—å·®å¼‚åŸºå› : {len(sig_genes)} (â†‘{n_up} â†“{n_down})")
        
        return self
    
    def functional_enrichment(self):
        """æ­¥éª¤5ï¼šåŠŸèƒ½å¯Œé›†åˆ†æ"""
        print("\nğŸ”¬ æ­¥éª¤5ï¼šåŠŸèƒ½å¯Œé›†åˆ†æ")
        print("-" * 50)
        
        # æ¨¡æ‹ŸGO termså’ŒKEGG pathways
        go_terms = {
            'GO:0006915': 'apoptotic process',
            'GO:0007049': 'cell cycle',
            'GO:0006954': 'inflammatory response',
            'GO:0008283': 'cell proliferation',
            'GO:0001525': 'angiogenesis',
            'GO:0006629': 'lipid metabolic process',
            'GO:0006096': 'glycolytic process',
            'GO:0042981': 'regulation of apoptotic process'
        }
        
        kegg_pathways = {
            'hsa04110': 'Cell cycle',
            'hsa04210': 'Apoptosis',
            'hsa04151': 'PI3K-Akt signaling pathway',
            'hsa04668': 'TNF signaling pathway',
            'hsa00010': 'Glycolysis / Gluconeogenesis'
        }
        
        enrichment_results = []
        
        for cell_line in self.de_results['cell_line'].unique():
            # è·å–æ˜¾è‘—ä¸Šè°ƒå’Œä¸‹è°ƒåŸºå› 
            cell_line_de = self.de_results[self.de_results['cell_line'] == cell_line]
            sig_up = cell_line_de[
                (cell_line_de['fdr'] < 0.05) & 
                (cell_line_de['log2_fc'] > np.log2(1.5))
            ]['gene'].values
            
            sig_down = cell_line_de[
                (cell_line_de['fdr'] < 0.05) & 
                (cell_line_de['log2_fc'] < -np.log2(1.5))
            ]['gene'].values
            
            print(f"\n{cell_line} å¯Œé›†åˆ†æ:")
            
            # æ¨¡æ‹ŸGOå¯Œé›†ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if len(sig_up) > 0:
                print("  ä¸Šè°ƒåŸºå› å¯Œé›†çš„GO terms:")
                for go_id, go_name in list(go_terms.items())[:3]:
                    # æ¨¡æ‹Ÿå¯Œé›†åˆ†æ•°
                    enrichment_score = np.random.uniform(2, 5)
                    p_value = np.random.uniform(0.0001, 0.05)
                    print(f"    â€¢ {go_name}: ES={enrichment_score:.2f}, p={p_value:.4f}")
                    
                    enrichment_results.append({
                        'cell_line': cell_line,
                        'direction': 'up',
                        'term_id': go_id,
                        'term_name': go_name,
                        'enrichment_score': enrichment_score,
                        'p_value': p_value
                    })
            
            if len(sig_down) > 0:
                print("  ä¸‹è°ƒåŸºå› å¯Œé›†çš„GO terms:")
                for go_id, go_name in list(go_terms.items())[3:6]:
                    enrichment_score = np.random.uniform(2, 5)
                    p_value = np.random.uniform(0.0001, 0.05)
                    print(f"    â€¢ {go_name}: ES={enrichment_score:.2f}, p={p_value:.4f}")
                    
                    enrichment_results.append({
                        'cell_line': cell_line,
                        'direction': 'down',
                        'term_id': go_id,
                        'term_name': go_name,
                        'enrichment_score': enrichment_score,
                        'p_value': p_value
                    })
        
        self.enrichment_results = pd.DataFrame(enrichment_results)
        
        return self
    
    def generate_report(self):
        """æ­¥éª¤6ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“ æ­¥éª¤6ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("-" * 50)
        
        print("\n" + "=" * 70)
        print("                    RNA-seqåˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("=" * 70)
        
        print(f"\nç ”ç©¶åç§°: {self.name}")
        print(f"åˆ†ææ—¥æœŸ: 2024-01-15")
        print(f"åˆ†æäººå‘˜: BioPythonå­¦ä¹ è€…")
        
        print("\n1. æ•°æ®æ¦‚å†µ:")
        print(f"   â€¢ æ€»åŸºå› æ•°: {len(self.raw_counts)}")
        print(f"   â€¢ åˆ†æåŸºå› æ•°: {len(self.filtered_counts)}")
        print(f"   â€¢ æ ·æœ¬æ•°: {len(self.sample_metadata)}")
        print(f"   â€¢ ç»†èƒç³»: {', '.join(self.sample_metadata['cell_line'].unique())}")
        
        print("\n2. å·®å¼‚è¡¨è¾¾æ±‡æ€»:")
        de_summary = self.de_results[self.de_results['fdr'] < 0.05].groupby('cell_line').agg({
            'gene': 'count',
            'log2_fc': lambda x: (x > 0).sum()
        })
        de_summary.columns = ['total_sig_genes', 'up_regulated']
        de_summary['down_regulated'] = de_summary['total_sig_genes'] - de_summary['up_regulated']
        print(de_summary)
        
        print("\n3. ä¸»è¦å‘ç°:")
        print("   â€¢ è¯ç‰©å¤„ç†å¯¼è‡´å¤šä¸ªç»†èƒç³»çš„è½¬å½•ç»„å‘ç”Ÿæ˜¾è‘—å˜åŒ–")
        print("   â€¢ ä¸åŒç»†èƒç³»æ˜¾ç¤ºå‡ºç‰¹å¼‚æ€§çš„è¯ç‰©å“åº”æ¨¡å¼")
        print("   â€¢ å‡‹äº¡å’Œç»†èƒå‘¨æœŸç›¸å…³é€šè·¯æ˜¾è‘—å¯Œé›†")
        
        print("\n4. å…³é”®åŸºå› ï¼ˆTopå“åº”åŸºå› ï¼‰:")
        # æ‰¾å‡ºåœ¨æ‰€æœ‰ç»†èƒç³»ä¸­éƒ½æ˜¾è‘—çš„åŸºå› 
        sig_in_all = self.de_results[self.de_results['fdr'] < 0.05].groupby('gene').size()
        consistent_genes = sig_in_all[sig_in_all == 3].index[:5]
        if len(consistent_genes) > 0:
            print("   åœ¨æ‰€æœ‰ç»†èƒç³»ä¸­ä¸€è‡´å“åº”çš„åŸºå› :")
            for gene in consistent_genes:
                gene_data = self.de_results[self.de_results['gene'] == gene]
                mean_fc = gene_data['log2_fc'].mean()
                direction = "â†‘" if mean_fc > 0 else "â†“"
                print(f"     â€¢ {gene}: {direction} (å¹³å‡log2FC: {mean_fc:.2f})")
        
        print("\n5. å»ºè®®åç»­å®éªŒ:")
        print("   â€¢ ä½¿ç”¨qPCRéªŒè¯å…³é”®å·®å¼‚è¡¨è¾¾åŸºå› ")
        print("   â€¢ Western blotéªŒè¯è›‹ç™½æ°´å¹³å˜åŒ–")
        print("   â€¢ åŠŸèƒ½å®éªŒéªŒè¯å‡‹äº¡é€šè·¯æ¿€æ´»")
        print("   â€¢ æ—¶é—´åºåˆ—å®éªŒè¿½è¸ªåŠ¨æ€å“åº”")
        
        print("\n" + "=" * 70)
        print("                    åˆ†æå®Œæˆï¼")
        print("=" * 70)
        
        return self

# è¿è¡Œå®Œæ•´çš„åˆ†æç®¡é“
print("\nğŸš€ å¯åŠ¨RNA-seqåˆ†æç®¡é“...")
pipeline = RNASeqAnalysisPipeline()

# æ‰§è¡Œåˆ†ææµç¨‹
(pipeline
 .load_data()
 .quality_control()
 .normalize_data()
 .differential_expression()
 .functional_enrichment()
 .generate_report())

print("\nâœ¨ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„RNA-seqæ•°æ®åˆ†ææµç¨‹ï¼")
```

## ğŸ“ ç»ƒä¹ é¢˜é¢„è§ˆ

### å¾ªåºæ¸è¿›çš„ç»ƒä¹ è®¾è®¡

æœ¬ç« çš„ç»ƒä¹ é¢˜è®¾è®¡éµå¾ª"ç”±æµ…å…¥æ·±ã€ç†è®ºç»“åˆå®è·µ"çš„åŸåˆ™ï¼š

#### ğŸŒŸ åŸºç¡€ç»ƒä¹ ï¼ˆâ˜…ï¼‰- å¤¯å®åŸºæœ¬åŠŸ
1. **æ•°æ®æ•´åˆç»ƒä¹ **ï¼šåˆå¹¶æ¥è‡ªä¸åŒå®éªŒå®¤çš„è¡¨è¾¾æ•°æ®
2. **è´¨é‡æ§åˆ¶ç»ƒä¹ **ï¼šè¯†åˆ«å’Œå¤„ç†å¼‚å¸¸æ ·æœ¬
3. **åŸºç¡€ç»Ÿè®¡ç»ƒä¹ **ï¼šè®¡ç®—åŸºå› è¡¨è¾¾çš„æè¿°æ€§ç»Ÿè®¡

#### ğŸš€ è¿›é˜¶ç»ƒä¹ ï¼ˆâ˜…â˜…ï¼‰- æ¨¡æ‹ŸçœŸå®åœºæ™¯
4. **å·®å¼‚è¡¨è¾¾åˆ†æ**ï¼šå®Œæ•´çš„DEåˆ†ææµç¨‹å®è·µ
5. **æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£**ï¼šè¯†åˆ«å¹¶æ¶ˆé™¤æŠ€æœ¯åå·®
6. **åŠŸèƒ½åˆ†ç»„åˆ†æ**ï¼šæŒ‰åŸºå› åŠŸèƒ½è¿›è¡Œåˆ†ç»„æ¯”è¾ƒ

#### ğŸ† æŒ‘æˆ˜ç»ƒä¹ ï¼ˆâ˜…â˜…â˜…ï¼‰- ç»¼åˆåº”ç”¨
7. **æ—¶é—´åºåˆ—åˆ†æ**ï¼šè¯ç‰©å¤„ç†çš„æ—¶é—´å“åº”ç ”ç©¶
8. **å¤šç»„å­¦æ•´åˆ**ï¼šè½¬å½•ç»„ä¸è›‹ç™½ç»„æ•°æ®æ•´åˆ
9. **æœºå™¨å­¦ä¹ åº”ç”¨**ï¼šåŸºäºè¡¨è¾¾è°±çš„æ ·æœ¬åˆ†ç±»

æ¯ä¸ªç»ƒä¹ éƒ½é…æœ‰ï¼š
- æ˜ç¡®çš„å­¦ä¹ ç›®æ ‡
- è¯¦ç»†çš„ä»»åŠ¡è¯´æ˜
- å¾ªåºæ¸è¿›çš„æç¤º
- å®Œæ•´çš„å‚è€ƒç­”æ¡ˆ

## ğŸ“ æœ¬ç« æ€»ç»“ï¼šä»æ•°æ®åˆ°æ´å¯Ÿçš„æ¡¥æ¢

### ğŸŒŸ ä½ å·²ç»æŒæ¡çš„æ ¸å¿ƒèƒ½åŠ›

ç»è¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ ç°åœ¨èƒ½å¤Ÿï¼š

1. **æ•°æ®å·¥ç¨‹å¸ˆçš„èƒ½åŠ›**
   - æ•´åˆå¤šæ¥æºçš„å¤æ‚æ•°æ®
   - å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
   - ä¼˜åŒ–å¤§è§„æ¨¡æ•°æ®çš„å¤„ç†æ€§èƒ½

2. **ç»Ÿè®¡å­¦å®¶çš„æ€ç»´**
   - é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•
   - è¿›è¡Œä¸¥æ ¼çš„å¤šé‡æ£€éªŒæ ¡æ­£
   - è¯„ä¼°ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§å’Œç”Ÿç‰©å­¦æ„ä¹‰

3. **ç”Ÿç‰©ä¿¡æ¯å­¦å®¶çš„è§†è§’**
   - æ‰§è¡Œå®Œæ•´çš„å·®å¼‚è¡¨è¾¾åˆ†æ
   - è¯†åˆ«å…³é”®çš„ç”Ÿç‰©å­¦é€šè·¯
   - ä»æ•°æ®ä¸­æå–ç”Ÿç‰©å­¦æ´å¯Ÿ

### ğŸ’¡ å…³é”®è¦ç‚¹å›é¡¾

```python
# æœ¬ç« æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“
chapter_summary = {
    "æ•°æ®æ•´åˆ": {
        "merge": "ç²¾ç¡®è¿æ¥å¤šä¸ªæ•°æ®æº",
        "concat": "çºµå‘æˆ–æ¨ªå‘åˆå¹¶æ•°æ®",
        "pivot": "æ•°æ®é‡å¡‘å’Œé€è§†åˆ†æ"
    },
    
    "åˆ†ç»„åˆ†æ": {
        "groupby": "æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡",
        "agg": "å¤šé‡èšåˆå‡½æ•°",
        "transform": "ç»„å†…æ•°æ®å˜æ¢"
    },
    
    "ç»Ÿè®¡æ£€éªŒ": {
        "t_test": "ä¸¤ç»„æ¯”è¾ƒ",
        "ANOVA": "å¤šç»„æ¯”è¾ƒ",
        "FDR": "å¤šé‡æ£€éªŒæ ¡æ­£"
    },
    
    "æ—¶é—´åºåˆ—": {
        "rolling": "æ»‘åŠ¨çª—å£åˆ†æ",
        "expanding": "ç´¯ç§¯åˆ†æ",
        "correlation": "æ—¶é—´ç‚¹ç›¸å…³æ€§"
    },
    
    "æ€§èƒ½ä¼˜åŒ–": {
        "vectorization": "å‘é‡åŒ–æ“ä½œ",
        "dtypes": "æ•°æ®ç±»å‹ä¼˜åŒ–",
        "chunking": "åˆ†å—å¤„ç†å¤§æ–‡ä»¶"
    }
}
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

1. **å®è·µé¡¹ç›®**ï¼šç”¨çœŸå®çš„GEOæˆ–TCGAæ•°æ®é‡ç°æœ¬ç« åˆ†æ
2. **æ·±å…¥å­¦ä¹ **ï¼šæ¢ç´¢DESeq2ã€edgeRç­‰ä¸“ä¸šå·®å¼‚åˆ†æå·¥å…·
3. **å¯è§†åŒ–æå‡**ï¼šå­¦ä¹ ç¬¬8ç« ï¼Œå°†åˆ†æç»“æœè½¬åŒ–ä¸ºä¸“ä¸šå›¾è¡¨
4. **æœºå™¨å­¦ä¹ è¿›é˜¶**ï¼šç»“åˆç¬¬10ç« ï¼Œåº”ç”¨MLæ–¹æ³•è¿›è¡Œé¢„æµ‹å»ºæ¨¡

## ğŸŒˆ å†™åœ¨æœ€åï¼šæ•°æ®åˆ†æçš„è‰ºæœ¯

äº²çˆ±çš„å­¦ä¹ è€…ï¼š

å¦‚æœä½ è®¤çœŸå­¦ä¹ äº†æœ¬ç« çš„å†…å®¹ï¼Œä½ å·²ç»æŒæ¡äº†ä¸“ä¸šçº§åˆ«çš„è½¬å½•ç»„æ•°æ®åˆ†ææŠ€èƒ½ã€‚è¿™ä¸ä»…ä»…æ˜¯æŠ€æœ¯çš„æå‡ï¼Œæ›´æ˜¯æ€ç»´æ–¹å¼çš„è½¬å˜ã€‚

è®°ä½ï¼Œæ•°æ®åˆ†æä¸ä»…æ˜¯ä¸€é—¨ç§‘å­¦ï¼Œæ›´æ˜¯ä¸€é—¨è‰ºæœ¯ã€‚å®ƒéœ€è¦ï¼š
- **ä¸¥è°¨çš„ç»Ÿè®¡æ€ç»´**ï¼šç¡®ä¿ç»“æœçš„å¯é æ€§
- **æ·±åšçš„ç”Ÿç‰©å­¦çŸ¥è¯†**ï¼šç†è§£æ•°æ®èƒŒåçš„æ„ä¹‰
- **åˆ›æ–°çš„é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šåº”å¯¹å„ç§æ•°æ®æŒ‘æˆ˜
- **æ¸…æ™°çš„æ²Ÿé€šè¡¨è¾¾**ï¼šå°†å‘ç°è½¬åŒ–ä¸ºç§‘å­¦æ•…äº‹

åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ç”¨Matplotlibå’ŒSeabornåˆ›å»ºå‡ºç‰ˆçº§åˆ«çš„ç§‘å­¦å›¾è¡¨ï¼Œè®©ä½ çš„æ•°æ®åˆ†æç»“æœä»¥æœ€ç¾çš„æ–¹å¼å‘ˆç°ï¼

åŠ æ²¹ï¼ä½ ç¦»æˆä¸ºä¸€åçœŸæ­£çš„ç”Ÿç‰©ä¿¡æ¯å­¦å®¶åˆè¿‘äº†ä¸€æ­¥ï¼ğŸ‰

---

**æ€è€ƒé¢˜**ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªåŒ…å«100ä¸ªç—…äººã€20000ä¸ªåŸºå› ã€5ä¸ªæ—¶é—´ç‚¹çš„ä¸´åºŠè½¬å½•ç»„æ•°æ®é›†ï¼Œä½ ä¼šå¦‚ä½•è®¾è®¡åˆ†ææµç¨‹æ¥å‘ç°ç–¾ç—…ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Ÿ

**æŒ‘æˆ˜è‡ªå·±**ï¼šå°è¯•ç”¨æœ¬ç« å­¦åˆ°çš„æ–¹æ³•ï¼Œåˆ†æä¸€ä¸ªçœŸå®çš„GEOæ•°æ®é›†ï¼ˆå¦‚GSE48213ï¼‰ï¼Œçœ‹çœ‹èƒ½å¦é‡ç°è®ºæ–‡ä¸­çš„ä¸»è¦å‘ç°ï¼